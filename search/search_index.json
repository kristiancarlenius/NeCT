{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"NeCT: Neural Computed Tomography","text":"<p>Paper (to be added) Code </p> <p>NeCT leverages deep learning to improve computed tomography (CT) image quality, supporting both static and dynamic CT reconstruction. The project is a collaboration between the Norwegian Univeristy of Science and Technology (NTNU) and the CT lab at Equinor.  The project is based on the INR framework <code>tiny-cuda-nn</code>.</p> <ul> <li>User guide</li> <li>Installation</li> <li>Quick start</li> <li>Geometry configuration</li> <li>Configuration</li> <li>Demo</li> <li>Licensing and Citation</li> </ul>"},{"location":"#showcase-bentheimer-sandstone-imbibition","title":"Showcase: Bentheimer Sandstone Imbibition","text":"Your browser does not support the video tag.               Rendering of spontaneous imbibition in a Bentheimer sandstone reconstructed using NeCT. The brine flowing into the sample is shown in light blue, while the salt grains dissolving are presented in red. The video recording was started when brine entered the core.           Your browser does not support the video tag.               Rendering of the dissolution of a salt grain. Three orthogonal slices visualize its temporal evolution. In the xz slice, it is possible to observe the brine coming into contact with the salt before it starts to dissolve."},{"location":"about/licensing-and-citation/","title":"Licensing and Citation","text":"<p>The project is licensed under the MIT license. The project is a collaboration between the Norwegian Univeristy of Science and Technology (NTNU) and the CT lab at Equinor.</p> <p>If you use this project in your research, please cite the following paper: <pre><code>To be added\n</code></pre></p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li> nect<ul> <li> cfg</li> <li> config</li> <li> data</li> <li> download_demo_data</li> <li> dynamic_export</li> <li> fdk</li> <li> loss_plotter</li> <li> network<ul> <li> kplanes</li> <li> networks</li> </ul> </li> <li> reconstruct</li> <li> sampling<ul> <li> geometry</li> <li> projector</li> </ul> </li> <li> scivis_data</li> <li> src<ul> <li> evaluation<ul> <li> config</li> <li> evaluator</li> </ul> </li> <li> phantom<ul> <li> demo_3d_phantom</li> <li> geometric</li> <li> phantom_config</li> <li> pore</li> <li> utils</li> </ul> </li> <li> reconstruction<ul> <li> leap</li> <li> reconstructor</li> <li> scikit_image</li> <li> tigre_toolbox</li> </ul> </li> <li> sampling<ul> <li> leap</li> <li> methods</li> <li> sampler</li> <li> scikit_image</li> <li> tigre_toolbox</li> </ul> </li> <li> simulator<ul> <li> configuration<ul> <li> config</li> </ul> </li> <li> scheduler</li> </ul> </li> <li> test<ul> <li> test_conv</li> <li> test_pooling</li> <li> test_sampling</li> <li> test_st_ms_ssim</li> </ul> </li> <li> utils<ul> <li> nsi_to_inr_yaml</li> <li> video</li> </ul> </li> </ul> </li> <li> static_export</li> <li> trainers<ul> <li> base_trainer</li> <li> continous_scanning_trainer</li> <li> initrainer</li> <li> porous_medium_trainer</li> <li> projections_loaded_trainer</li> <li> scivis_trainer</li> </ul> </li> <li> utils</li> </ul> </li> <li> torch_extra<ul> <li> nn<ul> <li> common_types</li> <li> functional<ul> <li> conv</li> <li> loss</li> <li> pooling</li> </ul> </li> <li> modules<ul> <li> conv</li> <li> loss</li> </ul> </li> </ul> </li> <li> typing</li> </ul> </li> </ul>"},{"location":"reference/nect/","title":"Index","text":""},{"location":"reference/nect/#nect","title":"nect","text":""},{"location":"reference/nect/#nect.Geometry","title":"Geometry","text":"<pre><code>Geometry(nDetector: _list_2_int, dDetector: _list_2_float, mode: str, DSD: float | None = None, DSO: float | None = None, nVoxel: _list_3_int | None = None, dVoxel: _list_3_float | None = None, radius: float | None = None, height: float | None = None, offOrigin: _list_3_float = (0.0, 0.0, 0.0), COR: float = 0.0, offDetector: _list_2_float = (0.0, 0.0), rotDetector: _list_3_float = (0.0, 0.0, 0.0), reconstruction_mode: str = 'voxel', detector_binning: int = 1, angles: _list_float | None = None, radians: bool = True, timesteps: _list | None = None)\n</code></pre> <p>Set up the geometry for the CT system.</p> <p>Parameters:</p> Name Type Description Default <code>nDetector</code> <code>tuple[int, int] | list[int] | Tensor | ndarray</code> <p>Number of pixels <code>[height, width]</code> of the detector</p> required <code>dDetector</code> <code>tuple[float, float] | list[float] | Tensor | ndarray</code> <p>Height and width of the detector (mm)</p> required <code>mode</code> <code>str</code> <p>Type of geometry. Supported modes are <code>cone</code> and <code>parallel</code></p> required <code>DSD</code> <code>float</code> <p>Distance Source Detector (mm)</p> <code>None</code> <code>DSO</code> <code>float</code> <p>Distance Source Origin (mm)</p> <code>None</code> <code>nVoxel</code> <code>tuple[int, int, int] | list[int] | Tensor | ndarray</code> <p>Number of voxels <code>[z, y, x]</code> of the volume</p> <code>None</code> <code>dVoxel</code> <code>tuple[float, float, float] | list[float] | Tensor | ndarray</code> <p>Size of a voxel <code>[z, y, x]</code> (mm)</p> <code>None</code> <code>radius</code> <code>float</code> <p>Radius of the object (mm)</p> <code>None</code> <code>height</code> <code>float</code> <p>Height of the object (mm)</p> <code>None</code> <code>offOrigin</code> <code>tuple[float, float, float] | list[float] | Tensor | ndarray</code> <p>Offset of the object from the origin <code>[z, y, x]</code> (mm)</p> <code>(0.0, 0.0, 0.0)</code> <code>COR</code> <code>float</code> <p>Center of rotation (mm)</p> <code>0.0</code> <code>offDetector</code> <code>tuple[float, float] | list[float] | Tensor | ndarray</code> <p>Offset of the detector from the center <code>[height, width]</code> (mm)</p> <code>(0.0, 0.0)</code> <code>rotDetector</code> <code>tuple[float, float, float] | list[float] | Tensor | ndarray</code> <p>Rotation of the detector <code>[roll, pitch, yaw]</code> (radians).</p> <code>(0.0, 0.0, 0.0)</code> <code>reconstruction_mode</code> <code>str</code> <p>Type of reconstruction. Supported modes are <code>'voxel'</code> and <code>'cylindrical'</code>. Default is <code>'voxel'</code></p> <code>'voxel'</code> <code>detector_binning</code> <code>int</code> <p>Binning factor of the detector. Default is 1</p> <code>1</code> <code>angles</code> <code>list[float] | Tensor | ndarray | None</code> <p>List of angles.</p> <code>None</code> <code>radians</code> <code>bool</code> <p>Unit of angles. If <code>True</code>, the unit is radians, if <code>False</code> the unit is degrees. Default is <code>True</code></p> <code>True</code> <code>timesteps</code> <code>list[float | int] | Tensor | ndarray | None</code> <p>An array of timesteps. Do not need to be normalized. If the order of the angles and corresponding projections does not equal the acqustition order, this parameter needs to be set to get the timesteps correct. Only important for dynamic reconstruction. Overrides the timestep of the Geometry if not <code>None</code>.</p> <code>None</code> Source code in <code>nect/sampling/geometry.py</code> <pre><code>def __init__(\n    self,\n    nDetector: _list_2_int,\n    dDetector: _list_2_float,\n    mode: str,\n    DSD: float | None = None,\n    DSO: float | None = None,\n    nVoxel: _list_3_int | None = None,\n    dVoxel: _list_3_float | None = None,\n    radius: float | None = None,\n    height: float | None = None,\n    offOrigin: _list_3_float = (0.0, 0.0, 0.0),\n    COR: float = 0.0,\n    offDetector: _list_2_float = (0.0, 0.0),\n    rotDetector: _list_3_float = (0.0, 0.0, 0.0),\n    reconstruction_mode: str = \"voxel\",\n    detector_binning: int = 1,\n    angles: _list_float | None = None,\n    radians: bool = True,\n    timesteps: _list | None = None,\n):\n    \"\"\"\n    Set up the geometry for the CT system.\n\n    Args:\n        nDetector (tuple[int, int] | list[int] | torch.Tensor | np.ndarray):\n            Number of pixels `[height, width]` of the detector\n        dDetector (tuple[float, float] | list[float] | torch.Tensor | np.ndarray):\n            Height and width of the detector (mm)\n        mode (str):\n            Type of geometry. Supported modes are `cone` and `parallel`\n        DSD (float):\n            Distance Source Detector (mm)\n        DSO (float):\n            Distance Source Origin (mm)\n        nVoxel (tuple[int, int, int] | list[int] | torch.Tensor | np.ndarray):\n            Number of voxels `[z, y, x]` of the volume\n        dVoxel (tuple[float, float, float] | list[float] | torch.Tensor | np.ndarray):\n            Size of a voxel `[z, y, x]` (mm)\n        radius (float):\n            Radius of the object (mm)\n        height (float):\n            Height of the object (mm)\n        offOrigin (tuple[float, float, float] | list[float] | torch.Tensor | np.ndarray):\n            Offset of the object from the origin `[z, y, x]` (mm)\n        COR (float):\n            Center of rotation (mm)\n        offDetector (tuple[float, float] | list[float] | torch.Tensor | np.ndarray):\n            Offset of the detector from the center `[height, width]` (mm)\n        rotDetector (tuple[float, float, float] | list[float] | torch.Tensor | np.ndarray):\n            Rotation of the detector `[roll, pitch, yaw]` (radians).\n        reconstruction_mode (str):\n            Type of reconstruction. Supported modes are `'voxel'` and `'cylindrical'`. Default is `'voxel'`\n        detector_binning (int):\n            Binning factor of the detector. Default is 1\n        angles (list[float] | torch.Tensor | np.ndarray | None):\n            List of angles.\n        radians (bool):\n            Unit of angles. If `True`, the unit is radians, if `False` the unit is degrees. Default is `True`\n        timesteps (list[float | int] | torch.Tensor | np.ndarray | None):\n            An array of timesteps. Do not need to be normalized.\n            If the order of the angles and corresponding projections does not equal the acqustition order, this parameter needs to be set to get the timesteps correct.\n            Only important for dynamic reconstruction. Overrides the timestep of the Geometry if not `None`.\n    \"\"\"\n    if mode not in [\"cone\", \"parallel\"]:\n        raise ValueError(f\"Unsupported mode '{mode}'\")\n    if mode == \"cone\":\n        if DSD is None:\n            raise ValueError(\"DSD is required for cone geometry\")\n        if DSO is None:\n            raise ValueError(\"DSO is required for cone geometry\")\n        self.DSD = float(DSD)\n        self.DSO = float(DSO)\n    else:\n        if DSD is not None:\n            warn(\"DSD is not required for parallel geometry. Ignoring the value of DSD\")\n        if DSO is not None:\n            warn(\"DSO is not required for parallel geometry. Ignoring the value of DSO\")\n        self.DSD = None\n        self.DSO = None\n    if reconstruction_mode not in [\"voxel\", \"cylindrical\"]:\n        raise ValueError(f\"Unsupported reconstruction mode '{reconstruction_mode}'\")\n    self.mode = mode\n    self.reconstruction_mode = reconstruction_mode\n    self.detector_binning = detector_binning\n    self.dDetector = cast(_tuple_2_float, self._check_and_return_float(dDetector, 2, \"dDetector\"))\n    self.nDetector = cast(_tuple_2_int, self._check_and_return_int(nDetector, 2, \"nDetector\"))\n    self.offDetector = cast(_tuple_2_float, self._check_and_return_float(offDetector, 2, \"offDetector\"))\n    self.rotDetector = cast(_tuple_3_float, self._check_and_return_float(rotDetector, 3, \"rotDetector\"))\n    self.COR = COR\n    self.set_angles(angles, radians)\n    self.set_timesteps(timesteps)\n    if radius is None or height is None:\n        self.nVoxel = cast(_tuple_3_int, self._check_and_return_int(nVoxel, 3, \"nVoxel\"))\n        self.dVoxel = cast(_tuple_3_float, self._check_and_return_float(dVoxel, 3, \"dVoxel\"))\n        self.sVoxel = (\n            self.nVoxel[0] * self.dVoxel[0],\n            self.nVoxel[1] * self.dVoxel[1],\n            self.nVoxel[2] * self.dVoxel[2],\n        )\n        self.offOrigin = cast(_tuple_3_float, self._check_and_return_float(offOrigin, 3, \"offOrigin\"))\n        if self.reconstruction_mode == \"cylindrical\":\n            if radius is None:\n                self.radius = max(self.sVoxel[1], self.sVoxel[2]) / 2\n            if height is None:\n                self.height = self.sVoxel[0]\n        else:\n            self.radius = None\n            self.height = None\n    else:\n        self.radius = radius\n        self.height = height\n        if nVoxel is not None:\n            self.nVoxel = cast(_tuple_3_int, self._check_and_return_int(nVoxel, 3, \"nVoxel\"))\n        else:\n            self.nVoxel = (self.nDetector[0], self.nDetector[1], self.nDetector[1])\n        if dVoxel is not None:\n            self.dVoxel = cast(_tuple_3_float, self._check_and_return_float(dVoxel, 3, \"dVoxel\"))\n        else:\n            self.dVoxel = (\n                self.nDetector[0] / height,\n                self.nDetector[1] / (2 * radius),\n                self.nDetector[1] / (2 * radius),\n            )\n        self.sVoxel = (\n            self.nVoxel[0] * self.dVoxel[0],\n            self.nVoxel[1] * self.dVoxel[1],\n            self.nVoxel[2] * self.dVoxel[2],\n        )\n    if self.mode == \"cone\":\n        if (nVoxel is None or dVoxel is None) and reconstruction_mode == \"cylindrical\":\n            max_length = self.radius * 2\n            triangle_theta = np.arctan((self.height / 2) / (self.DSO + max_length / 2))\n            self.max_distance_traveled = max_length / np.cos(triangle_theta)\n        else:\n            max_length = ((self.sVoxel[1] - self.dVoxel[1]) ** 2 + (self.sVoxel[2] - self.dVoxel[2]) ** 2) ** 0.5\n            triangle_theta = np.arctan(((self.sVoxel[0] - self.dVoxel[0]) / 2) / (self.DSO + max_length / 2))\n            self.max_distance_traveled = max_length / np.cos(triangle_theta)\n    else:\n        if self.reconstruction_mode == \"voxel\":\n            self.max_distance_traveled = (\n                (self.sVoxel[1] - self.dVoxel[1]) ** 2 + (self.sVoxel[2] - self.dVoxel[2]) ** 2\n            ) ** 0.5\n        elif self.reconstruction_mode == \"cylindrical\":\n            if nVoxel is None or dVoxel is None:\n                self.max_distance_traveled = self.radius * 2\n            else:\n                self.max_distance_traveled = max(self.sVoxel[1], self.sVoxel[2])\n    self.sDetector = (\n        self.nDetector[0] * self.dDetector[0],\n        self.nDetector[1] * self.dDetector[1],\n    )\n</code></pre>"},{"location":"reference/nect/#nect.Geometry.from_cfg","title":"from_cfg  <code>classmethod</code>","text":"<pre><code>from_cfg(cfg: GeometryCone | Geometry, reconstruction_mode: str = 'voxel', sample_outside: int = 0) -&gt; 'Geometry'\n</code></pre> <p>Load the geometry from a configuration object.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>GeometryCone | Geometry</code> <p>The configuration object.</p> required <code>reconstruction_mode</code> <code>str</code> <p>The reconstruction mode. Default is <code>'voxel'</code>.</p> <code>'voxel'</code> <code>sample_outside</code> <code>int</code> <p>The number of voxels to sample outside the object. Default is <code>0</code>.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>Geometry</code> <code>'Geometry'</code> <p>The geometry object.</p> Source code in <code>nect/sampling/geometry.py</code> <pre><code>@classmethod\ndef from_cfg(\n    cls,\n    cfg: nect.config.GeometryCone | nect.config.Geometry,\n    reconstruction_mode: str = \"voxel\",\n    sample_outside: int = 0,\n) -&gt; \"Geometry\":\n    \"\"\"\n    Load the geometry from a configuration object.\n\n    Args:\n        cfg (nect.config.GeometryCone | nect.config.Geometry): The configuration object.\n        reconstruction_mode (str): The reconstruction mode. Default is `'voxel'`.\n        sample_outside (int): The number of voxels to sample outside the object. Default is `0`.\n\n    Returns:\n        Geometry: The geometry object.\n    \"\"\"\n    nVoxel = cfg.nVoxel\n    if sample_outside &gt; 0:\n        nVoxel = [s + 2 * sample_outside for s in nVoxel]\n    return cls(\n        nDetector=cfg.nDetector,\n        dDetector=cfg.dDetector,\n        mode=cfg.mode,\n        DSD=cfg.DSD if hasattr(cfg, \"DSD\") else None,\n        DSO=cfg.DSO if hasattr(cfg, \"DSO\") else None,\n        nVoxel=nVoxel,\n        dVoxel=cfg.dVoxel,\n        radius=cfg.radius,\n        height=cfg.height,\n        offOrigin=cfg.offOrigin,\n        COR=cfg.COR,\n        offDetector=cfg.offDetector,\n        rotDetector=cfg.rotDetector if cfg.rotDetector is not None else (0.0, 0.0, 0.0),\n        reconstruction_mode=reconstruction_mode,\n        detector_binning=1,\n        angles=cfg.angles,\n        timesteps=cfg.timesteps,\n    )\n</code></pre>"},{"location":"reference/nect/#nect.Geometry.from_yaml","title":"from_yaml  <code>classmethod</code>","text":"<pre><code>from_yaml(path: str | Path, reconstruction_mode: str | None = None) -&gt; 'Geometry'\n</code></pre> <p>Load the geometry from a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>The path to the YAML file.</p> required <code>reconstruction_mode</code> <code>str | None</code> <p>The reconstruction mode. Supported strings are <code>'voxel'</code> and <code>'cylindrical'</code>. Default is <code>None</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Geometry</code> <code>'Geometry'</code> <p>The geometry object.</p> Source code in <code>nect/sampling/geometry.py</code> <pre><code>@classmethod\ndef from_yaml(cls, path: str | Path, reconstruction_mode: str | None = None) -&gt; \"Geometry\":\n    \"\"\"\n    Load the geometry from a YAML file.\n\n    Args:\n        path (str | Path): The path to the YAML file.\n        reconstruction_mode (str | None): The reconstruction mode. Supported strings are `'voxel'` and `'cylindrical'`. Default is `None`.\n\n    Returns:\n        Geometry: The geometry object.\"\"\"\n    with open(path, \"r\") as f:\n        cfg = yaml.safe_load(f)\n    if reconstruction_mode is None:\n        reconstruction_mode = cfg.get(\"reconstruction_mode\", \"voxel\")\n    elif reconstruction_mode not in (\"voxel\", \"cylindrical\"):\n        raise NotImplementedError(\n            f\"Only reconstruction mode 'voxel' and 'cylindrical' is implemented, got '{reconstruction_mode}'\"\n        )\n    return cls(\n        nDetector=cfg[\"nDetector\"],\n        dDetector=cfg[\"dDetector\"],\n        mode=cfg[\"mode\"],\n        DSD=cfg.get(\"DSD\", None),\n        DSO=cfg.get(\"DSO\", None),\n        nVoxel=cfg.get(\"nVoxel\", None),\n        dVoxel=cfg.get(\"dVoxel\", None),\n        radius=cfg.get(\"radius\", None),\n        height=cfg.get(\"height\", None),\n        offOrigin=cfg.get(\"offOrigin\", (0.0, 0.0, 0.0)),\n        COR=cfg.get(\"COR\", 0.0),\n        offDetector=cfg.get(\"offDetector\", (0.0, 0.0)),\n        rotDetector=cfg.get(\"rotDetector\", (0.0, 0.0, 0.0)),\n        reconstruction_mode=reconstruction_mode,\n        detector_binning=1,\n        angles=cfg.get(\"angles\", None),\n        timesteps=cfg.get(\"timesteps\", None),\n        radians=cfg.get(\"radians\", True),\n    )\n</code></pre>"},{"location":"reference/nect/#nect.Geometry.set_angles","title":"set_angles","text":"<pre><code>set_angles(angles: _list_float | None, radians: bool)\n</code></pre> <p>Set the angles for the geometry.</p> <p>Parameters:</p> Name Type Description Default <code>angles</code> <code>list[float] | Tensor | ndarray | None</code> <p>List of angles.</p> required <code>radians</code> <code>bool</code> <p>Unit of angles. If <code>True</code>, the unit is radians, if <code>False</code> the unit is degrees. Default is <code>True</code></p> required Source code in <code>nect/sampling/geometry.py</code> <pre><code>def set_angles(self, angles: _list_float | None, radians: bool):\n    \"\"\"\n    Set the angles for the geometry.\n\n    Args:\n        angles (list[float] | torch.Tensor | np.ndarray | None):\n            List of angles.\n        radians (bool):\n            Unit of angles. If `True`, the unit is radians, if `False` the unit is degrees. Default is `True`\"\"\"\n    self.angles = angles\n    if self.angles is not None:\n        if not isinstance(self.angles, np.ndarray):\n            if isinstance(self.angles, list):\n                self.angles = np.array(self.angles)\n            elif isinstance(self.angles, torch.Tensor):\n                self.angles = self.angles.cpu().numpy()\n        if radians is False:\n            self.angles = np.radians(self.angles)\n</code></pre>"},{"location":"reference/nect/#nect.Geometry.set_detector_binning","title":"set_detector_binning","text":"<pre><code>set_detector_binning(detector_binning: int)\n</code></pre> <p>Set the detector binning factor.</p> <p>Parameters:</p> Name Type Description Default <code>detector_binning</code> <code>int</code> <p>The binning factor of the detector</p> required Source code in <code>nect/sampling/geometry.py</code> <pre><code>def set_detector_binning(self, detector_binning: int):\n    \"\"\"\n    Set the detector binning factor.\n\n    Args:\n        detector_binning (int): The binning factor of the detector\"\"\"\n    self.detector_binning = detector_binning\n</code></pre>"},{"location":"reference/nect/#nect.Geometry.set_timesteps","title":"set_timesteps","text":"<pre><code>set_timesteps(timesteps: _list | None)\n</code></pre> <p>Set the timesteps for dynamic reconstruction.</p> <p>Parameters:</p> Name Type Description Default <code>timesteps</code> <code>list[float | int] | Tensor | ndarray | None</code> <p>An array of timesteps. Do not need to be normalized. If the order of the angles and corresponding projections does not equal the acqustition order, this parameter needs to be set to get the timesteps correct. Only important for dynamic reconstruction. Overrides the timestep of the Geometry if not <code>None</code>.</p> required Source code in <code>nect/sampling/geometry.py</code> <pre><code>def set_timesteps(self, timesteps: _list | None):\n    \"\"\"\n    Set the timesteps for dynamic reconstruction.\n\n    Args:\n        timesteps (list[float | int] | torch.Tensor | np.ndarray | None):\n            An array of timesteps. Do not need to be normalized.\n            If the order of the angles and corresponding projections does not equal the acqustition order, this parameter needs to be set to get the timesteps correct.\n            Only important for dynamic reconstruction. Overrides the timestep of the Geometry if not `None`.\n    \"\"\"\n    self.timesteps = timesteps\n</code></pre>"},{"location":"reference/nect/#nect.export_dataset_to_npy","title":"export_dataset_to_npy","text":"<pre><code>export_dataset_to_npy(config_path: str | Path, output_file: str | Path, downsample: int = 1)\n</code></pre> <p>Load the dataset and export it as a single .npy file.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str | Path</code> <p>Path to the config YAML.</p> required <code>output_file</code> <code>str | Path</code> <p>Path where the .npy file will be saved.</p> required <code>downsample</code> <code>int</code> <p>Downsampling factor (default=1 = no downsampling).</p> <code>1</code> Source code in <code>nect/data.py</code> <pre><code>def export_dataset_to_npy(config_path: str | Path, output_file: str | Path, downsample: int = 1):\n        \"\"\"\n        Load the dataset and export it as a single .npy file.\n\n        Args:\n            config_path (str | Path): Path to the config YAML.\n            output_file (str | Path): Path where the .npy file will be saved.\n            downsample (int): Downsampling factor (default=1 = no downsampling).\n        \"\"\"\n        config = get_cfg(config_path)\n        dataset = NeCTDataset(config)\n\n        projections = dataset.get_full_projections(downsample_projections_factor=downsample)\n\n        output_file = Path(output_file).with_suffix(\".npy\")\n        np.save(output_file, projections)\n\n        print(f\"Saved projections with shape {projections.shape} to {output_file}\")\n</code></pre>"},{"location":"reference/nect/#nect.export_video","title":"export_video","text":"<pre><code>export_video(base_path: str | Path, add_scale_bar: bool = False, acquisition_time_minutes: float | None = None, plot_slice: str = 'XZ', fps: int = 5, difference: bool = True, difference_revolutions: int = 1, video_name: str = 'video') -&gt; Path\n</code></pre> <p>Exports a video of the dynamic model output. The video will be saved in the base_path directory.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>str | Path</code> <p>Path to the directory containing the config.yaml and checkpoints folder.</p> required <code>add_scale_bar</code> <code>bool</code> <p>Whether to add a scale bar to the video. Defaults to False.</p> <code>False</code> <code>acquisition_time_minutes</code> <code>float</code> <p>Acquisition time in minutes. Required if add_scale_bar is True. Defaults to None.</p> <code>None</code> <code>plot_slice</code> <code>str</code> <p>Slice to plot. Must be one of \"XY\", \"XZ\", \"YZ\". Defaults to \"XZ\".</p> <code>'XZ'</code> <code>fps</code> <code>int</code> <p>Frames per second. Defaults to 5.</p> <code>5</code> <code>difference</code> <code>bool</code> <p>Export a difference video. Defaults to True.</p> <code>True</code> <code>difference_revolutions</code> <code>int</code> <p>Number of revolutions for the background in the difference video. Defaults to 1.</p> <code>1</code> <code>video_name</code> <code>str</code> <p>Name of the video. Defaults to \"video\".</p> <code>'video'</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the saved video.</p> Source code in <code>nect/dynamic_export.py</code> <pre><code>def export_video(\n    base_path: str | Path,\n    add_scale_bar: bool = False,\n    acquisition_time_minutes: float | None = None,\n    plot_slice: str = \"XZ\",\n    fps: int = 5,\n    difference: bool = True,\n    difference_revolutions: int = 1,\n    video_name: str = \"video\",\n) -&gt; Path:\n    \"\"\"\n    Exports a video of the dynamic model output. The video will be saved in the base_path directory.\n\n    Args:\n        base_path (str | Path): Path to the directory containing the config.yaml and checkpoints folder.\n        add_scale_bar (bool, optional): Whether to add a scale bar to the video. Defaults to False.\n        acquisition_time_minutes (float, optional): Acquisition time in minutes. Required if add_scale_bar is True. Defaults to None.\n        plot_slice (str, optional): Slice to plot. Must be one of \"XY\", \"XZ\", \"YZ\". Defaults to \"XZ\".\n        fps (int, optional): Frames per second. Defaults to 5.\n        difference (bool, optional): Export a difference video. Defaults to True.\n        difference_revolutions (int, optional): Number of revolutions for the background in the difference video. Defaults to 1.\n        video_name (str, optional): Name of the video. Defaults to \"video\".\n\n    Returns:\n        Path to the saved video.\n    \"\"\"\n    setup_logger()\n    base_path = Path(base_path)\n    with torch.no_grad():  # use torch.no_grad() to disable gradient computation and avoid retaining graph\n        config = get_cfg(base_path / \"config.yaml\")\n        assert config.geometry is not None\n        model = config.get_model()\n        device = torch.device(0)\n        assert config.mode == \"dynamic\", \"Only dynamic mode is supported for video creation\"\n        logger.info(\"Starting to load model\")\n        checkpoints = torch.load(base_path / \"checkpoints\" / \"last.ckpt\", map_location=\"cpu\")\n        video_path = base_path.parent / \"videos\"\n        video_path.mkdir(parents=True, exist_ok=True)\n        model.load_state_dict(checkpoints[\"model\"])\n        model = model.to(device)\n        logger.info(\"Model loading finished\")\n        height, width = config.geometry.nVoxel[0], config.geometry.nVoxel[1]\n        if plot_slice.lower() not in [\"xy\", \"xz\", \"yz\"]:\n            raise ValueError(\"Invalid plot_slice. Must be one of 'XY', 'XZ', 'YZ'\")\n        plot_slice = plot_slice.lower()\n        if plot_slice == \"xy\":\n            height = width\n        z, y, x = torch.meshgrid(\n            [\n                torch.tensor(0.5, device=device)\n                if plot_slice == \"xy\"\n                else torch.linspace(0.0, 1.0, steps=height, device=device),\n                torch.tensor(0.5, device=device)\n                if plot_slice == \"xz\"\n                else torch.linspace(0.0, 1.0, steps=width, device=device),\n                torch.tensor(0.5, device=device)\n                if plot_slice == \"yz\"\n                else torch.linspace(0.0, 1.0, steps=width, device=device),\n            ],\n            indexing=\"ij\",\n        )\n        grid = torch.stack((z.flatten(), y.flatten(), x.flatten())).t()\n        angles = config.geometry.angles\n        projs_per_revolution = get_number_of_projections_per_revolution(angles)\n\n        # avc1 is not available on all systems, so we use mp4v as a fallback. avc1 is preferred because it uses the h.264 codec which can be viewed on most devices\n        if is_fourcc_available(\"avc1\"):\n            fourcc = cv2.VideoWriter_fourcc(*\"avc1\")\n        else:\n            fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n\n        if difference:\n            avg_img = torch.zeros((height, width), device=device)\n            n_percent = projs_per_revolution * difference_revolutions / len(angles)\n            n_steps = 30\n            for t in torch.linspace(0.00, n_percent, n_steps):\n                avg_img += model(grid, t).view(height, width) / n_steps\n            out_diff = cv2.VideoWriter(\n                str(video_path / f\"{video_name}_diff.mp4\"),\n                fourcc,\n                fps,\n                (width, height),\n                isColor=False,\n            )\n            out_merge = cv2.VideoWriter(\n                str(video_path / f\"{video_name}_merge.mp4\"),\n                fourcc,\n                fps,\n                (width * 2, height),\n                isColor=False,\n            )\n        out = cv2.VideoWriter(\n            str(video_path / f\"{video_name}.mp4\"),\n            fourcc,\n            fps,\n            (width, height),\n            isColor=False,\n        )\n\n        for t in tqdm(torch.linspace(0, 1, len(angles))):\n            output: torch.Tensor = model(grid, t).view(height, width)\n            if difference:\n                output_diff = output - avg_img\n            output = output\n            output = output / 3 * 255\n            output = output.clamp(0, 255)\n            output = output.cpu().numpy().astype(np.uint8)\n            output = np.rot90(output, 2)\n            if add_scale_bar:\n                assert (\n                    acquisition_time_minutes is not None\n                ), \"acquisition_time_minutes must be provided if add_scale_bar is True\"\n                output = add_scale_bar_and_text(\n                    frame=output,\n                    time=t.item(),\n                    config=config,\n                    acquisition_time_minutes=acquisition_time_minutes,\n                )\n            out.write(output)\n            if difference:\n                output_diff = output_diff + 4\n                output_diff = output_diff / 6 * 255\n                output_diff = output_diff.clamp(0, 255)\n\n                output_diff = output_diff.cpu().numpy().astype(np.uint8)\n                output_diff = np.rot90(output_diff, 2)\n                if add_scale_bar:\n                    assert (\n                        acquisition_time_minutes is not None\n                    ), \"acquisition_time_minutes must be provided if add_scale_bar is True\"\n                    output_diff = add_scale_bar_and_text(\n                        frame=output_diff,\n                        time=t.item(),\n                        config=config,\n                        acquisition_time_minutes=acquisition_time_minutes,\n                    )\n                # add text to the image in the top left corner saying the time\n\n                out_diff.write(output_diff)\n                out_merge.write(np.concatenate([output, output_diff], axis=1))\n\n        out.release()\n        if difference:\n            out_diff.release()\n            out_merge.release()\n    logger.info(f\"Video saved to {base_path/f'{video_name}.mp4'}\")\n    if difference:\n        logger.info(f\"Difference video saved to {base_path/f'{video_name}_diff.mp4'}\")\n        logger.info(f\"Merged video saved to {base_path/f'{video_name}_merge.mp4'}\")\n    return base_path / f\"{video_name}.mp4\"\n</code></pre>"},{"location":"reference/nect/#nect.export_volume","title":"export_volume","text":"<pre><code>export_volume(base_path: str | Path, binning: int = 1, show_slices: bool = False, ROIx: list[int] | None = None, ROIy: list[int] | None = None, ROIz: list[int] | None = None) -&gt; Path\n</code></pre> <p>Exports volume from the static model output. The volume will be saved in the base_path/volumes directory.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>str | Path</code> <p>Path to the directory containing the config.yaml and checkpoints folder.</p> required <code>binning</code> <code>int</code> <p>Binning factor. Defaults to 1.</p> <code>1</code> <code>show_slices</code> <code>bool</code> <p>If True, will show slices of the volume instead of saving it. Defaults to False.</p> <code>False</code> <code>ROIx</code> <code>list[int] | None</code> <p>Region of interest in x direction. Defaults to None. If None, the ROI will be the full range.</p> <code>None</code> <code>ROIy</code> <code>list[int] | None</code> <p>Region of interest in y direction. Defaults to None. If None, the ROI will be the full range.</p> <code>None</code> <code>ROIz</code> <code>list[int] | None</code> <p>Region of interest in z direction. Defaults to None. If None, the ROI will be the full range.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the saved volumes.</p> Source code in <code>nect/static_export.py</code> <pre><code>def export_volume(\n    base_path: str | Path,\n    binning: int = 1,\n    show_slices: bool = False,\n    ROIx: list[int] | None = None,\n    ROIy: list[int] | None = None,\n    ROIz: list[int] | None = None,\n) -&gt; Path:\n    \"\"\"\n    Exports volume from the static model output. The volume will be saved in the base_path/volumes directory.\n\n    Args:\n        base_path (str | Path): Path to the directory containing the config.yaml and checkpoints folder.\n        binning (int, optional): Binning factor. Defaults to 1.\n        show_slices (bool, optional): If True, will show slices of the volume instead of saving it. Defaults to False.\n        ROIx (list[int] | None, optional): Region of interest in x direction. Defaults to None. If None, the ROI will be the full range.\n        ROIy (list[int] | None, optional): Region of interest in y direction. Defaults to None. If None, the ROI will be the full range.\n        ROIz (list[int] | None, optional): Region of interest in z direction. Defaults to None. If None, the ROI will be the full range.\n\n    Returns:\n        Path to the saved volumes.\n    \"\"\"\n    setup_logger()\n    base_path = Path(base_path)\n    with torch.no_grad():  # use torch.no_grad() to disable gradient computation and avoid retaining graph\n        config = get_cfg(base_path / \"config.yaml\")\n        assert config.geometry is not None\n        model = config.get_model()\n        dataset = NeCTDataset(\n            config=config,\n            device=\"cpu\",  # if gpu memory is less than 50 GB, load to cpu\n        )\n        geometry = Geometry.from_cfg(\n            config.geometry,\n            reconstruction_mode=config.reconstruction_mode,\n            sample_outside=config.sample_outside,\n        )\n        device = torch.device(0)\n        checkpoints = torch.load(base_path / \"checkpoints\" / \"last.ckpt\", map_location=\"cpu\")\n        model.load_state_dict(checkpoints[\"model\"])\n        model = model.to(device)\n        height, width = config.geometry.nVoxel[0], config.geometry.nVoxel[1]\n        z_h = height // binning\n        y_w = width // binning\n        x_w = width // binning\n        base_output_path = base_path / \"volumefloat32\"\n        base_output_path.mkdir(exist_ok=True, parents=True)\n        total_volumes_saved = 0\n        nVoxels = config.geometry.nVoxel\n        rm = config.sample_outside\n        nVoxels = [nVoxels[0], nVoxels[1]+2*rm, nVoxels[2]+2*rm]\n        start_x = 0\n        end_x = 1\n        if ROIx is not None:\n            start_x = (ROIx[0] - rm) / nVoxels[2]\n            end_x = (ROIx[1] - rm) / nVoxels[2]\n            x_w = (ROIx[1]-ROIx[0]) // binning\n\n        start_y = 0\n        end_y = 1\n        if ROIy is not None:\n            start_y = (ROIy[0] - rm) / nVoxels[1]\n            end_y = (ROIy[1] - rm) / nVoxels[1]\n            y_w = (ROIy[1]-ROIy[0]) // binning\n\n        start_z = 0\n        end_z = 1\n        if ROIz is not None:\n            start_z = (ROIz[0]) / nVoxels[0]\n            end_z = (ROIz[1]) / nVoxels[0]\n            z_h = (ROIz[1]-ROIz[0]) // binning\n        if show_slices:\n            for slice_idx in [\"z\", \"y\", \"x\"]:\n                if slice_idx == \"z\":\n                    size = (y_w, x_w)\n                elif slice_idx == \"y\":\n                    size = (z_h, x_w)\n                elif slice_idx == \"x\":\n                    size = (z_h, y_w)\n                default_tensor = torch.tensor(0.5, device=device)\n                z_l = torch.linspace(start_z, end_z, steps=z_h, device=device) if slice_idx != \"z\" else default_tensor\n                y_l = torch.linspace(start_y, end_y, steps=y_w, device=device) if slice_idx != \"y\" else default_tensor\n                x_l = torch.linspace(start_x, end_x, steps=x_w, device=device) if slice_idx != \"x\" else default_tensor\n                z, y, x = torch.meshgrid([z_l, y_l, x_l], indexing=\"ij\")\n                grid = torch.stack((z.flatten(), y.flatten(), x.flatten()))\n                output = model(grid, torch.tensor(0.5, device=device)).view(size).cpu().numpy()\n                plt.imshow(output, cmap=\"gray\")\n                (base_path / \"imgs\").mkdir(parents=True, exist_ok=True)\n                plt.savefig(base_path / \"imgs\" / f\"{slice_idx}.png\")\n            return base_path / \"imgs\"\n\n        else: \n            output = torch.zeros((z_h, y_w, x_w), device=\"cpu\", dtype=torch.float32)\n            output = output.flatten()\n            z_lin = torch.linspace(start_z, end_z, steps=z_h, dtype=torch.float32, device=\"cpu\")\n            y_lin = torch.linspace(start_y, end_y, steps=y_w, dtype=torch.float32, device=\"cpu\")\n            x_lin = torch.linspace(start_x, end_x, steps=x_w, dtype=torch.float32, device=\"cpu\")\n\n            batch_size = 5_000_000\n\n            # Calculate total number of points\n            total_points = z_h * y_w * x_w\n\n            # Create a tensor of indices\n            indices = torch.arange(total_points, dtype=torch.int64)\n\n            # Split indices into batches\n            batches = torch.split(indices, batch_size)\n\n            # Process each batch\n            for batch in tqdm(batches):\n                # Calculate the z, y, x coordinates using vectorized operations\n                z_indices = batch // (y_w * x_w)\n                y_indices = (batch % (y_w * x_w)) // x_w\n                x_indices = batch % x_w\n\n                z = z_lin[z_indices]\n                y = y_lin[y_indices]\n                x = x_lin[x_indices]\n\n                grid = torch.stack((z, y, x), dim=1).cuda()                \n                batch_output = model(grid).flatten().float().detach().cpu()\n                output.view(-1)[batch] = batch_output\n            output = output.view((z_h, y_w, x_w))\n            output = output / geometry.max_distance_traveled\n            output = output * (dataset.maximum.item() - dataset.minimum.item()) + dataset.minimum.item()       \n            tif.imsave(base_output_path / f\"Static_Volume.tiff\", output.numpy())\n            total_volumes_saved += 1\n            logger.info(f\"{total_volumes_saved} volumes saved to {base_output_path}\")\n            return base_output_path\n</code></pre>"},{"location":"reference/nect/#nect.export_volumes","title":"export_volumes","text":"<pre><code>export_volumes(base_path: str | Path, binning: int = 1, avg_timesteps: int = 1, timesteps_per_revolution: int | Literal['all'] = 'all', export_revolutions: list | str = 'all', show_slices: bool = False, ROIx: list[int] | None = None, ROIy: list[int] | None = None, ROIz: list[int] | None = None, dtype: dtype = np.float32) -&gt; Path\n</code></pre> <p>Exports volumes from the dynamic model output. The volumes will be saved in the base_path/volumes directory.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>str | Path</code> <p>Path to the directory containing the config.yaml and checkpoints folder.</p> required <code>binning</code> <code>int</code> <p>Binning factor. Defaults to 1.</p> <code>1</code> <code>avg_timesteps</code> <code>int</code> <p>Number of timesteps to average together. Defaults to 1.</p> <code>1</code> <code>timesteps_per_revolution</code> <code>int | Literal['all']</code> <p>Number of timesteps to export per revolution. Defaults to \"all\".</p> <code>'all'</code> <code>export_revolutions</code> <code>list | str</code> <p>List of revolutions to export. Defaults to \"all\".</p> <code>'all'</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the saved volumes.</p> Source code in <code>nect/dynamic_export.py</code> <pre><code>def export_volumes(\n    base_path: str | Path,\n    binning: int = 1,\n    avg_timesteps: int = 1,\n    timesteps_per_revolution: int | Literal[\"all\"] = \"all\",\n    export_revolutions: list | str = \"all\",\n    show_slices: bool = False,\n    ROIx: list[int] | None = None,\n    ROIy: list[int] | None = None,\n    ROIz: list[int] | None = None,\n    dtype: np.dtype = np.float32\n) -&gt; Path:\n    \"\"\"\n    Exports volumes from the dynamic model output. The volumes will be saved in the base_path/volumes directory.\n\n    Args:\n        base_path (str | Path): Path to the directory containing the config.yaml and checkpoints folder.\n        binning (int, optional): Binning factor. Defaults to 1.\n        avg_timesteps (int, optional): Number of timesteps to average together. Defaults to 1.\n        timesteps_per_revolution (int | Literal[\"all\"], optional): Number of timesteps to export per revolution. Defaults to \"all\".\n        export_revolutions (list | str, optional): List of revolutions to export. Defaults to \"all\".\n\n    Returns:\n        Path to the saved volumes.\n    \"\"\"\n    setup_logger()\n    base_path = Path(base_path)\n    with torch.no_grad():  # use torch.no_grad() to disable gradient computation and avoid retaining graph\n        config = get_cfg(base_path / \"config.yaml\")\n        assert config.geometry is not None\n        model = config.get_model()\n        dataset = NeCTDataset(\n            config=config,\n            device=\"cpu\",  # if gpu memory is less than 50 GB, load to cpu\n        )\n        geometry = Geometry.from_cfg(\n            config.geometry,\n            reconstruction_mode=config.reconstruction_mode,\n            sample_outside=config.sample_outside,\n        )\n        device = torch.device(0)\n        checkpoints = torch.load(base_path / \"checkpoints\" / \"last.ckpt\", map_location=\"cpu\")\n        model.load_state_dict(checkpoints[\"model\"])\n        model = model.to(device)\n        assert config.mode == \"dynamic\", \"Only dynamic mode is supported for video creation\"\n        height, width = config.geometry.nVoxel[0], config.geometry.nVoxel[1]\n        z_h = height // binning\n        y_w = width // binning\n        x_w = width // binning\n        base_output_path = base_path / \"volumesfloat32\"\n        base_output_path.mkdir(exist_ok=True, parents=True)\n        angles = config.geometry.angles\n        linspace = torch.linspace(0, 1, steps=len(angles), device=device)\n        projs_per_revolution = get_number_of_projections_per_revolution(angles)\n        if export_revolutions == \"all\":\n            export_revolutions = [i for i in range(len(angles) // projs_per_revolution)]\n        if avg_timesteps &gt; 1:\n            logger.info(f\"Averaging {avg_timesteps} timesteps together\")\n        total_volumes_saved = 0\n        nVoxels = config.geometry.nVoxel\n        rm = config.sample_outside\n        nVoxels = [nVoxels[0], nVoxels[1]+2*rm, nVoxels[2]+2*rm]\n        start_x = 0\n        end_x = 1\n        if ROIx is not None:\n            start_x = (ROIx[0] - rm) / nVoxels[2]\n            end_x = (ROIx[1] - rm) / nVoxels[2]\n            x_w = (ROIx[1]-ROIx[0]) // binning\n\n        start_y = 0\n        end_y = 1\n        if ROIy is not None:\n            start_y = (ROIy[0] - rm) / nVoxels[1]\n            end_y = (ROIy[1] - rm) / nVoxels[1]\n            y_w = (ROIy[1]-ROIy[0]) // binning\n\n        start_z = 0\n        end_z = 1\n        if ROIz is not None:\n            start_z = (ROIz[0]) / nVoxels[0]\n            end_z = (ROIz[1]) / nVoxels[0]\n            z_h = (ROIz[1]-ROIz[0]) // binning\n        if show_slices:\n            for slice_idx in [\"z\", \"y\", \"x\"]:\n                if slice_idx == \"z\":\n                    size = (y_w, x_w)\n                elif slice_idx == \"y\":\n                    size = (z_h, x_w)\n                elif slice_idx == \"x\":\n                    size = (z_h, y_w)\n                default_tensor = torch.tensor(0.5, device=device)\n                z_l = torch.linspace(start_z, end_z, steps=z_h, device=device) if slice_idx != \"z\" else default_tensor\n                y_l = torch.linspace(start_y, end_y, steps=y_w, device=device) if slice_idx != \"y\" else default_tensor\n                x_l = torch.linspace(start_x, end_x, steps=x_w, device=device) if slice_idx != \"x\" else default_tensor\n                z, y, x = torch.meshgrid([z_l, y_l, x_l], indexing=\"ij\")\n                grid = torch.stack((z.flatten(), y.flatten(), x.flatten())).t()\n                output = model(grid, torch.tensor(0.5, device=device)).view(size).cpu().numpy()\n                plt.imshow(output, cmap=\"gray\")\n                (base_path / \"imgs\").mkdir(parents=True, exist_ok=True)\n                plt.savefig(base_path / \"imgs\" / f\"{slice_idx}.png\")\n            return base_path / \"imgs\"\n\n        else: \n            for i in tqdm(export_revolutions, leave=True, desc=\"Exporting revolutions\"):\n                revolution_start = i * projs_per_revolution\n                revolution_end = (i + 1) * projs_per_revolution\n                if timesteps_per_revolution != \"all\":\n\n                    sub_linspace = torch.linspace(linspace[revolution_start], linspace[revolution_end], timesteps_per_revolution)  # skip timesteps to speed up export\n                else:\n                    sub_linspace = linspace[revolution_start:revolution_end]\n                for j, t in tqdm(\n                    enumerate(sub_linspace),\n                    total=len(sub_linspace),\n                    leave=False,\n                    desc=\"Projection\",\n                ):\n                    output = torch.zeros((z_h, y_w, x_w), device=device)\n                    for avg in range(avg_timesteps):\n                        for ii, z_ in enumerate(\n                            torch.linspace(start_z, end_z, steps=z_h, device=device)\n                        ):  # progress through as we don't have enough memory to compute all at once\n                            z, y, x = torch.meshgrid(\n                                [\n                                    z_,\n                                    torch.linspace(start_y, end_y, steps=y_w, device=device),\n                                    torch.linspace(start_x, end_x, steps=x_w, device=device),\n                                ],\n                                indexing=\"ij\",\n                            )\n                            grid = torch.stack((z.flatten(), y.flatten(), x.flatten())).t()\n                            output[ii] += model(grid, t + avg/len(angles)).view(y_w, x_w) / avg_timesteps\n                    output = output / geometry.max_distance_traveled\n                    output = output * (dataset.maximum.item() - dataset.minimum.item()) + dataset.minimum.item()\n                    output = output.cpu().numpy()\n                    output = output.astype(dtype)\n                    if timesteps_per_revolution == \"all\":\n                        proj_n = j\n                    else:\n                        proj_n = ((j * projs_per_revolution)//timesteps_per_revolution)\n                    tif.imsave(base_output_path / f\"T{i:04}_{proj_n:04}.tiff\", output)\n                    total_volumes_saved += 1\n            logger.info(f\"{total_volumes_saved} volumes saved to {base_output_path}\")\n            return base_output_path\n</code></pre>"},{"location":"reference/nect/#nect.fdk_from_config","title":"fdk_from_config","text":"<pre><code>fdk_from_config(config: Config, output_directory: str | Path | None = None) -&gt; ndarray\n</code></pre> <p>Reconstruct the object with FDK as a validation step using the config object with the same dataloading as done in NeCT reconstruction.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config object for reconstruction</p> required <code>output_directory</code> <code>(str, Path)</code> <p>Save volume and image slices to the specified folder</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The reconstructed volume</p> Source code in <code>nect/fdk.py</code> <pre><code>def fdk_from_config(config: Config, output_directory: str | Path | None = None) -&gt; np.ndarray:\n    \"\"\"\n    Reconstruct the object with FDK as a validation step using the config object with the same dataloading as done in NeCT reconstruction.\n\n    Args:\n        config (nect.config.Config): The config object for reconstruction\n        output_directory (str, Path, optional): Save volume and image slices to the specified folder\n\n    Returns:\n        np.ndarray: The reconstructed volume\n    \"\"\"\n    import tigre\n\n    dataset = NeCTDataset(config=config, device=\"cpu\")\n    projs = np.zeros((len(dataset), config.geometry.nDetector[0], config.geometry.nDetector[1]), dtype=np.float32)\n    angles = np.zeros((len(dataset)), dtype=np.float32)\n    for i, (proj, angle, _) in tqdm(enumerate(dataset), total=len(dataset), desc=\"Loading projections\"):\n        projs[i] = proj\n        angles[i] = angle\n    geo = tigre_geometry_from_geometry(config.geometry)\n    argsort_idx = np.argsort(angles)\n    angles = np.copy(angles[argsort_idx])\n    projs = np.copy(projs[argsort_idx])\n    volume = tigre.algorithms.fdk(projs, geo, angles)\n    if output_directory is not None:\n        output_directory = Path(output_directory)\n        output_directory.mkdir(parents=True, exist_ok=True)\n        fig, axes = plt.subplot(1, 3)\n        axes[0].imshow(volume[geo.nVoxel[0] // 2])\n        axes[1].imshow(volume[:, geo.nVoxel[1] // 2])\n        axes[2].imshow(volume[:, :, geo.nVoxel[2] // 2])\n        plt.savefig(output_directory / \"slices.png\", dpi=300)\n        if config.save_volume is True:\n            np.save(output_directory / \"volume.npy\", volume)\n    return volume\n</code></pre>"},{"location":"reference/nect/#nect.reconstruct_from_config_file","title":"reconstruct_from_config_file","text":"<pre><code>reconstruct_from_config_file(cfg: str | Path, log_path: str = 'outputs', save_ckpt: bool = True, checkpoint: str | None = None, save_volume: bool = False, save_last: bool = True, save_optimizer: bool = True, cancel_at: str | None = None, prune: bool = True, keep_two: bool = True)\n</code></pre> <p>Create a 3D or 4D-CT reconstruction from a set of 2D projections from config file.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>str | Path</code> <p>Path to the configuration file.</p> required <code>log_path</code> <code>str</code> <p>Base path to the output directory.</p> <code>'outputs'</code> <code>save_ckpt</code> <code>bool</code> <p>Save the checkpoint during reconstruction.</p> <code>True</code> <code>checkpoint</code> <code>str | None</code> <p>Load a checkpoint file to continue training.</p> <code>None</code> <code>save_volume</code> <code>bool</code> <p>Save the final volume to a file.</p> <code>False</code> <code>save_last</code> <code>bool</code> <p>Save the last checkpoint.</p> <code>True</code> <code>save_optimizer</code> <code>bool</code> <p>Save the optimizer state.</p> <code>True</code> <code>cancel_at</code> <code>str</code> <p>Cancel training at the specified ISO-datetime. Save the model before canceling.</p> <code>None</code> <code>prune</code> <code>bool</code> <p>Prune the model to remove the optimizer.</p> <code>True</code> Source code in <code>nect/reconstruct.py</code> <pre><code>def reconstruct_from_config_file(\n    cfg: str | Path,\n    log_path: str = \"outputs\",\n    save_ckpt: bool = True,\n    checkpoint: str | None = None,\n    save_volume: bool = False,\n    save_last: bool = True,\n    save_optimizer: bool = True,\n    cancel_at: str | None = None,\n    prune: bool = True,\n    keep_two: bool = True\n):\n    \"\"\"\n    Create a 3D or 4D-CT reconstruction from a set of 2D projections from config file.\n\n    Args:\n        cfg (str | Path): Path to the configuration file.\n        log_path (str): Base path to the output directory.\n        save_ckpt (bool): Save the checkpoint during reconstruction.\n        checkpoint (str | None): Load a checkpoint file to continue training.\n        save_volume (bool): Save the final volume to a file.\n        save_last (bool): Save the last checkpoint.\n        save_optimizer (bool): Save the optimizer state.\n        cancel_at (str, optional): Cancel training at the specified ISO-datetime. Save the model before canceling.\n        prune (bool): Prune the model to remove the optimizer.\n    \"\"\"\n    config_file_path_splitted = str(cfg).split(os.sep)\n    config = get_cfg(cfg)\n\n    output_folder = os.path.join(*config_file_path_splitted[-2:-1])\n    exp_name = os.path.join(log_path, output_folder, config.mode)\n    if checkpoint:\n        exp_name = Path(checkpoint).parent.parent.parent\n\n    trainer = BaseTrainer\n\n    if config.evaluation is not None:\n        if config.evaluation.gt_path_mode.upper() == \"SCIVIS\":\n            from nect.trainers.scivis_trainer import SciVisTrainer\n            trainer = SciVisTrainer\n\n        elif config.evaluation.gt_path_mode.upper() == \"POROUSMEDIUM\":\n            from nect.trainers.porous_medium_trainer import PorousMediumTrainer\n            trainer = PorousMediumTrainer\n\n    if config.continous_scanning is True:\n        from nect.trainers.continous_scanning_trainer import ContinousScanningTrainer\n        trainer = ContinousScanningTrainer\n\n    trainer = trainer(\n        config=config,\n        checkpoint=checkpoint,\n        output_directory=exp_name,\n        save_ckpt=save_ckpt,\n        save_last=save_last,\n        save_optimizer=save_optimizer,\n        verbose=True,\n        cancel_at=cancel_at,\n        prune=prune,\n        keep_two=keep_two\n    )\n    if save_volume:\n        trainer.save_volume()\n    else:\n        trainer.fit()\n</code></pre>"},{"location":"reference/nect/config/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> config","text":""},{"location":"reference/nect/config/#nect.config","title":"config","text":""},{"location":"reference/nect/config/#nect.config.Config","title":"Config  <code>dataclass</code>","text":"<pre><code>Config(save_mode: Optional[str], uniform_ray_spacing: bool, batch_per_proj: str | int, add_poisson: bool, points_per_batch: int | Literal['auto'], reconstruction_mode: str, geometry: Geometry | GeometryCone, img_path: str | Path, sparse_view: list[int] | None, channel_order: str | None, evaluation: Evaluation | None, warmup: Warmup, lr_scheduler: LRScheduler, optimizer: Optimizer, loss: str, base_lr: float, clip_grad_value: float | None, epochs: int | Literal['auto'] | str, plot_type: str | None, image_interval: float, checkpoint_interval: float, points_per_ray: PointsPerRay, s3im: bool, model: str, encoder: HashEncoderConfig | KPlanesEncoderConfig, net: MLPNetConfig | PirateNetConfig, concat: Optional[bool], save_volume: bool = False, downsampling_detector: DownsamplingDetector = DownsamplingDetector(1, 1, 1), crop: Crop = Crop(0, 0, 0), use_prior: bool = False, lr: float | None = None, w0_steps: int | None = None, w0_lr_multi: float | None = None, checkpoint_prior: Optional[str] = None, checkpoint_epoch: Optional[int] = None, damp_multi: Optional[list[float]] = None, tv: float = 0.0, sample_outside: int = 0, accumulation_steps: int | None = None, continous_scanning: bool = False, num_workers: int = 0)\n</code></pre>"},{"location":"reference/nect/config/#nect.config.Config.get_model","title":"get_model","text":"<pre><code>get_model() -&gt; Module\n</code></pre> <p>Get the model from the configuration.</p> <p>Returns:</p> Type Description <code>Module</code> <p>nn.Module: Model.</p> Source code in <code>nect/config.py</code> <pre><code>def get_model(self) -&gt; torch.nn.Module:\n    \"\"\"\n    Get the model from the configuration.\n\n    Returns:\n        nn.Module: Model.\n    \"\"\"\n    nvmlInit()\n    h = nvmlDeviceGetHandleByIndex(0)\n    memory_info = nvmlDeviceGetMemoryInfo(h)\n    free_memory = int(memory_info.free)\n\n    model = self.model\n    byte_size = 4\n    if model == \"kplanes\":\n        from nect.network import KPlanes\n\n        if isinstance(self.encoder, KPlanesEncoderConfig) and isinstance(self.net, MLPNetConfig):\n            # memory_per_point = nodes_interpolation * byte_size (unknown) * self.encoder.n_levels (unknown) * num_encoders\n            memory_per_point = 4 * 2 * 4 * 4 * self.encoder.output_coordinate_dim\n            model = KPlanes(encoding_config=self.encoder, network_config=self.net)\n        else:\n            raise ValueError(f\"Encoder and network configuration for model type {model} is not valid\")\n\n    elif model in [\"hash_grid\", \"double_hash_grid\", \"quadcubes\", \"hypercubes\"]:\n        if not (isinstance(self.encoder, HashEncoderConfig) and isinstance(self.net, MLPNetConfig)):\n            raise ValueError(f\"Encoder and network configuration for model type {model} is not valid\")\n\n        if model == \"hash_grid\":\n            from nect.network import HashGrid\n\n            # memory_per_point = nodes_interpolation * byte_size * self.encoder.n_levels\n            memory_per_point = 8 * byte_size * self.encoder.n_levels\n            model = HashGrid(\n                encoding_config=self.encoder, \n                network_config=self.net\n            )\n\n        elif model == \"double_hash_grid\":\n            from nect.network import DoubleHashGrid\n\n            # memory_per_point = nodes_interpolation * byte_size * self.encoder.n_levels\n            memory_per_point = (8 + 16) * byte_size * self.encoder.n_levels\n            model = DoubleHashGrid(\n                encoding_config=self.encoder,\n                network_config=self.net,\n            )\n\n        elif model == \"quadcubes\":\n            from nect.network import QuadCubes\n\n            # memory_per_point = nodes_interpolation * byte_size * self.encoder.n_levels * num_encoders\n            memory_per_point = 8 * byte_size * self.encoder.n_levels * 4\n\n            model = QuadCubes(\n                encoding_config=self.encoder,\n                network_config=self.net,\n                prior=self.use_prior,\n                concat=self.concat if self.concat is not None else True,\n            )\n\n        elif model == \"hypercubes\":\n            from nect.network import HyperCubes\n\n            # memory_per_point = nodes_interpolation * byte_size * self.encoder.n_levels * num_encoders\n            memory_per_point = 8 * byte_size * self.encoder.n_levels * 5\n\n            model = HyperCubes(\n                encoding_config=self.encoder,\n                network_config=self.net,\n            )\n        else:\n            raise ValueError(f\"Model type {model} is not supported\")\n\n    else:\n        raise ValueError(f\"Model type {model} is not supported\")\n    # compute size of model\n    # number of elements in the model\n    memory_per_point *= 1.5  # buffer\n    if isinstance(self.net, MLPNetConfig):\n        memory_per_point += self.net.n_hidden_layers * self.net.n_neurons * 2 * 1.5  # 1.5 for buffer\n\n    elif isinstance(self.encoder, HashEncoderConfig):\n        memory_per_point += (\n            self.net.n_modules\n            * self.encoder.n_features_per_level\n            * self.encoder.n_levels\n            * (2 if self.mode == \"dynamic\" else 1)\n            * byte_size\n        )\n\n    model_and_optimizer_size = sum(p.numel() for p in model.parameters()) * 4 * 3\n    if torch.cuda.device_count() &gt; 1:\n        model_and_optimizer_size *= 2  # ddp stores copy\n\n    model_and_optimizer_size += 1024**3 * 3 + max(1024**3 * 3, model_and_optimizer_size * 0.3)  # buffer\n    if self.points_per_batch == \"auto\":\n        # memory_per_point = memory_per_point * 2 # buffer as we need for some calculation during backprop\n        if self.uniform_ray_spacing:\n            avg_memory_per_point = memory_per_point / 4 * 3\n        else:\n            avg_memory_per_point = memory_per_point\n\n        self.points_per_batch = int((free_memory - model_and_optimizer_size) / avg_memory_per_point)\n        self.points_per_batch = min(5_000_000, self.points_per_batch)\n        logger.info(f\"Setting points_per_batch to {self.points_per_batch:_}\")\n        if self.points_per_batch &lt; 1:\n            raise ValueError(\"Not enough memory to store even one point\")\n\n    return model\n</code></pre>"},{"location":"reference/nect/config/#nect.config.cfg_sanity_check","title":"cfg_sanity_check","text":"<pre><code>cfg_sanity_check(cfg: dict)\n</code></pre> <p>Check if the configuration is valid.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>dict</code> <p>Configuration.</p> required Source code in <code>nect/config.py</code> <pre><code>def cfg_sanity_check(cfg: dict):\n    \"\"\"\n    Check if the configuration is valid.\n\n    Args:\n        cfg (dict): Configuration.\n\n    \"\"\"\n    mode = cfg.get(\"mode\")\n    if mode not in [\"static\", \"dynamic\"]:\n        raise ValueError(f\"Mode {mode} is not valid, must be either 'static' or 'dynamic'\")\n\n    in_func = lambda x, y: x.casefold() in [y_el.casefold() for y_el in y]\n    supported_activation_functions = [\"none\", \"relu\", \"sigmoid\", \"leakyrelu\", \"exponential\", \"tanh\", \"sine\", \"squareplus\", \"softplus\",]\n\n    in_list = \"in the list\"\n    gt_eq = \"greater or equal to\"\n    gt = \"greater than\"\n    lt_eq = \"less or equal to\"\n    eq = \"equal to\"\n    len_eq = \"length equal to\"\n    correct_type = \"correct type\"\n    hash_encoder = {\n        \"otype\": (str, [(str.__eq__, \"HashGrid\", eq)]),\n        \"n_levels\": [(int, [(int.__ge__, 1)], gt_eq)],\n        \"n_features_per_level\": (int, [(int.__ge__, 1, gt_eq)]),\n        \"log2_hashmap_size\": (int, [(int.__ge__, 1, gt_eq)]),\n        \"base_resolution\": (int, [(int.__ge__, 1, gt_eq)]),\n        \"max_resolution_factor\": (float, [(float.__gt__, 0.0, gt)]),\n        \"max_resolution_type\": (Optional[str], [(in_func, [\"nVoxel\", \"nDetector\"], in_list)])}\n\n    mlp_net = {\n        \"otype\": (str, [(in_func, [\"FullyFusedMLP\", \"CutlassMLP\"], in_list)]),\n        \"n_hidden_layers\": (int, [(int.__ge__, 1, gt_eq)]),\n        \"n_neurons\": (int, [(int.__ge__, 1, gt_eq)]),\n        \"activation\": (str, [(in_func, supported_activation_functions, in_list)]),\n        \"output_activation\": (str, [(in_func, supported_activation_functions, in_list)]),\n        \"include_adaptive_skip\": (Optional[bool], []),\n        \"include_identity\": (Optional[bool], []),}\n\n    sanity = {\n        \"hash_grid\": {\"encoder\": hash_encoder, \"net\": mlp_net},\n        \"kplanes\": {\n            \"encoder\": {\n                \"grid_dimensions\": (int, [(int.__eq__, 2, eq)]),\n                \"input_coordinate_dim\": (int, [(lambda x, y: x == y, 3 if mode == \"static\" else 4, eq)],),\n                \"output_coordinate_dim\": (int, [(int.__ge__, 1, gt_eq)]),\n                \"resolution\": (list, [(lambda x, y: len(x) == y, 3 if mode == \"static\" else 4, len_eq,), (lambda x, y: all(isinstance(i, y) for i in x), int, correct_type,),],),\n                \"regularization\": {\n                    \"space_lambda\": (float, [(float.__ge__, 0.0, gt_eq)]),\n                    \"time_lambda\": (float, [(float.__ge__, 0.0, gt_eq)]),\n                    \"time_type\": (str, [(in_func, [\"l1\", \"smoothnes\"], in_list)],),\n                },\n            },\n            \"net\": mlp_net,\n        },\n        \"double_hash_grid\": {\"encoder\": hash_encoder, \"net\": mlp_net},\n        \"quadcubes\": {\"encoder\": hash_encoder, \"net\": mlp_net, \"cat\": (Optional[bool], []),},\n        \"hypercubes\": {\"encoder\": hash_encoder, \"net\": mlp_net, \"cat\": (Optional[bool], []), },}\n\n    sanity[\"kplanes_dynamic\"] = sanity[\"kplanes\"]\n    sanity_all = {\n        \"image_interval\": (float, []),\n        \"checkpoint_interval\": (float, []),\n        \"epochs\": (str, [(lambda x, y: x == y or (len(x.split(\"x\")) == 2 and float(x.split(\"x\")[0]) &gt; 0) or int(x) &gt; 0, \"auto\", \"not a int or 'auto' or '&lt;float&gt;x'\",)],),\n        \"loss\": (str, [(in_func, [\"L1\", \"L2\", \"L1+L2\"], in_list)]),\n        \"optimizer\": {\n            \"otype\": (str, [(in_func, [\"Adam\", \"NAdam\", \"RAdam\", \"Lion\", \"SGD\"], in_list)],),\n            \"weight_decay\": (float, [(float.__ge__, 0.0, gt_eq)]),\n            \"beta1\": (float, [(float.__ge__, 0.0, gt_eq), (float.__le__, 1.0, lt_eq)]),\n            \"beta2\": (Optional[float], [(float.__ge__, 0.0, gt_eq), (float.__le__, 1.0, lt_eq)],),\n        },\n        \"lr_scheduler\": {\n            \"otype\": (str, [(in_func, [\"Exponential\", \"Cosine\"], in_list)]),\n            \"lrf\": (str, [(lambda x, y: x == y or (float(x) &gt; 0 and float(x) &lt;= 1), \"auto\", \"not a float or 'auto'\",)],),\n        },\n        \"warmup\": {\n            \"steps\": (int, [(int.__ge__, 0, gt_eq)]),\n            \"lr0\": (float, [(float.__gt__, 0.0, gt_eq)]),\n            \"otype\": (Optional[str], [(in_func, [\"Linear\", \"Exponential\"], in_list)])\n        },\n        \"s3im\": (bool, []),\n        \"points_per_ray\": {\n            \"start\": (int, [(int.__ge__, 1, gt_eq)]),\n            \"end\": (str, [(lambda x, y: x == y or (len(x.split(\"x\")) == 2 and float(x.split(\"x\")[0]) &gt; 0) or int(x) &gt; 0, \"auto\", \"not a int or 'auto' or '&lt;float&gt;x'\",)],),\n            \"update_interval\": (str, [(lambda x, y: x == y or (len(x.split(\"x\")) == 2 and float(x.split(\"x\")[0]) &gt; 0) or int(x) &gt; 0, \"auto\", \"not a int or 'auto' or '&lt;float&gt;x'\")],),\n            \"linear\": (Optional[bool], []),\n        },\n        \"batch_per_proj\": (str,  [(lambda x, y: x == y or int(x) &gt; 0, \"all\", \"not a int or 'all'\")],),\n        \"add_poisson\": (bool, []),\n        \"points_per_batch\": (str, [(lambda x, y: x == y or int(x) &gt; 0, \"auto\", \"not a int or 'auto'\")],),\n        \"reconstruction_mode\": (str, [(in_func, [\"voxel\", \"cylindrical\"], in_list)]),\n        \"img_path\": (str, [], ),\n        \"uniform_ray_spacing\": (bool, []),\n        \"geometry\": {\n            \"nDetector\": (list, [(lambda x, y: len(x) == y, 2, len_eq), (lambda x, y: all(isinstance(i, y) for i in x), int, correct_type),],),\n            \"dDetector\": (list, [(lambda x, y: len(x) == y, 2, len_eq), (lambda x, y: all(isinstance(i, y) for i in x), float | int, correct_type,),],),\n            \"nVoxel\": (list, [(lambda x, y: len(x) == y, 3, len_eq), (lambda x, y: all(isinstance(i, y) for i in x), int, correct_type),],),\n            \"dVoxel\": (list, [(lambda x, y: len(x) == y, 3, len_eq), (lambda x, y: all(isinstance(i, y) for i in x), float | int, correct_type,),],),\n            \"offOrigin\": (list, [(lambda x, y: len(x) == y, 3, len_eq), (lambda x, y: all(isinstance(i, y) for i in x), float | int, correct_type,),],),\n            \"offDetector\": (list, [(lambda x, y: len(x) == y, 2, len_eq), (lambda x, y: all(isinstance(i, y) for i in x), float | int, correct_type,),],),\n            \"rotDetector\": (Optional[list], [(lambda x, y: len(x) == y, 3, len_eq), (lambda x, y: all(isinstance(i, y) for i in x), float | int, correct_type,),],),\n            \"mode\": (str, [(in_func, [\"parallel\", \"cone\"], in_list)]),\n            \"COR\": (float, []),\n            \"angles\": (list, [(lambda x, y: all(isinstance(i, y) for i in x), float | int, correct_type,)],),\n            \"radians\": (bool, []),\n            \"timesteps\": (Optional[list], []),\n            \"radius\": (Optional[float], []),\n            \"height\": (Optional[float], []),\n            \"remove_top\": (Optional[float], []),\n            \"remove_bottom\": (Optional[float], []),\n            \"flip\": (Optional[bool], []),\n            \"DSD\": (Optional[float], []),\n            \"DSO\": (Optional[float], []),\n            \"invert_angles\": (Optional[bool], []), },}\n\n    sanity_optional = {\n        \"save_volume\": (bool, []),\n        \"clip_grad_value\": (float, [(float.__ge__, 0.0, gt_eq)]),\n        \"plot_type\": (str, [(in_func, [\"XY\", \"XZ\", \"YZ\"])]),\n        \"sparse_view\": (list, [(lambda x, y: len(x) == y, 2, len_eq), (lambda x, y: all(isinstance(i, y) for i in x), int, correct_type), (lambda x, y: x[0] &lt; x[1], None, \"first element is not less than the second\",),],),\n        \"channel_order\": (str, []),\n        \"downsampling_detector\": {\"start\": (int, [(int.__ge__, 1, gt_eq)]), \"end\": (int, [(int.__ge__, 1, gt_eq)]), \"update_interval\": (int, [(int.__ge__, 1, gt_eq)]),},\n        \"evaluation\": {\n            \"gt_path\": (str, []), \n            \"gt_path_mode\": (str,[(lambda x, y: x in y or os.path.exists(x), [\"SciVis\", \"PorousMedium\"], \"path does not exist\",)],),\n            \"evaluate_interval\": (float, [(float.__ge__, 1.0, gt_eq)]),\n            },\n        \"crop\": {\n            \"top\": (float, [(float.__ge__, 0.0, gt_eq)]),\n            \"bottom\": (float, [(float.__ge__, 0.0, gt_eq)]),\n            \"left_right\": (float, [(float.__ge__, 0.0, gt_eq)]),\n        },\n        \"use_prior\": (bool, []),\n        \"checkpoint_prior\": (Optional[str], []),\n        \"base_lr\": (float, [(float.__gt__, 0.0, gt_eq)]),\n        \"lr\": (float, [(float.__gt__, 0.0, gt_eq)]),\n        \"tv\": (float, [(float.__ge__, 0.0, gt_eq)]),\n        \"sample_outside\": (int, [(int.__ge__, 0, gt_eq)]),\n        \"accumulation_steps\": (Optional[int], [(int.__ge__, 1, gt_eq)]),\n        \"continous_scanning\": (bool, []),\n        \"num_workers\": (int, [(int.__ge__, 0, gt_eq)]),}\n\n    model = cfg.get(\"model\")\n    if model is None:\n        raise ValueError(\"Model type is not provided\")\n\n    if model not in sanity.keys():\n        raise ValueError(f\"Model type {model} is not supported, must be one of {list(sanity.keys())}\")\n\n    check_cfg(sanity[model], cfg)\n    check_cfg(sanity_all, cfg)\n    check_cfg(sanity_optional, cfg, optional=True)\n</code></pre>"},{"location":"reference/nect/config/#nect.config.check_cfg","title":"check_cfg","text":"<pre><code>check_cfg(sanity: dict, cfg: dict, nested_key: list = [], optional: bool = False)\n</code></pre> <p>Check if the configuration is valid.</p> <p>Parameters:</p> Name Type Description Default <code>sanity</code> <code>dict</code> <p>Configuration sanity check.</p> required <code>cfg</code> <code>dict</code> <p>Configuration.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the configuration is not valid.</p> Source code in <code>nect/config.py</code> <pre><code>def check_cfg(sanity: dict, cfg: dict, nested_key: list = [], optional: bool = False):\n    \"\"\"\n    Check if the configuration is valid.\n\n    Args:\n        sanity (dict): Configuration sanity check.\n        cfg (dict): Configuration.\n\n    Raises:\n        ValueError: If the configuration is not valid.\n    \"\"\"\n    for key, value in sanity.items():\n        cfg_value = cfg.get(key)\n        nested_key_ = copy.deepcopy(nested_key)\n        nested_key_.append(key)\n        if cfg_value is None:  # check if the value is provided\n            if optional or isinstance(None, value[0]):  # check if the value is optional\n                continue\n            raise ValueError(f\"Key {key} is not in the configuration at key {nested_key_}\")\n\n        if isinstance(value, dict):\n            if not isinstance(cfg_value, dict):\n                raise ValueError(f\"Value {cfg_value} is not a dictionary, but is supposed to be, at key {nested_key_}\")\n            check_cfg(value, cfg_value, nested_key_)\n\n        elif isinstance(value, tuple):\n            try:\n                cast = value[0]\n                if isinstance(None, value[0]):\n                    cast = value[0].__args__[0]  # get the type of the optional value\n                typed_cfg_value = cast(cfg_value)\n                if cast in [float, int]:\n                    assert (\n                        typed_cfg_value == cfg_value\n                    ), f\"Value {cfg_value} is not of type {value[0]} at key {nested_key_}\"\n            except Exception as e:\n                raise ValueError(f\"Value {cfg_value} is not of type {value[0]} at key {nested_key_}\") from e\n\n            for check in value[1]:\n                if not check[0](typed_cfg_value, check[1]):\n                    raise ValueError(f\"Value {cfg_value} does not satisfy the condition {check[2], check[1]} at key {nested_key_}\")\n</code></pre>"},{"location":"reference/nect/config/#nect.config.get_cfg","title":"get_cfg","text":"<pre><code>get_cfg(path: str | Path, model: str | None = None, static: bool | None = None) -&gt; Config\n</code></pre> <p>Load configuration from file and return it as a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the configuration file.</p> required <code>model</code> <code>str</code> <p>Model type. Defaults to None.</p> <code>None</code> <code>static</code> <code>bool</code> <p>Static mode. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Config</code> <p>Configuration.</p> Source code in <code>nect/config.py</code> <pre><code>def get_cfg(path: str | pathlib.Path, model: str | None = None, static: bool | None = None) -&gt; Config:\n    \"\"\"\n    Load configuration from file and return it as a dictionary.\n\n    Args:\n        path (str): Path to the configuration file.\n        model (str, optional): Model type. Defaults to None.\n        static (bool, optional): Static mode. Defaults to None.\n\n    Returns:\n        dict: Configuration.\n    \"\"\"\n    cfg_specific = None\n    if model is None or static is None:\n        cfg_specific = load_config(path)\n        if model is None:\n            model = cfg_specific.get(\"model\")\n            if model is None:\n                raise ValueError(\"Model type via parameter 'model' is not provided\")\n\n        if static is None:\n            static_cfg = cfg_specific.get(\"mode\")\n            if static_cfg not in [\"static\", \"dynamic\"]:\n                if model in [\"kplanes\", \"hash_grid\"]:\n                    static_cfg = \"static\"\n                else:\n                    static_cfg = \"dynamic\"\n\n            static = static_cfg == \"static\"\n\n    if static:\n        cfg = get_static_cfg(model)\n    else:\n        cfg = get_dynamic_cfg(model)\n\n    if not os.path.exists(path):\n        raise FileNotFoundError(f\"Path {path} does not exist\")\n\n    if cfg_specific is None:\n        cfg.update(load_config(path))\n    else:\n        cfg.update(cfg_specific)\n\n    if cfg.get(\"geometry\") == \"SAME_FOLDER\":\n        cfg[\"geometry\"] = os.path.join(os.path.dirname(path), \"geometry.yaml\")\n\n    if not pathlib.Path(cfg[\"img_path\"]).is_absolute():\n        cfg[\"img_path\"] = os.path.join(os.path.dirname(path), cfg[\"img_path\"])\n\n    geometry_cfg = load_config(cfg.get(\"geometry\"))\n    if geometry_cfg is None:\n        raise ValueError(\"Geometry configuration is not provided\")\n\n    cfg.update({\"geometry\": geometry_cfg})\n    cfg_sanity_check(cfg)\n    if cfg.get(\"lr\") is None:\n        cfg[\"lr\"] = None\n    return setup_cfg(cfg)\n</code></pre>"},{"location":"reference/nect/config/#nect.config.get_config","title":"get_config","text":"<pre><code>get_config(geometry: Geometry, img_path: str | Path, model: str | None = None, mode: str = 'static', channel_order: str | None = None) -&gt; Config\n</code></pre> <p>Get configuration.</p> <p>Parameters:</p> Name Type Description Default <code>geometry</code> <code>Geometry</code> <p>Geometry.</p> required <code>img_path</code> <code>str | Path</code> <p>Image path.</p> required <code>model</code> <code>str</code> <p>Model type. Defaults to None.</p> <code>None</code> <code>mode</code> <code>str</code> <p>Mode. Defaults to \"static\".</p> <code>'static'</code> <code>channel_order</code> <code>str</code> <p>Channel order. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Config</code> <p>Config. Configuration as a dataclass.</p> Source code in <code>nect/config.py</code> <pre><code>def get_config(\n    geometry: nect.sampling.geometry.Geometry,\n    img_path: str | pathlib.Path,\n    model: str | None = None,\n    mode: str = \"static\",\n    channel_order: str | None = None,\n) -&gt; Config:\n    \"\"\"\n    Get configuration.\n\n    Args:\n        geometry (nect.sampling.geometry.Geometry): Geometry.\n        img_path (str | pathlib.Path): Image path.\n        model (str, optional): Model type. Defaults to None.\n        mode (str, optional): Mode. Defaults to \"static\".\n        channel_order (str, optional): Channel order. Defaults to None.\n\n    Returns:\n        Config. Configuration as a dataclass.\n    \"\"\"\n    if mode not in [\"static\", \"dynamic\"]:\n        raise ValueError(f\"Mode {mode} is not valid, must be either 'static' or 'dynamic\")\n\n    if model is None:\n        model = \"hash_grid\" if mode == \"static\" else \"quadcubes\"\n\n    if mode == \"static\":\n        cfg = get_static_cfg(model)\n    else:\n        cfg = get_dynamic_cfg(model)\n\n    cfg.update({\"geometry\": geometry.to_dict(), \"img_path\": img_path, \"channel_order\": channel_order, \"model\": model})\n    return setup_cfg(cfg)\n</code></pre>"},{"location":"reference/nect/config/#nect.config.get_default_cfg","title":"get_default_cfg","text":"<pre><code>get_default_cfg() -&gt; dict\n</code></pre> <p>Load default configuration from file and return it as a dictionary.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Default configuration.</p> Source code in <code>nect/config.py</code> <pre><code>def get_default_cfg() -&gt; dict:\n    \"\"\"\n    Load default configuration from file and return it as a dictionary.\n\n    Returns:\n        dict: Default configuration.\"\"\"\n    return load_config(pathlib.Path(__file__).parent / \"cfg/default.yaml\")\n</code></pre>"},{"location":"reference/nect/config/#nect.config.get_dynamic_cfg","title":"get_dynamic_cfg","text":"<pre><code>get_dynamic_cfg(name: str) -&gt; dict\n</code></pre> <p>Load dynamic configuration from file and return it as a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Model type.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dynamic default configuration for the model type.</p> Source code in <code>nect/config.py</code> <pre><code>def get_dynamic_cfg(name: str) -&gt; dict:\n    \"\"\"\n    Load dynamic configuration from file and return it as a dictionary.\n\n    Args:\n        name (str): Model type.\n\n    Returns:\n        dict: Dynamic default configuration for the model type.\n    \"\"\"\n    default_cfg = get_default_cfg()\n    dynamic_cfg = load_config(cfg_paths[\"dynamic\"][name])\n    default_cfg.update(dynamic_cfg)\n    default_cfg.update({\"mode\": \"dynamic\"})\n    return default_cfg\n</code></pre>"},{"location":"reference/nect/config/#nect.config.get_static_cfg","title":"get_static_cfg","text":"<pre><code>get_static_cfg(name: str) -&gt; dict\n</code></pre> <p>Load static configuration from file and return it as a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Model type.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Static default configuration for the model type.</p> Source code in <code>nect/config.py</code> <pre><code>def get_static_cfg(name: str) -&gt; dict:\n    \"\"\"\n    Load static configuration from file and return it as a dictionary.\n\n    Args:\n        name (str): Model type.\n\n    Returns:\n        dict: Static default configuration for the model type.\n    \"\"\"\n    default_cfg = get_default_cfg()\n    static_cfg = load_config(cfg_paths[\"static\"][name])\n    default_cfg.update(static_cfg)\n    default_cfg.update({\"mode\": \"static\"})\n    return default_cfg\n</code></pre>"},{"location":"reference/nect/config/#nect.config.setup_cfg","title":"setup_cfg","text":"<pre><code>setup_cfg(cfg: dict) -&gt; Config\n</code></pre> <p>Setup configuration from a dict and return it as a dataclass.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>dict</code> <p>Configuration.</p> required <p>Returns:</p> Name Type Description <code>Config</code> <code>Config</code> <p>Configuration as a dataclass.</p> Source code in <code>nect/config.py</code> <pre><code>def setup_cfg(cfg: dict) -&gt; Config:\n    \"\"\"\n    Setup configuration from a dict and return it as a dataclass.\n\n    Args:\n        cfg (dict): Configuration.\n\n    Returns:\n        Config: Configuration as a dataclass.\n    \"\"\"\n    config = from_dict(data_class=Config, data=cfg)\n    if config.geometry.mode == \"cone\":\n        config.geometry = from_dict(data_class=GeometryCone, data=cfg[\"geometry\"])\n\n    return config\n</code></pre>"},{"location":"reference/nect/data/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> data","text":""},{"location":"reference/nect/data/#nect.data","title":"data","text":""},{"location":"reference/nect/data/#nect.data.NeCTDataset","title":"NeCTDataset","text":"<pre><code>NeCTDataset(config: Config, device: int | str | device = 'cpu')\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>Dataset for loading projections from file. Args:     config (Config): configuration object     device (int | str): device to load the projections to. If the whole dataset is contained in a single file, the device specifies where the projections is loaded to.                         If the dataset is contained in multiple files, the device specifies where the projections is loaded to when it is accessed.</p> Source code in <code>nect/data.py</code> <pre><code>def __init__(self, config: Config, device: int | str | torch.device = \"cpu\"):\n    \"\"\"\n    Dataset for loading projections from file.\n    Args:\n        config (Config): configuration object\n        device (int | str): device to load the projections to. If the whole dataset is contained in a single file, the device specifies where the projections is loaded to.\n                            If the dataset is contained in multiple files, the device specifies where the projections is loaded to when it is accessed.\n    \"\"\"\n    self.device = device\n    self.config = config\n    if config.channel_order is not None and config.channel_order.lower() not in [\n        \"nwh\",\n        \"nhw\",\n        \"hw\",\n        \"wh\",\n    ]:\n        raise ValueError(f\"Only NWH, NHW, WH and HW supported, got {config.channel_order}.\")\n    self.channel_order = config.channel_order.lower() if config.channel_order is not None else None\n    self.geometry = config.geometry\n    self.img_files = []\n    self.data_suffix = [\".npy\", \".raw\"]\n    self.image_suffixes = [\".tiff\", \".png\", \".jpg\", \".bmp\"]\n    self.all_valid_suffixes = [\".npy\", \".raw\", \".tiff\", \".png\", \".jpg\", \".bmp\"]\n\n    # the path may either be a single file or a directory\n    self.setup_dataset()\n\n    # timesteps can either be defined as a list in the same way as angles, or it is assumed to be linearly spaced\n    if isinstance(config.geometry.timesteps, (list, np.ndarray, torch.Tensor )):\n        self.timesteps = torch.tensor(config.geometry.timesteps)\n        if self.timesteps.size(0) != self.num_timesteps:\n            raise ValueError(\"Number of timesteps must match number of images\")\n        if torch.max(self.timesteps) &gt; 1:\n            self.timesteps = self.timesteps / torch.max(self.timesteps)\n    elif config.geometry.timesteps is not None:\n        raise ValueError(f\"If timesteps is given it must be a list, but got {type(config.geometry.timesteps)}\")\n    else:\n        self.timesteps = torch.linspace(0, 1, steps=self.num_timesteps)\n\n    # angles can either be defined as radians or degrees. Default is radians\n    self.angles = torch.tensor(config.geometry.angles)\n    if self.config.geometry.radians is False:\n        self.angles = self.angles / 180 * np.pi\n    if self.config.geometry.invert_angles is True:\n        self.angles = np.pi * 2 - self.angles\n    if self.config.sparse_view is not None:\n        self.angles = self.angles[self.config.sparse_view[0] : self.config.sparse_view[1]]\n        self.timesteps = self.timesteps[self.config.sparse_view[0] : self.config.sparse_view[1]]\n        if len(self.img_files) == 1:\n            self.proj = self.proj[self.config.sparse_view[0] : self.config.sparse_view[1]]\n        else:\n            self.img_files = self.img_files[self.config.sparse_view[0] : self.config.sparse_view[1]]\n</code></pre>"},{"location":"reference/nect/data/#nect.data.NeCTDataset.export_video","title":"export_video","text":"<pre><code>export_video(file: str | Path, skip_first: int = 0, num_projections: int = -1)\n</code></pre> <p>Export all projections as a video, for data analysis. The order of the projections is determined by the time of acqusition.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | Path</code> <p>The path to the video file.                If the file does not exist, it will be created. If the file exists, it will be overwritten.                A .mp4 extension will be added if not already present.</p> required <code>skip_first</code> <code>int</code> <p>Number of first projections to skip. Defaults to 0.</p> <code>0</code> <code>num_projections</code> <code>int</code> <p>Number of projections to export. If -1, all projections are exported. Defaults to -1.</p> <code>-1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no suitable codec is found for video writing. acv1 and mp4v are checked in that order.</p> Source code in <code>nect/data.py</code> <pre><code>def export_video(self, file: str | Path, skip_first: int = 0, num_projections: int = -1):\n    \"\"\"Export all projections as a video, for data analysis. The order of the projections is determined by the time of acqusition.\n\n    Args:\n        file (str | Path): The path to the video file.\n                           If the file does not exist, it will be created. If the file exists, it will be overwritten.\n                           A .mp4 extension will be added if not already present.\n        skip_first (int, optional): Number of first projections to skip. Defaults to 0.\n        num_projections (int, optional): Number of projections to export. If -1, all projections are exported. Defaults to -1.\n\n    Raises:\n        ValueError: If no suitable codec is found for video writing. acv1 and mp4v are checked in that order.\n    \"\"\"\n    file = Path(file)\n    file = file.with_suffix(\".mp4\")\n    file.parent.mkdir(parents=True, exist_ok=True)\n    if is_fourcc_available(\"avc1\"):\n        fourcc = cv2.VideoWriter_fourcc(*\"avc1\")\n    elif is_fourcc_available(\"mp4v\"):\n        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    else:\n        raise ValueError(\"No suitable codec found for video writing\")\n    width = int(self.config.geometry.nDetector[1])\n    height = int(self.config.geometry.nDetector[0])\n    video = cv2.VideoWriter(\n        str(file),\n        fourcc,\n        10,\n        (width, height),\n        isColor=False,\n    )\n    total_projections = len(self)\n    if num_projections == -1:\n        num_projections = total_projections - skip_first\n    num_projections = min(num_projections, total_projections - skip_first)\n    logger.info(f\"Exporting {num_projections}/{total_projections} projections\")\n    for idx in tqdm(\n        torch.argsort(self.timesteps)[skip_first : num_projections + skip_first], desc=\"Exporting video\"\n    ):\n        projection = self._get(idx)\n        projection = (projection * 255).cpu().numpy().astype(np.uint8)\n        video.write(projection)\n    video.release()\n    logger.info(f\"Video saved to {file.absolute()}\")\n</code></pre>"},{"location":"reference/nect/data/#nect.data.NeCTDatasetLoaded","title":"NeCTDatasetLoaded","text":"<pre><code>NeCTDatasetLoaded(config: Config, projections: Tensor | ndarray, device: int | str | device = 'cpu')\n</code></pre> <p>               Bases: <code>NeCTDataset</code></p> <p>Dataset for loading projections from file. Args:     config (Config): configuration object     projections (torch.Tensor | np.ndarray): projections already loaded     device (int | str): device to load the projections to. If the whole dataset is contained in a single file, the device specifies where the projections is loaded to.         If the dataset is contained in multiple files, the device specifies where the projections is loaded to when it is accessed.</p> Source code in <code>nect/data.py</code> <pre><code>def __init__(\n    self,\n    config: Config,\n    projections: torch.Tensor | np.ndarray,\n    device: int | str | torch.device = \"cpu\",\n):\n    \"\"\"\n    Dataset for loading projections from file.\n    Args:\n        config (Config): configuration object\n        projections (torch.Tensor | np.ndarray): projections already loaded\n        device (int | str): device to load the projections to. If the whole dataset is contained in a single file, the device specifies where the projections is loaded to.\n            If the dataset is contained in multiple files, the device specifies where the projections is loaded to when it is accessed.\n    \"\"\"\n    self.proj = projections\n    super().__init__(config, device)\n</code></pre>"},{"location":"reference/nect/data/#nect.data.NeCTDatasetLoaded.export_video","title":"export_video","text":"<pre><code>export_video(file: str | Path, skip_first: int = 0, num_projections: int = -1)\n</code></pre> <p>Export all projections as a video, for data analysis. The order of the projections is determined by the time of acqusition.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>str | Path</code> <p>The path to the video file.                If the file does not exist, it will be created. If the file exists, it will be overwritten.                A .mp4 extension will be added if not already present.</p> required <code>skip_first</code> <code>int</code> <p>Number of first projections to skip. Defaults to 0.</p> <code>0</code> <code>num_projections</code> <code>int</code> <p>Number of projections to export. If -1, all projections are exported. Defaults to -1.</p> <code>-1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no suitable codec is found for video writing. acv1 and mp4v are checked in that order.</p> Source code in <code>nect/data.py</code> <pre><code>def export_video(self, file: str | Path, skip_first: int = 0, num_projections: int = -1):\n    \"\"\"Export all projections as a video, for data analysis. The order of the projections is determined by the time of acqusition.\n\n    Args:\n        file (str | Path): The path to the video file.\n                           If the file does not exist, it will be created. If the file exists, it will be overwritten.\n                           A .mp4 extension will be added if not already present.\n        skip_first (int, optional): Number of first projections to skip. Defaults to 0.\n        num_projections (int, optional): Number of projections to export. If -1, all projections are exported. Defaults to -1.\n\n    Raises:\n        ValueError: If no suitable codec is found for video writing. acv1 and mp4v are checked in that order.\n    \"\"\"\n    file = Path(file)\n    file = file.with_suffix(\".mp4\")\n    file.parent.mkdir(parents=True, exist_ok=True)\n    if is_fourcc_available(\"avc1\"):\n        fourcc = cv2.VideoWriter_fourcc(*\"avc1\")\n    elif is_fourcc_available(\"mp4v\"):\n        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n    else:\n        raise ValueError(\"No suitable codec found for video writing\")\n    width = int(self.config.geometry.nDetector[1])\n    height = int(self.config.geometry.nDetector[0])\n    video = cv2.VideoWriter(\n        str(file),\n        fourcc,\n        10,\n        (width, height),\n        isColor=False,\n    )\n    total_projections = len(self)\n    if num_projections == -1:\n        num_projections = total_projections - skip_first\n    num_projections = min(num_projections, total_projections - skip_first)\n    logger.info(f\"Exporting {num_projections}/{total_projections} projections\")\n    for idx in tqdm(\n        torch.argsort(self.timesteps)[skip_first : num_projections + skip_first], desc=\"Exporting video\"\n    ):\n        projection = self._get(idx)\n        projection = (projection * 255).cpu().numpy().astype(np.uint8)\n        video.write(projection)\n    video.release()\n    logger.info(f\"Video saved to {file.absolute()}\")\n</code></pre>"},{"location":"reference/nect/data/#nect.data.export_dataset_to_npy","title":"export_dataset_to_npy","text":"<pre><code>export_dataset_to_npy(config_path: str | Path, output_file: str | Path, downsample: int = 1)\n</code></pre> <p>Load the dataset and export it as a single .npy file.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str | Path</code> <p>Path to the config YAML.</p> required <code>output_file</code> <code>str | Path</code> <p>Path where the .npy file will be saved.</p> required <code>downsample</code> <code>int</code> <p>Downsampling factor (default=1 = no downsampling).</p> <code>1</code> Source code in <code>nect/data.py</code> <pre><code>def export_dataset_to_npy(config_path: str | Path, output_file: str | Path, downsample: int = 1):\n        \"\"\"\n        Load the dataset and export it as a single .npy file.\n\n        Args:\n            config_path (str | Path): Path to the config YAML.\n            output_file (str | Path): Path where the .npy file will be saved.\n            downsample (int): Downsampling factor (default=1 = no downsampling).\n        \"\"\"\n        config = get_cfg(config_path)\n        dataset = NeCTDataset(config)\n\n        projections = dataset.get_full_projections(downsample_projections_factor=downsample)\n\n        output_file = Path(output_file).with_suffix(\".npy\")\n        np.save(output_file, projections)\n\n        print(f\"Saved projections with shape {projections.shape} to {output_file}\")\n</code></pre>"},{"location":"reference/nect/data/#nect.data.export_video_projections","title":"export_video_projections","text":"<pre><code>export_video_projections(config_path: str | Path, file: str | Path, skip_first: int = 0, num_projections: int = -1)\n</code></pre> <p>Export all projections as a video, for data analysis. The order of the projections is determined by the time of acqusition.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>str | Path</code> <p>The path to the configuration file.</p> required <code>file</code> <code>str | Path</code> <p>The path to the video file.                If the file does not exist, it will be created. If the file exists, it will be overwritten.                A .mp4 extension will be added if not already present.</p> required <code>skip_first</code> <code>int</code> <p>Number of first projections to skip. Defaults to 0.</p> <code>0</code> <code>num_projections</code> <code>int</code> <p>Number of projections to export. If -1, all projections are exported. Defaults to -1.</p> <code>-1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no suitable codec is found for video writing. acv1 and mp4v are checked in that order.</p> Source code in <code>nect/data.py</code> <pre><code>def export_video_projections(config_path: str | Path, file: str | Path, skip_first: int = 0, num_projections: int = -1):\n    \"\"\"Export all projections as a video, for data analysis. The order of the projections is determined by the time of acqusition.\n\n    Args:\n        config_path (str | Path): The path to the configuration file.\n        file (str | Path): The path to the video file.\n                           If the file does not exist, it will be created. If the file exists, it will be overwritten.\n                           A .mp4 extension will be added if not already present.\n        skip_first (int, optional): Number of first projections to skip. Defaults to 0.\n        num_projections (int, optional): Number of projections to export. If -1, all projections are exported. Defaults to -1.\n\n    Raises:\n        ValueError: If no suitable codec is found for video writing. acv1 and mp4v are checked in that order.\n    \"\"\"\n    config = get_cfg(config_path)\n    dataset = NeCTDataset(config)\n    dataset.export_video(file, skip_first, num_projections)\n</code></pre>"},{"location":"reference/nect/data/#nect.data.load_prior","title":"load_prior","text":"<pre><code>load_prior(img_path: str | Path, img_dim: int | tuple[int, int, int]) -&gt; Tensor\n</code></pre> <p>Loading prior 3D volume from file. Args:     img_path (str | Path): path to the prior image file     img_dim (int | tuple): dimension of the image</p> Source code in <code>nect/data.py</code> <pre><code>def load_prior(img_path: str | Path, img_dim: int | tuple[int, int, int]) -&gt; torch.Tensor:\n    \"\"\"\n    Loading prior 3D volume from file.\n    Args:\n        img_path (str | Path): path to the prior image file\n        img_dim (int | tuple): dimension of the image\n\n    \"\"\"\n    img_dim = (img_dim, img_dim, img_dim) if isinstance(img_dim, int) else tuple(img_dim)\n    suffix = Path(img_path).suffix\n    if suffix == \".npy\":\n        image = np.load(img_path)\n    elif suffix == \".raw\":\n        npimg = np.fromfile(img_path, dtype=np.single)\n        image = npimg.reshape(img_dim)\n    else:\n        raise NotImplementedError(f\"Only npy, npz and raw files implemented, was {suffix}\")\n\n    return torch.tensor(image, dtype=torch.float32)\n</code></pre>"},{"location":"reference/nect/download_demo_data/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> download_demo_data","text":""},{"location":"reference/nect/download_demo_data/#nect.download_demo_data","title":"download_demo_data","text":""},{"location":"reference/nect/dynamic_export/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> dynamic_export","text":""},{"location":"reference/nect/dynamic_export/#nect.dynamic_export","title":"dynamic_export","text":""},{"location":"reference/nect/dynamic_export/#nect.dynamic_export.export_video","title":"export_video","text":"<pre><code>export_video(base_path: str | Path, add_scale_bar: bool = False, acquisition_time_minutes: float | None = None, plot_slice: str = 'XZ', fps: int = 5, difference: bool = True, difference_revolutions: int = 1, video_name: str = 'video') -&gt; Path\n</code></pre> <p>Exports a video of the dynamic model output. The video will be saved in the base_path directory.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>str | Path</code> <p>Path to the directory containing the config.yaml and checkpoints folder.</p> required <code>add_scale_bar</code> <code>bool</code> <p>Whether to add a scale bar to the video. Defaults to False.</p> <code>False</code> <code>acquisition_time_minutes</code> <code>float</code> <p>Acquisition time in minutes. Required if add_scale_bar is True. Defaults to None.</p> <code>None</code> <code>plot_slice</code> <code>str</code> <p>Slice to plot. Must be one of \"XY\", \"XZ\", \"YZ\". Defaults to \"XZ\".</p> <code>'XZ'</code> <code>fps</code> <code>int</code> <p>Frames per second. Defaults to 5.</p> <code>5</code> <code>difference</code> <code>bool</code> <p>Export a difference video. Defaults to True.</p> <code>True</code> <code>difference_revolutions</code> <code>int</code> <p>Number of revolutions for the background in the difference video. Defaults to 1.</p> <code>1</code> <code>video_name</code> <code>str</code> <p>Name of the video. Defaults to \"video\".</p> <code>'video'</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the saved video.</p> Source code in <code>nect/dynamic_export.py</code> <pre><code>def export_video(\n    base_path: str | Path,\n    add_scale_bar: bool = False,\n    acquisition_time_minutes: float | None = None,\n    plot_slice: str = \"XZ\",\n    fps: int = 5,\n    difference: bool = True,\n    difference_revolutions: int = 1,\n    video_name: str = \"video\",\n) -&gt; Path:\n    \"\"\"\n    Exports a video of the dynamic model output. The video will be saved in the base_path directory.\n\n    Args:\n        base_path (str | Path): Path to the directory containing the config.yaml and checkpoints folder.\n        add_scale_bar (bool, optional): Whether to add a scale bar to the video. Defaults to False.\n        acquisition_time_minutes (float, optional): Acquisition time in minutes. Required if add_scale_bar is True. Defaults to None.\n        plot_slice (str, optional): Slice to plot. Must be one of \"XY\", \"XZ\", \"YZ\". Defaults to \"XZ\".\n        fps (int, optional): Frames per second. Defaults to 5.\n        difference (bool, optional): Export a difference video. Defaults to True.\n        difference_revolutions (int, optional): Number of revolutions for the background in the difference video. Defaults to 1.\n        video_name (str, optional): Name of the video. Defaults to \"video\".\n\n    Returns:\n        Path to the saved video.\n    \"\"\"\n    setup_logger()\n    base_path = Path(base_path)\n    with torch.no_grad():  # use torch.no_grad() to disable gradient computation and avoid retaining graph\n        config = get_cfg(base_path / \"config.yaml\")\n        assert config.geometry is not None\n        model = config.get_model()\n        device = torch.device(0)\n        assert config.mode == \"dynamic\", \"Only dynamic mode is supported for video creation\"\n        logger.info(\"Starting to load model\")\n        checkpoints = torch.load(base_path / \"checkpoints\" / \"last.ckpt\", map_location=\"cpu\")\n        video_path = base_path.parent / \"videos\"\n        video_path.mkdir(parents=True, exist_ok=True)\n        model.load_state_dict(checkpoints[\"model\"])\n        model = model.to(device)\n        logger.info(\"Model loading finished\")\n        height, width = config.geometry.nVoxel[0], config.geometry.nVoxel[1]\n        if plot_slice.lower() not in [\"xy\", \"xz\", \"yz\"]:\n            raise ValueError(\"Invalid plot_slice. Must be one of 'XY', 'XZ', 'YZ'\")\n        plot_slice = plot_slice.lower()\n        if plot_slice == \"xy\":\n            height = width\n        z, y, x = torch.meshgrid(\n            [\n                torch.tensor(0.5, device=device)\n                if plot_slice == \"xy\"\n                else torch.linspace(0.0, 1.0, steps=height, device=device),\n                torch.tensor(0.5, device=device)\n                if plot_slice == \"xz\"\n                else torch.linspace(0.0, 1.0, steps=width, device=device),\n                torch.tensor(0.5, device=device)\n                if plot_slice == \"yz\"\n                else torch.linspace(0.0, 1.0, steps=width, device=device),\n            ],\n            indexing=\"ij\",\n        )\n        grid = torch.stack((z.flatten(), y.flatten(), x.flatten())).t()\n        angles = config.geometry.angles\n        projs_per_revolution = get_number_of_projections_per_revolution(angles)\n\n        # avc1 is not available on all systems, so we use mp4v as a fallback. avc1 is preferred because it uses the h.264 codec which can be viewed on most devices\n        if is_fourcc_available(\"avc1\"):\n            fourcc = cv2.VideoWriter_fourcc(*\"avc1\")\n        else:\n            fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n\n        if difference:\n            avg_img = torch.zeros((height, width), device=device)\n            n_percent = projs_per_revolution * difference_revolutions / len(angles)\n            n_steps = 30\n            for t in torch.linspace(0.00, n_percent, n_steps):\n                avg_img += model(grid, t).view(height, width) / n_steps\n            out_diff = cv2.VideoWriter(\n                str(video_path / f\"{video_name}_diff.mp4\"),\n                fourcc,\n                fps,\n                (width, height),\n                isColor=False,\n            )\n            out_merge = cv2.VideoWriter(\n                str(video_path / f\"{video_name}_merge.mp4\"),\n                fourcc,\n                fps,\n                (width * 2, height),\n                isColor=False,\n            )\n        out = cv2.VideoWriter(\n            str(video_path / f\"{video_name}.mp4\"),\n            fourcc,\n            fps,\n            (width, height),\n            isColor=False,\n        )\n\n        for t in tqdm(torch.linspace(0, 1, len(angles))):\n            output: torch.Tensor = model(grid, t).view(height, width)\n            if difference:\n                output_diff = output - avg_img\n            output = output\n            output = output / 3 * 255\n            output = output.clamp(0, 255)\n            output = output.cpu().numpy().astype(np.uint8)\n            output = np.rot90(output, 2)\n            if add_scale_bar:\n                assert (\n                    acquisition_time_minutes is not None\n                ), \"acquisition_time_minutes must be provided if add_scale_bar is True\"\n                output = add_scale_bar_and_text(\n                    frame=output,\n                    time=t.item(),\n                    config=config,\n                    acquisition_time_minutes=acquisition_time_minutes,\n                )\n            out.write(output)\n            if difference:\n                output_diff = output_diff + 4\n                output_diff = output_diff / 6 * 255\n                output_diff = output_diff.clamp(0, 255)\n\n                output_diff = output_diff.cpu().numpy().astype(np.uint8)\n                output_diff = np.rot90(output_diff, 2)\n                if add_scale_bar:\n                    assert (\n                        acquisition_time_minutes is not None\n                    ), \"acquisition_time_minutes must be provided if add_scale_bar is True\"\n                    output_diff = add_scale_bar_and_text(\n                        frame=output_diff,\n                        time=t.item(),\n                        config=config,\n                        acquisition_time_minutes=acquisition_time_minutes,\n                    )\n                # add text to the image in the top left corner saying the time\n\n                out_diff.write(output_diff)\n                out_merge.write(np.concatenate([output, output_diff], axis=1))\n\n        out.release()\n        if difference:\n            out_diff.release()\n            out_merge.release()\n    logger.info(f\"Video saved to {base_path/f'{video_name}.mp4'}\")\n    if difference:\n        logger.info(f\"Difference video saved to {base_path/f'{video_name}_diff.mp4'}\")\n        logger.info(f\"Merged video saved to {base_path/f'{video_name}_merge.mp4'}\")\n    return base_path / f\"{video_name}.mp4\"\n</code></pre>"},{"location":"reference/nect/dynamic_export/#nect.dynamic_export.export_volumes","title":"export_volumes","text":"<pre><code>export_volumes(base_path: str | Path, binning: int = 1, avg_timesteps: int = 1, timesteps_per_revolution: int | Literal['all'] = 'all', export_revolutions: list | str = 'all', show_slices: bool = False, ROIx: list[int] | None = None, ROIy: list[int] | None = None, ROIz: list[int] | None = None, dtype: dtype = np.float32) -&gt; Path\n</code></pre> <p>Exports volumes from the dynamic model output. The volumes will be saved in the base_path/volumes directory.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>str | Path</code> <p>Path to the directory containing the config.yaml and checkpoints folder.</p> required <code>binning</code> <code>int</code> <p>Binning factor. Defaults to 1.</p> <code>1</code> <code>avg_timesteps</code> <code>int</code> <p>Number of timesteps to average together. Defaults to 1.</p> <code>1</code> <code>timesteps_per_revolution</code> <code>int | Literal['all']</code> <p>Number of timesteps to export per revolution. Defaults to \"all\".</p> <code>'all'</code> <code>export_revolutions</code> <code>list | str</code> <p>List of revolutions to export. Defaults to \"all\".</p> <code>'all'</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the saved volumes.</p> Source code in <code>nect/dynamic_export.py</code> <pre><code>def export_volumes(\n    base_path: str | Path,\n    binning: int = 1,\n    avg_timesteps: int = 1,\n    timesteps_per_revolution: int | Literal[\"all\"] = \"all\",\n    export_revolutions: list | str = \"all\",\n    show_slices: bool = False,\n    ROIx: list[int] | None = None,\n    ROIy: list[int] | None = None,\n    ROIz: list[int] | None = None,\n    dtype: np.dtype = np.float32\n) -&gt; Path:\n    \"\"\"\n    Exports volumes from the dynamic model output. The volumes will be saved in the base_path/volumes directory.\n\n    Args:\n        base_path (str | Path): Path to the directory containing the config.yaml and checkpoints folder.\n        binning (int, optional): Binning factor. Defaults to 1.\n        avg_timesteps (int, optional): Number of timesteps to average together. Defaults to 1.\n        timesteps_per_revolution (int | Literal[\"all\"], optional): Number of timesteps to export per revolution. Defaults to \"all\".\n        export_revolutions (list | str, optional): List of revolutions to export. Defaults to \"all\".\n\n    Returns:\n        Path to the saved volumes.\n    \"\"\"\n    setup_logger()\n    base_path = Path(base_path)\n    with torch.no_grad():  # use torch.no_grad() to disable gradient computation and avoid retaining graph\n        config = get_cfg(base_path / \"config.yaml\")\n        assert config.geometry is not None\n        model = config.get_model()\n        dataset = NeCTDataset(\n            config=config,\n            device=\"cpu\",  # if gpu memory is less than 50 GB, load to cpu\n        )\n        geometry = Geometry.from_cfg(\n            config.geometry,\n            reconstruction_mode=config.reconstruction_mode,\n            sample_outside=config.sample_outside,\n        )\n        device = torch.device(0)\n        checkpoints = torch.load(base_path / \"checkpoints\" / \"last.ckpt\", map_location=\"cpu\")\n        model.load_state_dict(checkpoints[\"model\"])\n        model = model.to(device)\n        assert config.mode == \"dynamic\", \"Only dynamic mode is supported for video creation\"\n        height, width = config.geometry.nVoxel[0], config.geometry.nVoxel[1]\n        z_h = height // binning\n        y_w = width // binning\n        x_w = width // binning\n        base_output_path = base_path / \"volumesfloat32\"\n        base_output_path.mkdir(exist_ok=True, parents=True)\n        angles = config.geometry.angles\n        linspace = torch.linspace(0, 1, steps=len(angles), device=device)\n        projs_per_revolution = get_number_of_projections_per_revolution(angles)\n        if export_revolutions == \"all\":\n            export_revolutions = [i for i in range(len(angles) // projs_per_revolution)]\n        if avg_timesteps &gt; 1:\n            logger.info(f\"Averaging {avg_timesteps} timesteps together\")\n        total_volumes_saved = 0\n        nVoxels = config.geometry.nVoxel\n        rm = config.sample_outside\n        nVoxels = [nVoxels[0], nVoxels[1]+2*rm, nVoxels[2]+2*rm]\n        start_x = 0\n        end_x = 1\n        if ROIx is not None:\n            start_x = (ROIx[0] - rm) / nVoxels[2]\n            end_x = (ROIx[1] - rm) / nVoxels[2]\n            x_w = (ROIx[1]-ROIx[0]) // binning\n\n        start_y = 0\n        end_y = 1\n        if ROIy is not None:\n            start_y = (ROIy[0] - rm) / nVoxels[1]\n            end_y = (ROIy[1] - rm) / nVoxels[1]\n            y_w = (ROIy[1]-ROIy[0]) // binning\n\n        start_z = 0\n        end_z = 1\n        if ROIz is not None:\n            start_z = (ROIz[0]) / nVoxels[0]\n            end_z = (ROIz[1]) / nVoxels[0]\n            z_h = (ROIz[1]-ROIz[0]) // binning\n        if show_slices:\n            for slice_idx in [\"z\", \"y\", \"x\"]:\n                if slice_idx == \"z\":\n                    size = (y_w, x_w)\n                elif slice_idx == \"y\":\n                    size = (z_h, x_w)\n                elif slice_idx == \"x\":\n                    size = (z_h, y_w)\n                default_tensor = torch.tensor(0.5, device=device)\n                z_l = torch.linspace(start_z, end_z, steps=z_h, device=device) if slice_idx != \"z\" else default_tensor\n                y_l = torch.linspace(start_y, end_y, steps=y_w, device=device) if slice_idx != \"y\" else default_tensor\n                x_l = torch.linspace(start_x, end_x, steps=x_w, device=device) if slice_idx != \"x\" else default_tensor\n                z, y, x = torch.meshgrid([z_l, y_l, x_l], indexing=\"ij\")\n                grid = torch.stack((z.flatten(), y.flatten(), x.flatten())).t()\n                output = model(grid, torch.tensor(0.5, device=device)).view(size).cpu().numpy()\n                plt.imshow(output, cmap=\"gray\")\n                (base_path / \"imgs\").mkdir(parents=True, exist_ok=True)\n                plt.savefig(base_path / \"imgs\" / f\"{slice_idx}.png\")\n            return base_path / \"imgs\"\n\n        else: \n            for i in tqdm(export_revolutions, leave=True, desc=\"Exporting revolutions\"):\n                revolution_start = i * projs_per_revolution\n                revolution_end = (i + 1) * projs_per_revolution\n                if timesteps_per_revolution != \"all\":\n\n                    sub_linspace = torch.linspace(linspace[revolution_start], linspace[revolution_end], timesteps_per_revolution)  # skip timesteps to speed up export\n                else:\n                    sub_linspace = linspace[revolution_start:revolution_end]\n                for j, t in tqdm(\n                    enumerate(sub_linspace),\n                    total=len(sub_linspace),\n                    leave=False,\n                    desc=\"Projection\",\n                ):\n                    output = torch.zeros((z_h, y_w, x_w), device=device)\n                    for avg in range(avg_timesteps):\n                        for ii, z_ in enumerate(\n                            torch.linspace(start_z, end_z, steps=z_h, device=device)\n                        ):  # progress through as we don't have enough memory to compute all at once\n                            z, y, x = torch.meshgrid(\n                                [\n                                    z_,\n                                    torch.linspace(start_y, end_y, steps=y_w, device=device),\n                                    torch.linspace(start_x, end_x, steps=x_w, device=device),\n                                ],\n                                indexing=\"ij\",\n                            )\n                            grid = torch.stack((z.flatten(), y.flatten(), x.flatten())).t()\n                            output[ii] += model(grid, t + avg/len(angles)).view(y_w, x_w) / avg_timesteps\n                    output = output / geometry.max_distance_traveled\n                    output = output * (dataset.maximum.item() - dataset.minimum.item()) + dataset.minimum.item()\n                    output = output.cpu().numpy()\n                    output = output.astype(dtype)\n                    if timesteps_per_revolution == \"all\":\n                        proj_n = j\n                    else:\n                        proj_n = ((j * projs_per_revolution)//timesteps_per_revolution)\n                    tif.imsave(base_output_path / f\"T{i:04}_{proj_n:04}.tiff\", output)\n                    total_volumes_saved += 1\n            logger.info(f\"{total_volumes_saved} volumes saved to {base_output_path}\")\n            return base_output_path\n</code></pre>"},{"location":"reference/nect/dynamic_export/#nect.dynamic_export.export_volumes_at_timesteps","title":"export_volumes_at_timesteps","text":"<pre><code>export_volumes_at_timesteps(base_path: str | Path, timesteps: list[float], binning: int = 1, ROIx: list[int] | None = None, ROIy: list[int] | None = None, ROIz: list[int] | None = None, dtype: dtype = np.float32) -&gt; Path\n</code></pre> <p>Exports volumes from the dynamic model output. The volumes will be saved in the base_path/volumes directory.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>str | Path</code> <p>Path to the directory containing the config.yaml and checkpoints folder.</p> required <code>timesteps</code> <code>list[float]</code> <p>List of timesteps to export.</p> required <code>binning</code> <code>int</code> <p>Binning factor. Defaults to 1.</p> <code>1</code> <code>ROIx</code> <code>list[int] | None</code> <p>Region of interest in the x direction. Defaults to None.</p> <code>None</code> <code>ROIy</code> <code>list[int] | None</code> <p>Region of interest in the y direction. Defaults to None.</p> <code>None</code> <code>ROIz</code> <code>list[int] | None</code> <p>Region of interest in the z direction. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>dtype</code> <p>Data type of the exported volumes. Defaults to np.float32.</p> <code>float32</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the saved volumes.</p> Source code in <code>nect/dynamic_export.py</code> <pre><code>def export_volumes_at_timesteps(\n    base_path: str | Path,\n    timesteps: list[float],\n    binning: int = 1,\n    ROIx: list[int] | None = None,\n    ROIy: list[int] | None = None,\n    ROIz: list[int] | None = None,\n    dtype: np.dtype = np.float32\n) -&gt; Path:\n    \"\"\"\n    Exports volumes from the dynamic model output. The volumes will be saved in the base_path/volumes directory.\n\n    Args:\n        base_path (str | Path): Path to the directory containing the config.yaml and checkpoints folder.\n        timesteps (list[float]): List of timesteps to export.\n        binning (int, optional): Binning factor. Defaults to 1.\n        ROIx (list[int] | None, optional): Region of interest in the x direction. Defaults to None.\n        ROIy (list[int] | None, optional): Region of interest in the y direction. Defaults to None.\n        ROIz (list[int] | None, optional): Region of interest in the z direction. Defaults to None.\n        dtype (np.dtype, optional): Data type of the exported volumes. Defaults to np.float32.\n\n    Returns:\n        Path to the saved volumes.\n    \"\"\"\n    setup_logger()\n    base_path = Path(base_path)\n    with torch.no_grad():  # use torch.no_grad() to disable gradient computation and avoid retaining graph\n        config = get_cfg(base_path / \"config.yaml\")\n        assert config.geometry is not None\n        model = config.get_model()\n        dataset = NeCTDataset(\n            config=config,\n            device=\"cpu\",  # if gpu memory is less than 50 GB, load to cpu\n        )\n        geometry = Geometry.from_cfg(\n            config.geometry,\n            reconstruction_mode=config.reconstruction_mode,\n            sample_outside=config.sample_outside,\n        )\n        device = torch.device(0)\n        checkpoints = torch.load(base_path / \"checkpoints\" / \"last.ckpt\", map_location=\"cpu\")\n        model.load_state_dict(checkpoints[\"model\"])\n        model = model.to(device)\n        assert config.mode == \"dynamic\", \"Only dynamic mode is supported for video creation\"\n        height, width = config.geometry.nVoxel[0], config.geometry.nVoxel[1]\n        z_h = height // binning\n        y_w = width // binning\n        x_w = width // binning\n        base_output_path = base_path / \"volumesfloat32\"\n        base_output_path.mkdir(exist_ok=True, parents=True)\n        angles = config.geometry.angles\n        linspace = torch.linspace(0, 1, steps=len(angles), device=device)\n        projs_per_revolution = get_number_of_projections_per_revolution(angles)\n        total_volumes_saved = 0\n        nVoxels = config.geometry.nVoxel\n        rm = config.sample_outside\n        nVoxels = [nVoxels[0], nVoxels[1]+2*rm, nVoxels[2]+2*rm]\n        start_x = 0\n        end_x = 1\n        if ROIx is not None:\n            start_x = (ROIx[0] - rm) / nVoxels[2]\n            end_x = (ROIx[1] - rm) / nVoxels[2]\n            x_w = (ROIx[1]-ROIx[0]) // binning\n\n        start_y = 0\n        end_y = 1\n        if ROIy is not None:\n            start_y = (ROIy[0] - rm) / nVoxels[1]\n            end_y = (ROIy[1] - rm) / nVoxels[1]\n            y_w = (ROIy[1]-ROIy[0]) // binning\n\n        start_z = 0\n        end_z = 1\n        if ROIz is not None:\n            start_z = (ROIz[0]) / nVoxels[0]\n            end_z = (ROIz[1]) / nVoxels[0]\n            z_h = (ROIz[1]-ROIz[0]) // binning\n        if max(timesteps) &gt; 1:\n            timesteps = [t/len(angles) for t in timesteps]\n        for t in tqdm(timesteps, leave=True, desc=\"Exporting revolutions\"):\n            output = torch.zeros((z_h, y_w, x_w), device=device)\n            for ii, z_ in enumerate(\n                torch.linspace(start_z, end_z, steps=z_h, device=device)\n            ):  # progress through as we don't have enough memory to compute all at once\n                z, y, x = torch.meshgrid(\n                    [\n                        z_,\n                        torch.linspace(start_y, end_y, steps=y_w, device=device),\n                        torch.linspace(start_x, end_x, steps=x_w, device=device),\n                    ],\n                    indexing=\"ij\",\n                )\n                grid = torch.stack((z.flatten(), y.flatten(), x.flatten())).t()\n                output[ii] = model(grid, t).view(y_w, x_w)\n            output = output / geometry.max_distance_traveled\n            output = output * (dataset.maximum.item() - dataset.minimum.item()) + dataset.minimum.item()\n            output = output.cpu().numpy()\n            output = output.astype(dtype)\n\n            tif.imsave(base_output_path / f\"T{t:04}.tiff\", output)\n            total_volumes_saved += 1\n        logger.info(f\"{total_volumes_saved} volumes saved to {base_output_path}\")\n        return base_output_path\n</code></pre>"},{"location":"reference/nect/fdk/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> fdk","text":""},{"location":"reference/nect/fdk/#nect.fdk","title":"fdk","text":""},{"location":"reference/nect/fdk/#nect.fdk.fdk","title":"fdk","text":"<pre><code>fdk(projections: Tensor | ndarray, geometry: Geometry | Geometry | GeometryCone | Geometry, angles: list[float] | ndarray | Tensor) -&gt; ndarray\n</code></pre> <p>Reconstruct object using FDK</p> <p>Parameters:</p> Name Type Description Default <code>projections</code> <code>(ndarray, Tensor)</code> <p>The projections for reconstruction with shape (nProjections, height, width). The order of the projections must match the order of the angles.</p> required <code>geometry</code> <code>(Geometry, Geometry, GeometryCone, Geometry)</code> <p>Geometry of the system</p> required <code>angles</code> <code>(list[float], ndarray, Tensor)</code> <p>The acqusition angles. Must match the order of the projections.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The reconstructed volume</p> Source code in <code>nect/fdk.py</code> <pre><code>def fdk(\n    projections: torch.Tensor | np.ndarray,\n    geometry: Geometry | ConfigGeometry | ConfigGeometryCone | tigre.geometry.Geometry,\n    angles: list[float] | np.ndarray | torch.Tensor,\n) -&gt; np.ndarray:\n    \"\"\"\n    Reconstruct object using FDK\n\n    Args:\n        projections (np.ndarray, torch.Tensor): The projections for reconstruction with shape (nProjections, height, width). The order of the projections must match the order of the angles.\n        geometry (nect.Geometry, nect.config.Geometry, nect.config.GeometryCone, tigre.geometry.Geometry): Geometry of the system\n        angles (list[float], np.ndarray, torch.Tensor): The acqusition angles. Must match the order of the projections.\n\n    Returns:\n        np.ndarray: The reconstructed volume\n    \"\"\"\n    import tigre\n\n    if isinstance(angles, list):\n        angles = np.array(angles, dtype=np.float32)\n    elif isinstance(angles, torch.Tensor):\n        angles = angles.detach().cpu().numpy().astype(np.float32)\n    argsort_idx = np.argsort(angles)\n    angles = np.copy(angles[argsort_idx])\n    if isinstance(projections, torch.Tensor):\n        projections = projections.detach().cpu().numpy()\n    projs = np.copy(projections[argsort_idx])\n    if isinstance(geometry, (Geometry, ConfigGeometryCone, ConfigGeometry)):\n        geo = tigre_geometry_from_geometry(geometry)\n    elif isinstance(geometry, tigre.geometry.Geometry):\n        geo = geometry\n    else:\n        raise NotImplementedError(\n            f\"geometry must be of type 'nect.config.Geometry', 'nect.config.GeometryCone' or 'tigre.geometry.Geometry', got {type(geometry)}\"\n        )\n    volume = tigre.algorithms.fdk(projs, geo, angles)\n    return volume\n</code></pre>"},{"location":"reference/nect/fdk/#nect.fdk.fdk_from_config","title":"fdk_from_config","text":"<pre><code>fdk_from_config(config: Config, output_directory: str | Path | None = None) -&gt; ndarray\n</code></pre> <p>Reconstruct the object with FDK as a validation step using the config object with the same dataloading as done in NeCT reconstruction.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Config</code> <p>The config object for reconstruction</p> required <code>output_directory</code> <code>(str, Path)</code> <p>Save volume and image slices to the specified folder</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The reconstructed volume</p> Source code in <code>nect/fdk.py</code> <pre><code>def fdk_from_config(config: Config, output_directory: str | Path | None = None) -&gt; np.ndarray:\n    \"\"\"\n    Reconstruct the object with FDK as a validation step using the config object with the same dataloading as done in NeCT reconstruction.\n\n    Args:\n        config (nect.config.Config): The config object for reconstruction\n        output_directory (str, Path, optional): Save volume and image slices to the specified folder\n\n    Returns:\n        np.ndarray: The reconstructed volume\n    \"\"\"\n    import tigre\n\n    dataset = NeCTDataset(config=config, device=\"cpu\")\n    projs = np.zeros((len(dataset), config.geometry.nDetector[0], config.geometry.nDetector[1]), dtype=np.float32)\n    angles = np.zeros((len(dataset)), dtype=np.float32)\n    for i, (proj, angle, _) in tqdm(enumerate(dataset), total=len(dataset), desc=\"Loading projections\"):\n        projs[i] = proj\n        angles[i] = angle\n    geo = tigre_geometry_from_geometry(config.geometry)\n    argsort_idx = np.argsort(angles)\n    angles = np.copy(angles[argsort_idx])\n    projs = np.copy(projs[argsort_idx])\n    volume = tigre.algorithms.fdk(projs, geo, angles)\n    if output_directory is not None:\n        output_directory = Path(output_directory)\n        output_directory.mkdir(parents=True, exist_ok=True)\n        fig, axes = plt.subplot(1, 3)\n        axes[0].imshow(volume[geo.nVoxel[0] // 2])\n        axes[1].imshow(volume[:, geo.nVoxel[1] // 2])\n        axes[2].imshow(volume[:, :, geo.nVoxel[2] // 2])\n        plt.savefig(output_directory / \"slices.png\", dpi=300)\n        if config.save_volume is True:\n            np.save(output_directory / \"volume.npy\", volume)\n    return volume\n</code></pre>"},{"location":"reference/nect/fdk/#nect.fdk.tigre_geometry_from_geometry","title":"tigre_geometry_from_geometry","text":"<pre><code>tigre_geometry_from_geometry(geo: Geometry | Geometry | GeometryCone) -&gt; Geometry\n</code></pre> <p>Create a tigre Geometry form nect geometry.</p> <p>Parameters:</p> Name Type Description Default <code>geo</code> <code>(Geometry, Geometry, GeometryCone)</code> <p>nect geometry to convert to tigre geometry</p> required <p>Returns:</p> Type Description <code>Geometry</code> <p>tigre.geometry.Geometry: The tigre geometry</p> Source code in <code>nect/fdk.py</code> <pre><code>def tigre_geometry_from_geometry(geo: Geometry | ConfigGeometry | ConfigGeometryCone) -&gt; tigre.geometry.Geometry:\n    \"\"\"\n    Create a tigre Geometry form nect geometry.\n\n    Args:\n        geo (nect.Geometry, nect.config.Geometry, nect.config.GeometryCone): nect geometry to convert to tigre geometry\n\n    Returns:\n        tigre.geometry.Geometry: The tigre geometry\n    \"\"\"\n    import tigre\n\n    tigre_geo = tigre.geometry()\n    tigre_geo.DSD = geo.DSD\n    tigre_geo.DSO = geo.DSO\n    tigre_geo.dDetector = np.array(geo.dDetector)\n    tigre_geo.nDetector = np.array(geo.nDetector).astype(np.int32)\n    tigre_geo.sDetector = tigre_geo.dDetector * tigre_geo.nDetector\n    tigre_geo.nVoxel = np.array(geo.nVoxel).astype(np.int32)\n    tigre_geo.dVoxel = np.array(geo.dVoxel)\n    tigre_geo.sVoxel = tigre_geo.nVoxel * tigre_geo.dVoxel\n    tigre_geo.offOrigin = np.array(geo.offOrigin)\n    tigre_geo.offDetector = np.array(geo.offDetector)\n    tigre_geo.accuracy = 0.5\n    tigre_geo.mode = geo.mode\n    tigre_geo.COR = geo.COR\n    tigre_geo.rotDetector = np.array(geo.rotDetector)\n    tigre_geo.filter = None\n    return tigre_geo\n</code></pre>"},{"location":"reference/nect/loss_plotter/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> loss_plotter","text":""},{"location":"reference/nect/loss_plotter/#nect.loss_plotter","title":"loss_plotter","text":""},{"location":"reference/nect/reconstruct/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> reconstruct","text":""},{"location":"reference/nect/reconstruct/#nect.reconstruct","title":"reconstruct","text":""},{"location":"reference/nect/reconstruct/#nect.reconstruct.reconstruct","title":"reconstruct","text":"<pre><code>reconstruct(geometry: Geometry, projections: str | Path | Tensor | ndarray, mode='static', angles: _list | None = None, radians: bool = True, quality: Literal['poor', 'low', 'medium', 'high', 'higher', 'highest'] = 'high', niter: int | None = None, lr: float | None = None, timesteps: _list | None = None, verbose: bool = True, log: bool = False, exp_name: str | None = None, flip_projections: bool = False, channel_order: str | None = None, config_override: dict | None = None, save_ckpt: bool = True, static_init: str | None = None, static_init_config: str | None = None) -&gt; ndarray | Path\n</code></pre> <p>Create a 3D or 4D-CT reconstruction from a set of 2D projections.</p> <p>Parameters:</p> Name Type Description Default <code>geometry</code> <code>Geometry</code> <p>Geometry object containing the acquisition geometry.</p> required <code>projections</code> <code>str | Path | Tensor | ndarray</code> <p>Path to the projections file or folder or the projections themselves.</p> required <code>mode</code> <code>Literal['static', 'dynamic']</code> <p>\"dynamic\" for 4D-CT reconstruction and \"static\" for 3D.</p> <code>'static'</code> <code>angles</code> <code>list | ndarray | Tensor | None</code> <p>Projection angles in radians. If None, the angles from the geometry object are used.</p> <code>None</code> <code>radians</code> <code>bool</code> <p>If angles are provided, this defines the angle type either \"radians\" (<code>True</code>) or \"degrees\" (<code>False</code>). If angles are None, it is ignored. Defaults to <code>True</code> (radians).</p> <code>True</code> <code>quality</code> <code>Literal['poor', 'low', 'medium', 'high', 'higher', 'highest']</code> <p>Time used for reconstructing and getting approximately the given quality</p> <code>'high'</code> <code>niter</code> <code>int | str | None</code> <p>Override the number of iterations from quality.</p> <code>None</code> <code>lr</code> <code>float | None</code> <p>Learning rate. Default depends on reconstruction type.</p> <code>None</code> <code>timesteps</code> <code>list | ndarray | Tensor | None</code> <p>An array of timesteps. Do not need to be normalized. If the order of the angles and corresponding projections does not equal the acqustition order, this parameter needs to be set to get the timesteps correct. Only important for dynamic reconstruction. Overrides the timestep of the Geometry if not None.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Verbosity. Default is True.</p> <code>True</code> <code>flip_projections</code> <code>bool</code> <p>Flip the projections. Default is False.</p> <code>False</code> <code>channel_order</code> <code>str | None</code> <p>Channel order. This is only used if the projections are a path to file (\"NHW\", \"NWH\") or files (\"HW\" or \"WH\").</p> <code>None</code> <code>config_override</code> <code>dict | None</code> <p>Dictionary containing the configuration that overrides all the other arguments.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray | Path</code> <p>The path to the reconstruction directory if mode is dynamic. If static, a np.ndarray is returned.</p> Source code in <code>nect/reconstruct.py</code> <pre><code>def reconstruct(\n    geometry: Geometry,\n    projections: str | Path | torch.Tensor | np.ndarray,\n    mode=\"static\",\n    angles: _list | None = None,\n    radians: bool = True,\n    quality: Literal[\"poor\", \"low\", \"medium\", \"high\", \"higher\", \"highest\"] = \"high\",\n    niter: int | None = None,\n    lr: float | None = None,\n    timesteps: _list | None = None,\n    verbose: bool = True,\n    log: bool = False,\n    exp_name: str | None = None,\n    flip_projections: bool = False,\n    channel_order: str | None = None,\n    config_override: dict | None = None,\n    save_ckpt: bool = True,\n    static_init: str | None = None, \n    static_init_config: str | None = None, \n) -&gt; np.ndarray | Path:\n    \"\"\"\n    Create a 3D or 4D-CT reconstruction from a set of 2D projections.\n\n    Args:\n        geometry (Geometry): Geometry object containing the acquisition geometry.\n        projections (str | Path | torch.Tensor | np.ndarray): Path to the projections file or folder or the projections themselves.\n        mode (Literal[\"static\", \"dynamic\"]): \"dynamic\" for 4D-CT reconstruction and \"static\" for 3D.\n        angles (list | np.ndarray | torch.Tensor | None): Projection angles in radians. If None, the angles from the geometry object are used.\n        radians (bool): If angles are provided, this defines the angle type either \"radians\" (`True`) or \"degrees\" (`False`). If angles are None, it is ignored. Defaults to `True` (radians).\n        quality (Literal[\"poor\", \"low\", \"medium\", \"high\", \"higher\", \"highest\"]): Time used for reconstructing and getting approximately the given quality\n        niter (int | str | None): Override the number of iterations from quality.\n        lr (float | None): Learning rate. Default depends on reconstruction type.\n        timesteps (list | np.ndarray | torch.Tensor | None): An array of timesteps. Do not need to be normalized.\n            If the order of the angles and corresponding projections does not equal the acqustition order, this parameter needs to be set to get the timesteps correct.\n            Only important for dynamic reconstruction. Overrides the timestep of the Geometry if not None.\n        verbose (bool): Verbosity. Default is True.\n        flip_projections (bool): Flip the projections. Default is False.\n        channel_order (str | None): Channel order. This is only used if the projections are a path to file (\"NHW\", \"NWH\") or files (\"HW\" or \"WH\").\n        config_override (dict | None): Dictionary containing the configuration that overrides all the other arguments.\n\n    Returns:\n        The path to the reconstruction directory if mode is dynamic. If static, a np.ndarray is returned.\n    \"\"\"\n    logger.remove()\n    logger.add(sys.stdout, colorize=True, format=\"&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss}&lt;/green&gt; | &lt;level&gt;{level}&lt;/level&gt; | &lt;level&gt;{message}&lt;/level&gt;\", level=\"INFO\" if verbose else \"WARNING\",)\n\n    if mode == \"static\":\n        cfg = get_static_cfg(name=\"hash_grid\")\n        cfg[\"model\"] = \"hash_grid\"\n    elif mode == \"dynamic\":\n        cfg = get_dynamic_cfg(name=\"quadcubes\")\n        cfg[\"model\"] = \"quadcubes\"\n\n    if channel_order is not None:\n        cfg[\"channel_order\"] = channel_order\n\n    cfg[\"flip\"] = flip_projections\n    if isinstance(projections, (str, Path)):\n        cfg[\"img_path\"] = projections\n    else:\n        cfg[\"img_path\"] = \"RECONSTRUCTING_FROM_ARRAY\"\n\n    if quality in [\"poor\", \"low\"]:\n        cfg[\"loss\"] = \"L2\"\n        cfg[\"encoder\"][\"log2_hashmap_size\"] = 19\n        if quality == \"poor\":\n            cfg[\"epochs\"] = \"0.02x\"\n            cfg[\"base_lr\"] *= 10\n        elif quality == \"low\":\n            cfg[\"epochs\"] = \"0.1x\"\n            cfg[\"base_lr\"] *= 5\n\n    elif quality == \"medium\":\n        cfg[\"epochs\"] = \"0.3x\"\n        cfg[\"base_lr\"] *= 2\n\n    elif quality == \"high\":\n        cfg[\"epochs\"] = \"1x\"\n        cfg[\"base_lr\"] /= 2\n        cfg[\"warmup\"][\"steps\"] = 500\n\n    elif quality in [\"higher\", \"highest\"]:\n#        cfg[\"net\"][\"n_neurons\"] = 64\n#        cfg[\"net\"][\"n_hidden_layers\"] = 6\n        if quality == \"higher\":\n            cfg[\"epochs\"] = \"2x\"\n            cfg[\"base_lr\"] /= 2\n            cfg[\"warmup\"][\"steps\"] = 3000\n            cfg[\"lr_scheduler\"][\"lrf\"] = 0.05\n            cfg[\"points_per_ray\"][\"end\"] = \"1.5x\"\n        elif quality == \"highest\":\n            cfg[\"epochs\"] = \"4x\"\n            cfg[\"base_lr\"] /= 5\n            cfg[\"warmup\"][\"steps\"] = 5000\n            cfg[\"lr_scheduler\"][\"lrf\"] = 0.01\n            cfg[\"points_per_ray\"][\"end\"] = \"1.5x\"\n            cfg[\"encoder\"][\"log2_hashmap_size\"] = 23\n\n    if niter is not None:\n        cfg[\"epochs\"] = niter\n\n    if lr is not None:\n        cfg[\"base_lr\"] = lr\n\n    if angles is not None:\n        geometry.set_angles(angles, radians)\n\n    elif geometry.angles is None:\n        raise ValueError(\"angles must be provided, either as an argument or in the `Geometry` object.\")\n\n    if timesteps is not None:\n        geometry.set_timesteps(timesteps)\n\n    if config_override is not None:\n        cfg.update(config_override)\n\n    cfg[\"geometry\"] = geometry.to_dict()\n    config = setup_cfg(cfg)\n\n    if exp_name is None:\n        log_path = Path(\"outputs\")\n    else:\n        log_path = Path(\"outputs\") / exp_name\n\n    (log_path).mkdir(parents=True, exist_ok=True)\n    config.save(log_path)\n\n    #if mode == \"dynamic\":\n    log = True\n\n    if static_init is not None:\n        from nect.trainers.initrainer import IniTrainer\n        trainer = IniTrainer(\n            config=config,\n            output_directory=log_path if log else None,\n            init_mode=\"hash_to_quadcubes\",\n            static_init=static_init,\n            static_init_config=static_init_config,\n            save_ckpt=save_ckpt,\n            save_last=True if mode == \"static\" else True,\n            save_optimizer=True,\n            verbose=verbose,\n            log=log,\n            prune=False,\n        )\n    else:\n        if isinstance(projections, (str, Path)):\n            trainer = BaseTrainer(\n                config=config,\n                output_directory=log_path if log else None,\n                save_ckpt=save_ckpt,\n                save_last= True,#False if mode == \"static\" else True,\n                save_optimizer=True, #False,\n                verbose=verbose,\n                log=log,\n                prune=False,\n            )\n        else:\n            trainer = ProjectionsLoadedTrainer(\n                config=config,\n                projections=projections,\n                output_directory=log_path if log else None,\n                save_ckpt=save_ckpt,\n                save_last=True, #False if mode == \"static\" else True,\n                save_optimizer=True, #False,\n                verbose=verbose,\n                log=log,\n            )\n\n    trainer.fit()\n    if mode == \"static\":\n        return torch.rot90(cast(torch.Tensor, trainer.create_volume(save=False, cpu=True)), 2, (1, 2)).cpu().numpy(), trainer.get_outputdir()\n    else:\n        return Path(trainer.checkpoint_directory_base).parent, trainer.get_outputdir()\n</code></pre>"},{"location":"reference/nect/reconstruct/#nect.reconstruct.reconstruct_from_config_file","title":"reconstruct_from_config_file","text":"<pre><code>reconstruct_from_config_file(cfg: str | Path, log_path: str = 'outputs', save_ckpt: bool = True, checkpoint: str | None = None, save_volume: bool = False, save_last: bool = True, save_optimizer: bool = True, cancel_at: str | None = None, prune: bool = True, keep_two: bool = True)\n</code></pre> <p>Create a 3D or 4D-CT reconstruction from a set of 2D projections from config file.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>str | Path</code> <p>Path to the configuration file.</p> required <code>log_path</code> <code>str</code> <p>Base path to the output directory.</p> <code>'outputs'</code> <code>save_ckpt</code> <code>bool</code> <p>Save the checkpoint during reconstruction.</p> <code>True</code> <code>checkpoint</code> <code>str | None</code> <p>Load a checkpoint file to continue training.</p> <code>None</code> <code>save_volume</code> <code>bool</code> <p>Save the final volume to a file.</p> <code>False</code> <code>save_last</code> <code>bool</code> <p>Save the last checkpoint.</p> <code>True</code> <code>save_optimizer</code> <code>bool</code> <p>Save the optimizer state.</p> <code>True</code> <code>cancel_at</code> <code>str</code> <p>Cancel training at the specified ISO-datetime. Save the model before canceling.</p> <code>None</code> <code>prune</code> <code>bool</code> <p>Prune the model to remove the optimizer.</p> <code>True</code> Source code in <code>nect/reconstruct.py</code> <pre><code>def reconstruct_from_config_file(\n    cfg: str | Path,\n    log_path: str = \"outputs\",\n    save_ckpt: bool = True,\n    checkpoint: str | None = None,\n    save_volume: bool = False,\n    save_last: bool = True,\n    save_optimizer: bool = True,\n    cancel_at: str | None = None,\n    prune: bool = True,\n    keep_two: bool = True\n):\n    \"\"\"\n    Create a 3D or 4D-CT reconstruction from a set of 2D projections from config file.\n\n    Args:\n        cfg (str | Path): Path to the configuration file.\n        log_path (str): Base path to the output directory.\n        save_ckpt (bool): Save the checkpoint during reconstruction.\n        checkpoint (str | None): Load a checkpoint file to continue training.\n        save_volume (bool): Save the final volume to a file.\n        save_last (bool): Save the last checkpoint.\n        save_optimizer (bool): Save the optimizer state.\n        cancel_at (str, optional): Cancel training at the specified ISO-datetime. Save the model before canceling.\n        prune (bool): Prune the model to remove the optimizer.\n    \"\"\"\n    config_file_path_splitted = str(cfg).split(os.sep)\n    config = get_cfg(cfg)\n\n    output_folder = os.path.join(*config_file_path_splitted[-2:-1])\n    exp_name = os.path.join(log_path, output_folder, config.mode)\n    if checkpoint:\n        exp_name = Path(checkpoint).parent.parent.parent\n\n    trainer = BaseTrainer\n\n    if config.evaluation is not None:\n        if config.evaluation.gt_path_mode.upper() == \"SCIVIS\":\n            from nect.trainers.scivis_trainer import SciVisTrainer\n            trainer = SciVisTrainer\n\n        elif config.evaluation.gt_path_mode.upper() == \"POROUSMEDIUM\":\n            from nect.trainers.porous_medium_trainer import PorousMediumTrainer\n            trainer = PorousMediumTrainer\n\n    if config.continous_scanning is True:\n        from nect.trainers.continous_scanning_trainer import ContinousScanningTrainer\n        trainer = ContinousScanningTrainer\n\n    trainer = trainer(\n        config=config,\n        checkpoint=checkpoint,\n        output_directory=exp_name,\n        save_ckpt=save_ckpt,\n        save_last=save_last,\n        save_optimizer=save_optimizer,\n        verbose=True,\n        cancel_at=cancel_at,\n        prune=prune,\n        keep_two=keep_two\n    )\n    if save_volume:\n        trainer.save_volume()\n    else:\n        trainer.fit()\n</code></pre>"},{"location":"reference/nect/scivis_data/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> scivis_data","text":""},{"location":"reference/nect/scivis_data/#nect.scivis_data","title":"scivis_data","text":""},{"location":"reference/nect/scivis_data/#nect.scivis_data.SciVisDataset","title":"SciVisDataset","text":"<pre><code>SciVisDataset(base_path: Path = Path(PROJECTION_BASE_PATH_STATIC), dataset: str = 'Teapot', scale: bool = False)\n</code></pre> <p>We read the data from https://klacansky.com/open-scivis-datasets/category-ct.html. Their format is (depth, height, width). We reshape it to (width, height, depth), then transpose it to (height, width, depth). Sometimes the data from open scivis is flipped upside down. Then we flip it the right way.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>Path</code> <p>The base path to the dataset. Defaults to Path(PROJECTION_BASE_PATH_STATIC).</p> <code>Path(PROJECTION_BASE_PATH_STATIC)</code> <code>dataset</code> <code>str</code> <p>The dataset to load. Defaults to \"Teapot\".</p> <code>'Teapot'</code> <code>scale</code> <code>bool</code> <p>Whether to scale the data between 0 and 1. Defaults to False.</p> <code>False</code> Source code in <code>nect/scivis_data.py</code> <pre><code>def __init__(\n    self,\n    base_path: Path = Path(PROJECTION_BASE_PATH_STATIC),\n    dataset: str = \"Teapot\",\n    scale: bool = False,\n):\n    r\"\"\"We read the data from https://klacansky.com/open-scivis-datasets/category-ct.html. Their format is (depth, height, width).\n    We reshape it to (width, height, depth), then transpose it to (height, width, depth). Sometimes the data from open scivis is flipped\n    upside down. Then we flip it the right way.\n\n    Args:\n        base_path (Path, optional): The base path to the dataset. Defaults to Path(PROJECTION_BASE_PATH_STATIC).\n        dataset (str, optional): The dataset to load. Defaults to \"Teapot\".\n        scale (bool, optional): Whether to scale the data between 0 and 1. Defaults to False.\n    \"\"\"\n\n    dataset = dataset.lower().capitalize()\n    self.base_path = base_path\n    self.dataset_name = dataset\n    self.cfg = load_config(self.base_path / dataset / \"GT\" / \"config.yaml\")\n    self.gt_dtype = np.dtype(self.cfg[\"dtype\"])\n    self.gt = (\n        np.fromfile(\n            self.base_path / self.dataset_name / \"GT\" / self.cfg[\"file_name\"],\n            dtype=self.gt_dtype,\n        )\n        .reshape(tuple(self.cfg[\"size\"]))\n        .transpose((1, 0, 2))\n    )\n    self.gt = self.gt.astype(np.float32)\n    self.geo = None\n    self.angles = None\n    if scale:\n        print(\"Scaling\")\n        maximum = np.max(self.gt)\n        minimum = np.min(self.gt)\n        self.gt = (self.gt - minimum) / (maximum - minimum)\n        print(\"Finished scaling\")\n    self.flip_z = self.cfg.get(\"flip_z\")\n\n    if not self.cfg.get(\"flip_z\"):\n        self.gt = np.flipud(self.gt)\n</code></pre>"},{"location":"reference/nect/scivis_data/#nect.scivis_data.SciVisDataset.generate_projections","title":"generate_projections","text":"<pre><code>generate_projections(method: str = 'tigre', nangles: int = 49, save: bool = False, detector_mag: float = 1.0, scale: bool = True, radians: bool = True) -&gt; ndarray\n</code></pre> <p>Generate the projections using the specified method.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The method to use. Defaults to \"tigre\". Supported methods are \"tigre\" and \"leap\".</p> <code>'tigre'</code> <code>nangles</code> <code>int</code> <p>The number of angles. Defaults to 49.</p> <code>49</code> <code>save</code> <code>bool</code> <p>Whether to save the projections. Defaults to False.</p> <code>False</code> <code>detector_mag</code> <code>float</code> <p>The detector magnification. Defaults to 1.0.</p> <code>1.0</code> <code>scale</code> <code>bool</code> <p>Whether to scale the data between 0 and 1. Defaults to True.</p> <code>True</code> <code>radians</code> <code>bool</code> <p>Whether the angles are in radians. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The projections.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented.</p> Source code in <code>nect/scivis_data.py</code> <pre><code>def generate_projections(\n    self,\n    method: str = \"tigre\",\n    nangles: int = 49,\n    save: bool = False,\n    detector_mag: float = 1.0,\n    scale: bool = True,\n    radians: bool = True,\n) -&gt; np.ndarray:\n    \"\"\"Generate the projections using the specified method.\n\n    Args:\n        method (str, optional): The method to use. Defaults to \"tigre\". Supported methods are \"tigre\" and \"leap\".\n        nangles (int, optional): The number of angles. Defaults to 49.\n        save (bool, optional): Whether to save the projections. Defaults to False.\n        detector_mag (float, optional): The detector magnification. Defaults to 1.0.\n        scale (bool, optional): Whether to scale the data between 0 and 1. Defaults to True.\n        radians (bool, optional): Whether the angles are in radians. Defaults to True.\n\n    Returns:\n        np.ndarray: The projections.\n\n    Raises:\n        NotImplementedError: If the method is not implemented.\n    \"\"\"\n    if method == \"tigre\":\n        print(\"Generating projections using tigre\")\n        return self._generate_projections_tigre(\n            nangles=nangles,\n            save=save,\n            detector_mag=detector_mag,\n            scale=scale,\n            radians=radians,\n        )\n    elif method == \"leap\":\n        print(\"Generating projections using leap\")\n        return self._generate_projections_leap(nangles=nangles, save=save, detector_mag=detector_mag, scale=scale)\n    else:\n        raise NotImplementedError(\n            f\"The method {method} is not implemented. Supported methods are 'tigre' and 'leap'.\"\n        )\n</code></pre>"},{"location":"reference/nect/scivis_data/#nect.scivis_data.SciVisDataset.get_scaled_gt","title":"get_scaled_gt","text":"<pre><code>get_scaled_gt() -&gt; ndarray\n</code></pre> <p>Get the scaled ground truth.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The scaled ground truth.</p> Source code in <code>nect/scivis_data.py</code> <pre><code>def get_scaled_gt(self) -&gt; np.ndarray:\n    \"\"\"Get the scaled ground truth.\n\n    Returns:\n        np.ndarray: The scaled ground truth.\"\"\"\n    return (self.gt - np.min(self.gt)) / (np.max(self.gt) - np.min(self.gt))\n</code></pre>"},{"location":"reference/nect/scivis_data/#nect.scivis_data.SciVisDataset.print_all_datasets","title":"print_all_datasets","text":"<pre><code>print_all_datasets()\n</code></pre> <p>Print all the datasets in the base path.</p> Source code in <code>nect/scivis_data.py</code> <pre><code>def print_all_datasets(self):\n    \"\"\"Print all the datasets in the base path.\"\"\"\n    for dataset in os.listdir(self.base_path):\n        cfg = load_config(self.base_path / dataset / \"GT\" / \"config.yaml\")\n        print(f\"Name: {dataset} Size: {str(cfg['size'])}\")\n</code></pre>"},{"location":"reference/nect/scivis_data/#nect.scivis_data.SciVisDataset.remove_projections","title":"remove_projections","text":"<pre><code>remove_projections(nangles_to_remove: int, dataset_to_remove: str = None)\n</code></pre> <p>Helper function to quickly remove one or all projections with a specific number of angles.</p> <p>Parameters:</p> Name Type Description Default <code>nangles_to_remove</code> <code>int</code> <p>Marks the projections with this number of angles to be removed.</p> required <code>dataset_to_remove</code> <code>str</code> <p>If provided, only this projections will be removed. Defaults to None.</p> <code>None</code> Source code in <code>nect/scivis_data.py</code> <pre><code>def remove_projections(self, nangles_to_remove: int, dataset_to_remove: str = None):\n    \"\"\"Helper function to quickly remove one or all projections with a specific number of angles.\n\n    Args:\n        nangles_to_remove (int): Marks the projections with this number of angles to be removed.\n        dataset_to_remove (str, optional): If provided, only this projections will be removed. Defaults to None.\n    \"\"\"\n    if dataset_to_remove is not None:\n        dataset_to_remove = dataset_to_remove.lower().capitalize()\n        shutil.rmtree(self.base_path / dataset_to_remove / f\"sinogram_nangles_{nangles_to_remove}\")\n    else:\n        for dataset_to_remove in os.listdir(self.base_path):\n            path = self.base_path / dataset_to_remove / f\"sinogram_nangles_{nangles_to_remove}\"\n            if path.exists():\n                shutil.rmtree(path)\n            else:\n                continue\n</code></pre>"},{"location":"reference/nect/scivis_data/#nect.scivis_data.SciVisDataset.save_as_quadrants","title":"save_as_quadrants","text":"<pre><code>save_as_quadrants()\n</code></pre> <p>Save the ground truth as 8 quadrants. Used for large datasets to save memory</p> Source code in <code>nect/scivis_data.py</code> <pre><code>def save_as_quadrants(self):\n    \"\"\"Save the ground truth as 8 quadrants. Used for large datasets to save memory\"\"\"\n    # save the gt into 8 quadrants\n    for i in range(2):\n        for j in range(2):\n            for k in range(2):\n                quad = self.gt[\n                    i * self.gt.shape[0] // 2 : (i + 1) * self.gt.shape[0] // 2,\n                    j * self.gt.shape[1] // 2 : (j + 1) * self.gt.shape[1] // 2,\n                    k * self.gt.shape[2] // 2 : (k + 1) * self.gt.shape[2] // 2,\n                ]\n                print(\"Finished indexing\")\n                np.save(\n                    self.base_path / self.dataset_name / \"GT/quadrants\" / f\"quadrant_z_{i}_y_{j}_x_{k}.npy\",\n                    quad,\n                )\n                print(f\"Finished {i} {j} {k}\")\n</code></pre>"},{"location":"reference/nect/scivis_data/#nect.scivis_data.SciVisDataset.scale_0_to_1","title":"scale_0_to_1","text":"<pre><code>scale_0_to_1()\n</code></pre> <p>Scales the ground truth between 0 and 1.</p> Source code in <code>nect/scivis_data.py</code> <pre><code>def scale_0_to_1(self):\n    \"\"\"Scales the ground truth between 0 and 1.\"\"\"\n    self.gt = (self.gt - np.min(self.gt)) / (np.max(self.gt) - np.min(self.gt))\n</code></pre>"},{"location":"reference/nect/scivis_data/#nect.scivis_data.geo_to_yaml","title":"geo_to_yaml","text":"<pre><code>geo_to_yaml(geo, angles, path: Path, radians: bool)\n</code></pre> <p>Save the geometry and angles to a yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>geo</code> <code>geometry</code> <p>The geometry object.</p> required <code>angles</code> <code>ndarray</code> <p>The angles.</p> required <code>path</code> <code>Path</code> <p>The path to save the yaml file.</p> required <code>radians</code> <code>bool</code> <p>Whether the angles are in radians.</p> required Source code in <code>nect/scivis_data.py</code> <pre><code>def geo_to_yaml(geo, angles, path: Path, radians: bool):\n    \"\"\"Save the geometry and angles to a yaml file.\n\n    Args:\n        geo (tigre.geometry): The geometry object.\n        angles (np.ndarray): The angles.\n        path (Path): The path to save the yaml file.\n        radians (bool): Whether the angles are in radians.\n    \"\"\"\n    data = {\n        \"DSD\": geo.DSD,\n        \"DSO\": geo.DSO,\n        \"nDetector\": geo.nDetector.tolist(),\n        \"dDetector\": geo.dDetector.tolist(),\n        \"sDetector\": geo.sDetector.tolist(),\n        \"nVoxel\": geo.nVoxel.tolist(),\n        \"sVoxel\": geo.sVoxel.tolist(),\n        \"dVoxel\": geo.dVoxel.tolist(),\n        \"offOrigin\": geo.offOrigin.tolist(),\n        \"offDetector\": geo.offDetector.tolist(),\n        \"rotDetector\": geo.rotDetector.tolist(),\n        \"COR\": geo.COR,\n        \"accuracy\": geo.accuracy,\n        \"mode\": geo.mode,\n        \"filter\": geo.filter,\n        \"radians\": radians,\n        \"angles\": angles.tolist(),\n    }\n\n    with open(path, \"w\") as file:\n        for key, value in data.items():\n            # Write each key-value pair without indentation\n            file.write(f\"{key}: \")\n            if isinstance(value, list):\n                # If value is a list, write it in square brackets without indentation\n                file.write(\"[\")\n                file.write(\", \".join([str(item) for item in value]))\n                file.write(\"]\\n\")\n            else:\n                # If value is not a list, write it directly followed by a newline\n                file.write(f\"{value}\\n\")\n</code></pre>"},{"location":"reference/nect/scivis_data/#nect.scivis_data.str_dtype_to_np_dtype","title":"str_dtype_to_np_dtype","text":"<pre><code>str_dtype_to_np_dtype(dtype: str) -&gt; dtype\n</code></pre> <p>Convert a string dtype to a numpy dtype.</p> <p>Parameters:</p> Name Type Description Default <code>dtype</code> <code>str</code> <p>The string dtype.</p> required <p>Returns:</p> Type Description <code>dtype</code> <p>np.dtype: The numpy dtype.</p> Source code in <code>nect/scivis_data.py</code> <pre><code>def str_dtype_to_np_dtype(dtype: str) -&gt; np.dtype:\n    \"\"\"Convert a string dtype to a numpy dtype.\n\n    Args:\n        dtype (str): The string dtype.\n\n    Returns:\n        np.dtype: The numpy dtype.\n    \"\"\"\n    if \"int8\" in dtype:\n        return np.dtype(dtype)\n    elif \"int\" in dtype:\n        return np.dtype(f\"&lt;{dtype}\")\n    else:\n        raise NotImplementedError(f\"The dtype {dtype} is not implemented yet.\")\n</code></pre>"},{"location":"reference/nect/scivis_data/#nect.scivis_data.yaml_to_geo","title":"yaml_to_geo","text":"<pre><code>yaml_to_geo(path: Path) -&gt; Geometry\n</code></pre> <p>Load the geometry from a yaml file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the yaml file.</p> required <p>Returns:</p> Type Description <code>Geometry</code> <p>tigre.geometry.Geometry: The geometry object.</p> Source code in <code>nect/scivis_data.py</code> <pre><code>def yaml_to_geo(path: Path) -&gt; tigre.geometry.Geometry:\n    \"\"\"Load the geometry from a yaml file.\n\n    Args:\n        path (Path): The path to the yaml file.\n\n    Returns:\n        tigre.geometry.Geometry: The geometry object.\n    \"\"\"\n    import tigre\n\n    with open(path, \"r\") as f:\n        data = yaml.safe_load(f)\n    geo = tigre.geometry()\n    geo.DSD = data[\"DSD\"]\n    geo.DSO = data[\"DSO\"]\n    geo.nDetector = np.array(data[\"nDetector\"]).astype(np.int32)\n    geo.dDetector = np.array(data[\"dDetector\"])\n    geo.sDetector = np.array(data[\"sDetector\"])\n    geo.nVoxel = np.array(data[\"nVoxel\"]).astype(np.int32)\n    geo.sVoxel = np.array(data[\"sVoxel\"])\n    geo.dVoxel = np.array(data[\"dVoxel\"])\n    geo.offOrigin = np.array(data[\"offOrigin\"])\n    geo.offDetector = np.array(data[\"offDetector\"])\n    geo.rotDetector = np.array(data[\"rotDetector\"])\n    geo.COR = data[\"COR\"]\n    geo.accuracy = data[\"accuracy\"]\n    geo.mode = data[\"mode\"]\n    geo.filter = data[\"filter\"]\n    return geo\n</code></pre>"},{"location":"reference/nect/static_export/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> static_export","text":""},{"location":"reference/nect/static_export/#nect.static_export","title":"static_export","text":""},{"location":"reference/nect/static_export/#nect.static_export.export_volume","title":"export_volume","text":"<pre><code>export_volume(base_path: str | Path, binning: int = 1, show_slices: bool = False, ROIx: list[int] | None = None, ROIy: list[int] | None = None, ROIz: list[int] | None = None) -&gt; Path\n</code></pre> <p>Exports volume from the static model output. The volume will be saved in the base_path/volumes directory.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>str | Path</code> <p>Path to the directory containing the config.yaml and checkpoints folder.</p> required <code>binning</code> <code>int</code> <p>Binning factor. Defaults to 1.</p> <code>1</code> <code>show_slices</code> <code>bool</code> <p>If True, will show slices of the volume instead of saving it. Defaults to False.</p> <code>False</code> <code>ROIx</code> <code>list[int] | None</code> <p>Region of interest in x direction. Defaults to None. If None, the ROI will be the full range.</p> <code>None</code> <code>ROIy</code> <code>list[int] | None</code> <p>Region of interest in y direction. Defaults to None. If None, the ROI will be the full range.</p> <code>None</code> <code>ROIz</code> <code>list[int] | None</code> <p>Region of interest in z direction. Defaults to None. If None, the ROI will be the full range.</p> <code>None</code> <p>Returns:</p> Type Description <code>Path</code> <p>Path to the saved volumes.</p> Source code in <code>nect/static_export.py</code> <pre><code>def export_volume(\n    base_path: str | Path,\n    binning: int = 1,\n    show_slices: bool = False,\n    ROIx: list[int] | None = None,\n    ROIy: list[int] | None = None,\n    ROIz: list[int] | None = None,\n) -&gt; Path:\n    \"\"\"\n    Exports volume from the static model output. The volume will be saved in the base_path/volumes directory.\n\n    Args:\n        base_path (str | Path): Path to the directory containing the config.yaml and checkpoints folder.\n        binning (int, optional): Binning factor. Defaults to 1.\n        show_slices (bool, optional): If True, will show slices of the volume instead of saving it. Defaults to False.\n        ROIx (list[int] | None, optional): Region of interest in x direction. Defaults to None. If None, the ROI will be the full range.\n        ROIy (list[int] | None, optional): Region of interest in y direction. Defaults to None. If None, the ROI will be the full range.\n        ROIz (list[int] | None, optional): Region of interest in z direction. Defaults to None. If None, the ROI will be the full range.\n\n    Returns:\n        Path to the saved volumes.\n    \"\"\"\n    setup_logger()\n    base_path = Path(base_path)\n    with torch.no_grad():  # use torch.no_grad() to disable gradient computation and avoid retaining graph\n        config = get_cfg(base_path / \"config.yaml\")\n        assert config.geometry is not None\n        model = config.get_model()\n        dataset = NeCTDataset(\n            config=config,\n            device=\"cpu\",  # if gpu memory is less than 50 GB, load to cpu\n        )\n        geometry = Geometry.from_cfg(\n            config.geometry,\n            reconstruction_mode=config.reconstruction_mode,\n            sample_outside=config.sample_outside,\n        )\n        device = torch.device(0)\n        checkpoints = torch.load(base_path / \"checkpoints\" / \"last.ckpt\", map_location=\"cpu\")\n        model.load_state_dict(checkpoints[\"model\"])\n        model = model.to(device)\n        height, width = config.geometry.nVoxel[0], config.geometry.nVoxel[1]\n        z_h = height // binning\n        y_w = width // binning\n        x_w = width // binning\n        base_output_path = base_path / \"volumefloat32\"\n        base_output_path.mkdir(exist_ok=True, parents=True)\n        total_volumes_saved = 0\n        nVoxels = config.geometry.nVoxel\n        rm = config.sample_outside\n        nVoxels = [nVoxels[0], nVoxels[1]+2*rm, nVoxels[2]+2*rm]\n        start_x = 0\n        end_x = 1\n        if ROIx is not None:\n            start_x = (ROIx[0] - rm) / nVoxels[2]\n            end_x = (ROIx[1] - rm) / nVoxels[2]\n            x_w = (ROIx[1]-ROIx[0]) // binning\n\n        start_y = 0\n        end_y = 1\n        if ROIy is not None:\n            start_y = (ROIy[0] - rm) / nVoxels[1]\n            end_y = (ROIy[1] - rm) / nVoxels[1]\n            y_w = (ROIy[1]-ROIy[0]) // binning\n\n        start_z = 0\n        end_z = 1\n        if ROIz is not None:\n            start_z = (ROIz[0]) / nVoxels[0]\n            end_z = (ROIz[1]) / nVoxels[0]\n            z_h = (ROIz[1]-ROIz[0]) // binning\n        if show_slices:\n            for slice_idx in [\"z\", \"y\", \"x\"]:\n                if slice_idx == \"z\":\n                    size = (y_w, x_w)\n                elif slice_idx == \"y\":\n                    size = (z_h, x_w)\n                elif slice_idx == \"x\":\n                    size = (z_h, y_w)\n                default_tensor = torch.tensor(0.5, device=device)\n                z_l = torch.linspace(start_z, end_z, steps=z_h, device=device) if slice_idx != \"z\" else default_tensor\n                y_l = torch.linspace(start_y, end_y, steps=y_w, device=device) if slice_idx != \"y\" else default_tensor\n                x_l = torch.linspace(start_x, end_x, steps=x_w, device=device) if slice_idx != \"x\" else default_tensor\n                z, y, x = torch.meshgrid([z_l, y_l, x_l], indexing=\"ij\")\n                grid = torch.stack((z.flatten(), y.flatten(), x.flatten()))\n                output = model(grid, torch.tensor(0.5, device=device)).view(size).cpu().numpy()\n                plt.imshow(output, cmap=\"gray\")\n                (base_path / \"imgs\").mkdir(parents=True, exist_ok=True)\n                plt.savefig(base_path / \"imgs\" / f\"{slice_idx}.png\")\n            return base_path / \"imgs\"\n\n        else: \n            output = torch.zeros((z_h, y_w, x_w), device=\"cpu\", dtype=torch.float32)\n            output = output.flatten()\n            z_lin = torch.linspace(start_z, end_z, steps=z_h, dtype=torch.float32, device=\"cpu\")\n            y_lin = torch.linspace(start_y, end_y, steps=y_w, dtype=torch.float32, device=\"cpu\")\n            x_lin = torch.linspace(start_x, end_x, steps=x_w, dtype=torch.float32, device=\"cpu\")\n\n            batch_size = 5_000_000\n\n            # Calculate total number of points\n            total_points = z_h * y_w * x_w\n\n            # Create a tensor of indices\n            indices = torch.arange(total_points, dtype=torch.int64)\n\n            # Split indices into batches\n            batches = torch.split(indices, batch_size)\n\n            # Process each batch\n            for batch in tqdm(batches):\n                # Calculate the z, y, x coordinates using vectorized operations\n                z_indices = batch // (y_w * x_w)\n                y_indices = (batch % (y_w * x_w)) // x_w\n                x_indices = batch % x_w\n\n                z = z_lin[z_indices]\n                y = y_lin[y_indices]\n                x = x_lin[x_indices]\n\n                grid = torch.stack((z, y, x), dim=1).cuda()                \n                batch_output = model(grid).flatten().float().detach().cpu()\n                output.view(-1)[batch] = batch_output\n            output = output.view((z_h, y_w, x_w))\n            output = output / geometry.max_distance_traveled\n            output = output * (dataset.maximum.item() - dataset.minimum.item()) + dataset.minimum.item()       \n            tif.imsave(base_output_path / f\"Static_Volume.tiff\", output.numpy())\n            total_volumes_saved += 1\n            logger.info(f\"{total_volumes_saved} volumes saved to {base_output_path}\")\n            return base_output_path\n</code></pre>"},{"location":"reference/nect/utils/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> utils","text":""},{"location":"reference/nect/utils/#nect.utils","title":"utils","text":""},{"location":"reference/nect/utils/#nect.utils.create_sub_folders","title":"create_sub_folders","text":"<pre><code>create_sub_folders(output_directory) -&gt; tuple[str, str]\n</code></pre> <p>Create the sub-folders for the output directory.</p> <p>Parameters:</p> Name Type Description Default <code>output_directory</code> <code>str</code> <p>The output directory.</p> required <p>Returns:</p> Type Description <code>tuple[str, str]</code> <p>Tuple[str, str]: The checkpoint and image directories.</p> Source code in <code>nect/utils.py</code> <pre><code>def create_sub_folders(output_directory) -&gt; tuple[str, str]:\n    \"\"\"Create the sub-folders for the output directory.\n\n    Args:\n        output_directory (str): The output directory.\n\n    Returns:\n        Tuple[str, str]: The checkpoint and image directories.\n    \"\"\"\n    logger.info(f\"Using output directory: {output_directory}\")\n    checkpoint_directory = os.path.join(output_directory, \"model\", \"checkpoints\")\n    if not os.path.exists(checkpoint_directory):\n        os.makedirs(checkpoint_directory)\n    image_directory = os.path.join(output_directory, \"images\")\n    if not os.path.exists(image_directory):\n        os.makedirs(image_directory)\n    return checkpoint_directory, image_directory\n</code></pre>"},{"location":"reference/nect/utils/#nect.utils.is_fourcc_available","title":"is_fourcc_available","text":"<pre><code>is_fourcc_available(codec) -&gt; bool\n</code></pre> <p>Check if the codec is available for video writing.</p> <p>Parameters:</p> Name Type Description Default <code>codec</code> <code>str</code> <p>Codec to check.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the codec is available, False otherwise.</p> Source code in <code>nect/utils.py</code> <pre><code>def is_fourcc_available(codec) -&gt; bool:\n    \"\"\"\n    Check if the codec is available for video writing.\n\n    Args:\n        codec (str): Codec to check.\n\n    Returns:\n        True if the codec is available, False otherwise.\n    \"\"\"\n    try:\n        fourcc = cv2.VideoWriter_fourcc(*codec)\n        temp_video = cv2.VideoWriter(\"temp.mp4\", fourcc, 30, (640, 480), isColor=False)\n        is_open = temp_video.isOpened()\n        temp_video.release()\n        os.remove(\"temp.mp4\")\n        return is_open\n    except Exception as e:\n        logger.warning(e)\n        return False\n</code></pre>"},{"location":"reference/nect/utils/#nect.utils.load_config","title":"load_config","text":"<pre><code>load_config(config) -&gt; dict\n</code></pre> <p>Load the configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>str</code> <p>The path to the configuration file.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The loaded configuration.</p> Source code in <code>nect/utils.py</code> <pre><code>def load_config(config) -&gt; dict:\n    \"\"\"Load the configuration file.\n\n    Args:\n        config (str): The path to the configuration file.\n\n    Returns:\n        dict: The loaded configuration.\n    \"\"\"\n    with open(config, \"r\") as f:\n        return yaml.safe_load(f)\n</code></pre>"},{"location":"reference/nect/utils/#nect.utils.prune_from_path","title":"prune_from_path","text":"<pre><code>prune_from_path(base_path: str | Path)\n</code></pre> <p>Prunes model to remove optimizer.</p> <p>Parameters:</p> Name Type Description Default <code>base_path</code> <code>str | Path</code> <p>Path to the directory containing the config.yaml and checkpoints folder.</p> required Source code in <code>nect/utils.py</code> <pre><code>def prune_from_path(base_path: str | Path):\n    \"\"\"\n    Prunes model to remove optimizer.\n\n    Args:\n        base_path (str | Path): Path to the directory containing the config.yaml and checkpoints folder.\n\n    \"\"\"\n    base_path = Path(base_path)\n    config = get_cfg(base_path / \"config.yaml\")\n    assert config.geometry is not None\n    setup_logger()\n    logger.info(\"LOADING CHECKPOINT. This might take a while...\")\n    checkpoints = torch.load(base_path / \"checkpoints\" / \"last.ckpt\", map_location=\"cpu\")\n    state = {\n        \"model\": checkpoints[\"model\"],\n    }\n    pruned_path = base_path.parent / \"pruned\"\n    model_path = pruned_path / \"checkpoints\" / \"last.ckpt\"\n    model_path.parent.mkdir(parents=True, exist_ok=True)\n    logger.info(\"FINISHED LOADING. Starting to export pruned model...\")\n\n    torch.save(state, model_path)\n    shutil.copy(base_path / \"config.yaml\", pruned_path / \"config.yaml\")\n    shutil.copy(base_path / \"geometry.yaml\", pruned_path / \"geometry.yaml\")\n    logger.info(f\"Pruned model saved to {pruned_path}\")\n</code></pre>"},{"location":"reference/nect/utils/#nect.utils.prune_model","title":"prune_model","text":"<pre><code>prune_model(model: Module, base_path: str | Path)\n</code></pre> <p>Save a pruned version of the model (no optimizer state) along with config and geometry.</p> Source code in <code>nect/utils.py</code> <pre><code>def prune_model(model: torch.nn.Module, base_path: str | Path):\n    \"\"\"\n    Save a pruned version of the model (no optimizer state) along with config and geometry.\n    \"\"\"\n    base_path = Path(base_path)\n    pruned_path = base_path.parent / \"pruned\"\n    model_path = pruned_path / \"checkpoints\" / \"last.ckpt\"\n    model_path.parent.mkdir(parents=True, exist_ok=True)\n\n    # Save model weights only\n    state = {\"model\": model.state_dict()}\n    torch.save(state, model_path)\n\n    # Copy config + geometry from model subdir\n    model_dir = base_path / \"model\"\n    for fname in [\"config.yaml\", \"geometry.yaml\"]:\n        src = model_dir / fname\n        dst = pruned_path / fname\n        if not src.exists():\n            raise FileNotFoundError(f\"Could not find {src} when pruning model\")\n        dst.parent.mkdir(parents=True, exist_ok=True)\n        shutil.copy(src, dst)\n</code></pre>"},{"location":"reference/nect/utils/#nect.utils.setup_logger","title":"setup_logger","text":"<pre><code>setup_logger(level=logging.INFO)\n</code></pre> <p>Set up the logger.</p> <p>Parameters:</p> Name Type Description Default <code>level</code> <code>int</code> <p>The logging level. Defaults to logging.INFO.</p> <code>INFO</code> Source code in <code>nect/utils.py</code> <pre><code>def setup_logger(level=logging.INFO):\n    \"\"\"Set up the logger.\n\n    Args:\n        level (int, optional): The logging level. Defaults to logging.INFO.\n    \"\"\"\n    logger.remove()\n    logger.add(\n        sys.stdout,\n        colorize=True,\n        format=\"&lt;green&gt;{time:YYYY-MM-DD HH:mm:ss}&lt;/green&gt; | &lt;level&gt;{level}&lt;/level&gt; | &lt;level&gt;{message}&lt;/level&gt;\",\n        level=level,\n    )\n</code></pre>"},{"location":"reference/nect/utils/#nect.utils.total_variation_3d","title":"total_variation_3d","text":"<pre><code>total_variation_3d(input, weight: float = 1.0) -&gt; Tensor\n</code></pre> <p>Calculate the total variation in 3D.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>The input tensor. It may have an arbitrary number of dimensions.</p> required <code>weight</code> <code>float</code> <p>The weight for the total variation. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The total variation.</p> Source code in <code>nect/utils.py</code> <pre><code>def total_variation_3d(input, weight: float = 1.0) -&gt; torch.Tensor:\n    \"\"\"Calculate the total variation in 3D.\n\n    Args:\n        input (torch.Tensor): The input tensor. It may have an arbitrary number of dimensions.\n        weight (float, optional): The weight for the total variation. Defaults to 1.0.\n\n    Returns:\n        torch.Tensor: The total variation.\n    \"\"\"\n    diff_i = input[..., 1:, :, :] - input[..., :-1, :, :]\n    diff_j = input[..., :, 1:, :] - input[..., :, :-1, :]\n    diff_k = input[..., :, :, 1:] - input[..., :, :, :-1]\n    tv = (torch.sum(torch.abs(diff_i)) + torch.sum(torch.abs(diff_j)) + torch.sum(torch.abs(diff_k))) / (\n        input.size(0) * input.size(1) * input.size(2)\n    )\n    return tv * weight\n</code></pre>"},{"location":"reference/nect/cfg/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> cfg","text":""},{"location":"reference/nect/cfg/#nect.cfg","title":"cfg","text":""},{"location":"reference/nect/network/","title":"Index","text":""},{"location":"reference/nect/network/#nect.network","title":"network","text":""},{"location":"reference/nect/network/kplanes/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> kplanes","text":""},{"location":"reference/nect/network/kplanes/#nect.network.kplanes","title":"kplanes","text":""},{"location":"reference/nect/network/networks/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> networks","text":""},{"location":"reference/nect/network/networks/#nect.network.networks","title":"networks","text":""},{"location":"reference/nect/sampling/","title":"Index","text":""},{"location":"reference/nect/sampling/#nect.sampling","title":"sampling","text":""},{"location":"reference/nect/sampling/#nect.sampling.Geometry","title":"Geometry","text":"<pre><code>Geometry(nDetector: _list_2_int, dDetector: _list_2_float, mode: str, DSD: float | None = None, DSO: float | None = None, nVoxel: _list_3_int | None = None, dVoxel: _list_3_float | None = None, radius: float | None = None, height: float | None = None, offOrigin: _list_3_float = (0.0, 0.0, 0.0), COR: float = 0.0, offDetector: _list_2_float = (0.0, 0.0), rotDetector: _list_3_float = (0.0, 0.0, 0.0), reconstruction_mode: str = 'voxel', detector_binning: int = 1, angles: _list_float | None = None, radians: bool = True, timesteps: _list | None = None)\n</code></pre> <p>Set up the geometry for the CT system.</p> <p>Parameters:</p> Name Type Description Default <code>nDetector</code> <code>tuple[int, int] | list[int] | Tensor | ndarray</code> <p>Number of pixels <code>[height, width]</code> of the detector</p> required <code>dDetector</code> <code>tuple[float, float] | list[float] | Tensor | ndarray</code> <p>Height and width of the detector (mm)</p> required <code>mode</code> <code>str</code> <p>Type of geometry. Supported modes are <code>cone</code> and <code>parallel</code></p> required <code>DSD</code> <code>float</code> <p>Distance Source Detector (mm)</p> <code>None</code> <code>DSO</code> <code>float</code> <p>Distance Source Origin (mm)</p> <code>None</code> <code>nVoxel</code> <code>tuple[int, int, int] | list[int] | Tensor | ndarray</code> <p>Number of voxels <code>[z, y, x]</code> of the volume</p> <code>None</code> <code>dVoxel</code> <code>tuple[float, float, float] | list[float] | Tensor | ndarray</code> <p>Size of a voxel <code>[z, y, x]</code> (mm)</p> <code>None</code> <code>radius</code> <code>float</code> <p>Radius of the object (mm)</p> <code>None</code> <code>height</code> <code>float</code> <p>Height of the object (mm)</p> <code>None</code> <code>offOrigin</code> <code>tuple[float, float, float] | list[float] | Tensor | ndarray</code> <p>Offset of the object from the origin <code>[z, y, x]</code> (mm)</p> <code>(0.0, 0.0, 0.0)</code> <code>COR</code> <code>float</code> <p>Center of rotation (mm)</p> <code>0.0</code> <code>offDetector</code> <code>tuple[float, float] | list[float] | Tensor | ndarray</code> <p>Offset of the detector from the center <code>[height, width]</code> (mm)</p> <code>(0.0, 0.0)</code> <code>rotDetector</code> <code>tuple[float, float, float] | list[float] | Tensor | ndarray</code> <p>Rotation of the detector <code>[roll, pitch, yaw]</code> (radians).</p> <code>(0.0, 0.0, 0.0)</code> <code>reconstruction_mode</code> <code>str</code> <p>Type of reconstruction. Supported modes are <code>'voxel'</code> and <code>'cylindrical'</code>. Default is <code>'voxel'</code></p> <code>'voxel'</code> <code>detector_binning</code> <code>int</code> <p>Binning factor of the detector. Default is 1</p> <code>1</code> <code>angles</code> <code>list[float] | Tensor | ndarray | None</code> <p>List of angles.</p> <code>None</code> <code>radians</code> <code>bool</code> <p>Unit of angles. If <code>True</code>, the unit is radians, if <code>False</code> the unit is degrees. Default is <code>True</code></p> <code>True</code> <code>timesteps</code> <code>list[float | int] | Tensor | ndarray | None</code> <p>An array of timesteps. Do not need to be normalized. If the order of the angles and corresponding projections does not equal the acqustition order, this parameter needs to be set to get the timesteps correct. Only important for dynamic reconstruction. Overrides the timestep of the Geometry if not <code>None</code>.</p> <code>None</code> Source code in <code>nect/sampling/geometry.py</code> <pre><code>def __init__(\n    self,\n    nDetector: _list_2_int,\n    dDetector: _list_2_float,\n    mode: str,\n    DSD: float | None = None,\n    DSO: float | None = None,\n    nVoxel: _list_3_int | None = None,\n    dVoxel: _list_3_float | None = None,\n    radius: float | None = None,\n    height: float | None = None,\n    offOrigin: _list_3_float = (0.0, 0.0, 0.0),\n    COR: float = 0.0,\n    offDetector: _list_2_float = (0.0, 0.0),\n    rotDetector: _list_3_float = (0.0, 0.0, 0.0),\n    reconstruction_mode: str = \"voxel\",\n    detector_binning: int = 1,\n    angles: _list_float | None = None,\n    radians: bool = True,\n    timesteps: _list | None = None,\n):\n    \"\"\"\n    Set up the geometry for the CT system.\n\n    Args:\n        nDetector (tuple[int, int] | list[int] | torch.Tensor | np.ndarray):\n            Number of pixels `[height, width]` of the detector\n        dDetector (tuple[float, float] | list[float] | torch.Tensor | np.ndarray):\n            Height and width of the detector (mm)\n        mode (str):\n            Type of geometry. Supported modes are `cone` and `parallel`\n        DSD (float):\n            Distance Source Detector (mm)\n        DSO (float):\n            Distance Source Origin (mm)\n        nVoxel (tuple[int, int, int] | list[int] | torch.Tensor | np.ndarray):\n            Number of voxels `[z, y, x]` of the volume\n        dVoxel (tuple[float, float, float] | list[float] | torch.Tensor | np.ndarray):\n            Size of a voxel `[z, y, x]` (mm)\n        radius (float):\n            Radius of the object (mm)\n        height (float):\n            Height of the object (mm)\n        offOrigin (tuple[float, float, float] | list[float] | torch.Tensor | np.ndarray):\n            Offset of the object from the origin `[z, y, x]` (mm)\n        COR (float):\n            Center of rotation (mm)\n        offDetector (tuple[float, float] | list[float] | torch.Tensor | np.ndarray):\n            Offset of the detector from the center `[height, width]` (mm)\n        rotDetector (tuple[float, float, float] | list[float] | torch.Tensor | np.ndarray):\n            Rotation of the detector `[roll, pitch, yaw]` (radians).\n        reconstruction_mode (str):\n            Type of reconstruction. Supported modes are `'voxel'` and `'cylindrical'`. Default is `'voxel'`\n        detector_binning (int):\n            Binning factor of the detector. Default is 1\n        angles (list[float] | torch.Tensor | np.ndarray | None):\n            List of angles.\n        radians (bool):\n            Unit of angles. If `True`, the unit is radians, if `False` the unit is degrees. Default is `True`\n        timesteps (list[float | int] | torch.Tensor | np.ndarray | None):\n            An array of timesteps. Do not need to be normalized.\n            If the order of the angles and corresponding projections does not equal the acqustition order, this parameter needs to be set to get the timesteps correct.\n            Only important for dynamic reconstruction. Overrides the timestep of the Geometry if not `None`.\n    \"\"\"\n    if mode not in [\"cone\", \"parallel\"]:\n        raise ValueError(f\"Unsupported mode '{mode}'\")\n    if mode == \"cone\":\n        if DSD is None:\n            raise ValueError(\"DSD is required for cone geometry\")\n        if DSO is None:\n            raise ValueError(\"DSO is required for cone geometry\")\n        self.DSD = float(DSD)\n        self.DSO = float(DSO)\n    else:\n        if DSD is not None:\n            warn(\"DSD is not required for parallel geometry. Ignoring the value of DSD\")\n        if DSO is not None:\n            warn(\"DSO is not required for parallel geometry. Ignoring the value of DSO\")\n        self.DSD = None\n        self.DSO = None\n    if reconstruction_mode not in [\"voxel\", \"cylindrical\"]:\n        raise ValueError(f\"Unsupported reconstruction mode '{reconstruction_mode}'\")\n    self.mode = mode\n    self.reconstruction_mode = reconstruction_mode\n    self.detector_binning = detector_binning\n    self.dDetector = cast(_tuple_2_float, self._check_and_return_float(dDetector, 2, \"dDetector\"))\n    self.nDetector = cast(_tuple_2_int, self._check_and_return_int(nDetector, 2, \"nDetector\"))\n    self.offDetector = cast(_tuple_2_float, self._check_and_return_float(offDetector, 2, \"offDetector\"))\n    self.rotDetector = cast(_tuple_3_float, self._check_and_return_float(rotDetector, 3, \"rotDetector\"))\n    self.COR = COR\n    self.set_angles(angles, radians)\n    self.set_timesteps(timesteps)\n    if radius is None or height is None:\n        self.nVoxel = cast(_tuple_3_int, self._check_and_return_int(nVoxel, 3, \"nVoxel\"))\n        self.dVoxel = cast(_tuple_3_float, self._check_and_return_float(dVoxel, 3, \"dVoxel\"))\n        self.sVoxel = (\n            self.nVoxel[0] * self.dVoxel[0],\n            self.nVoxel[1] * self.dVoxel[1],\n            self.nVoxel[2] * self.dVoxel[2],\n        )\n        self.offOrigin = cast(_tuple_3_float, self._check_and_return_float(offOrigin, 3, \"offOrigin\"))\n        if self.reconstruction_mode == \"cylindrical\":\n            if radius is None:\n                self.radius = max(self.sVoxel[1], self.sVoxel[2]) / 2\n            if height is None:\n                self.height = self.sVoxel[0]\n        else:\n            self.radius = None\n            self.height = None\n    else:\n        self.radius = radius\n        self.height = height\n        if nVoxel is not None:\n            self.nVoxel = cast(_tuple_3_int, self._check_and_return_int(nVoxel, 3, \"nVoxel\"))\n        else:\n            self.nVoxel = (self.nDetector[0], self.nDetector[1], self.nDetector[1])\n        if dVoxel is not None:\n            self.dVoxel = cast(_tuple_3_float, self._check_and_return_float(dVoxel, 3, \"dVoxel\"))\n        else:\n            self.dVoxel = (\n                self.nDetector[0] / height,\n                self.nDetector[1] / (2 * radius),\n                self.nDetector[1] / (2 * radius),\n            )\n        self.sVoxel = (\n            self.nVoxel[0] * self.dVoxel[0],\n            self.nVoxel[1] * self.dVoxel[1],\n            self.nVoxel[2] * self.dVoxel[2],\n        )\n    if self.mode == \"cone\":\n        if (nVoxel is None or dVoxel is None) and reconstruction_mode == \"cylindrical\":\n            max_length = self.radius * 2\n            triangle_theta = np.arctan((self.height / 2) / (self.DSO + max_length / 2))\n            self.max_distance_traveled = max_length / np.cos(triangle_theta)\n        else:\n            max_length = ((self.sVoxel[1] - self.dVoxel[1]) ** 2 + (self.sVoxel[2] - self.dVoxel[2]) ** 2) ** 0.5\n            triangle_theta = np.arctan(((self.sVoxel[0] - self.dVoxel[0]) / 2) / (self.DSO + max_length / 2))\n            self.max_distance_traveled = max_length / np.cos(triangle_theta)\n    else:\n        if self.reconstruction_mode == \"voxel\":\n            self.max_distance_traveled = (\n                (self.sVoxel[1] - self.dVoxel[1]) ** 2 + (self.sVoxel[2] - self.dVoxel[2]) ** 2\n            ) ** 0.5\n        elif self.reconstruction_mode == \"cylindrical\":\n            if nVoxel is None or dVoxel is None:\n                self.max_distance_traveled = self.radius * 2\n            else:\n                self.max_distance_traveled = max(self.sVoxel[1], self.sVoxel[2])\n    self.sDetector = (\n        self.nDetector[0] * self.dDetector[0],\n        self.nDetector[1] * self.dDetector[1],\n    )\n</code></pre>"},{"location":"reference/nect/sampling/#nect.sampling.Geometry.from_cfg","title":"from_cfg  <code>classmethod</code>","text":"<pre><code>from_cfg(cfg: GeometryCone | Geometry, reconstruction_mode: str = 'voxel', sample_outside: int = 0) -&gt; 'Geometry'\n</code></pre> <p>Load the geometry from a configuration object.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>GeometryCone | Geometry</code> <p>The configuration object.</p> required <code>reconstruction_mode</code> <code>str</code> <p>The reconstruction mode. Default is <code>'voxel'</code>.</p> <code>'voxel'</code> <code>sample_outside</code> <code>int</code> <p>The number of voxels to sample outside the object. Default is <code>0</code>.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>Geometry</code> <code>'Geometry'</code> <p>The geometry object.</p> Source code in <code>nect/sampling/geometry.py</code> <pre><code>@classmethod\ndef from_cfg(\n    cls,\n    cfg: nect.config.GeometryCone | nect.config.Geometry,\n    reconstruction_mode: str = \"voxel\",\n    sample_outside: int = 0,\n) -&gt; \"Geometry\":\n    \"\"\"\n    Load the geometry from a configuration object.\n\n    Args:\n        cfg (nect.config.GeometryCone | nect.config.Geometry): The configuration object.\n        reconstruction_mode (str): The reconstruction mode. Default is `'voxel'`.\n        sample_outside (int): The number of voxels to sample outside the object. Default is `0`.\n\n    Returns:\n        Geometry: The geometry object.\n    \"\"\"\n    nVoxel = cfg.nVoxel\n    if sample_outside &gt; 0:\n        nVoxel = [s + 2 * sample_outside for s in nVoxel]\n    return cls(\n        nDetector=cfg.nDetector,\n        dDetector=cfg.dDetector,\n        mode=cfg.mode,\n        DSD=cfg.DSD if hasattr(cfg, \"DSD\") else None,\n        DSO=cfg.DSO if hasattr(cfg, \"DSO\") else None,\n        nVoxel=nVoxel,\n        dVoxel=cfg.dVoxel,\n        radius=cfg.radius,\n        height=cfg.height,\n        offOrigin=cfg.offOrigin,\n        COR=cfg.COR,\n        offDetector=cfg.offDetector,\n        rotDetector=cfg.rotDetector if cfg.rotDetector is not None else (0.0, 0.0, 0.0),\n        reconstruction_mode=reconstruction_mode,\n        detector_binning=1,\n        angles=cfg.angles,\n        timesteps=cfg.timesteps,\n    )\n</code></pre>"},{"location":"reference/nect/sampling/#nect.sampling.Geometry.from_yaml","title":"from_yaml  <code>classmethod</code>","text":"<pre><code>from_yaml(path: str | Path, reconstruction_mode: str | None = None) -&gt; 'Geometry'\n</code></pre> <p>Load the geometry from a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>The path to the YAML file.</p> required <code>reconstruction_mode</code> <code>str | None</code> <p>The reconstruction mode. Supported strings are <code>'voxel'</code> and <code>'cylindrical'</code>. Default is <code>None</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Geometry</code> <code>'Geometry'</code> <p>The geometry object.</p> Source code in <code>nect/sampling/geometry.py</code> <pre><code>@classmethod\ndef from_yaml(cls, path: str | Path, reconstruction_mode: str | None = None) -&gt; \"Geometry\":\n    \"\"\"\n    Load the geometry from a YAML file.\n\n    Args:\n        path (str | Path): The path to the YAML file.\n        reconstruction_mode (str | None): The reconstruction mode. Supported strings are `'voxel'` and `'cylindrical'`. Default is `None`.\n\n    Returns:\n        Geometry: The geometry object.\"\"\"\n    with open(path, \"r\") as f:\n        cfg = yaml.safe_load(f)\n    if reconstruction_mode is None:\n        reconstruction_mode = cfg.get(\"reconstruction_mode\", \"voxel\")\n    elif reconstruction_mode not in (\"voxel\", \"cylindrical\"):\n        raise NotImplementedError(\n            f\"Only reconstruction mode 'voxel' and 'cylindrical' is implemented, got '{reconstruction_mode}'\"\n        )\n    return cls(\n        nDetector=cfg[\"nDetector\"],\n        dDetector=cfg[\"dDetector\"],\n        mode=cfg[\"mode\"],\n        DSD=cfg.get(\"DSD\", None),\n        DSO=cfg.get(\"DSO\", None),\n        nVoxel=cfg.get(\"nVoxel\", None),\n        dVoxel=cfg.get(\"dVoxel\", None),\n        radius=cfg.get(\"radius\", None),\n        height=cfg.get(\"height\", None),\n        offOrigin=cfg.get(\"offOrigin\", (0.0, 0.0, 0.0)),\n        COR=cfg.get(\"COR\", 0.0),\n        offDetector=cfg.get(\"offDetector\", (0.0, 0.0)),\n        rotDetector=cfg.get(\"rotDetector\", (0.0, 0.0, 0.0)),\n        reconstruction_mode=reconstruction_mode,\n        detector_binning=1,\n        angles=cfg.get(\"angles\", None),\n        timesteps=cfg.get(\"timesteps\", None),\n        radians=cfg.get(\"radians\", True),\n    )\n</code></pre>"},{"location":"reference/nect/sampling/#nect.sampling.Geometry.set_angles","title":"set_angles","text":"<pre><code>set_angles(angles: _list_float | None, radians: bool)\n</code></pre> <p>Set the angles for the geometry.</p> <p>Parameters:</p> Name Type Description Default <code>angles</code> <code>list[float] | Tensor | ndarray | None</code> <p>List of angles.</p> required <code>radians</code> <code>bool</code> <p>Unit of angles. If <code>True</code>, the unit is radians, if <code>False</code> the unit is degrees. Default is <code>True</code></p> required Source code in <code>nect/sampling/geometry.py</code> <pre><code>def set_angles(self, angles: _list_float | None, radians: bool):\n    \"\"\"\n    Set the angles for the geometry.\n\n    Args:\n        angles (list[float] | torch.Tensor | np.ndarray | None):\n            List of angles.\n        radians (bool):\n            Unit of angles. If `True`, the unit is radians, if `False` the unit is degrees. Default is `True`\"\"\"\n    self.angles = angles\n    if self.angles is not None:\n        if not isinstance(self.angles, np.ndarray):\n            if isinstance(self.angles, list):\n                self.angles = np.array(self.angles)\n            elif isinstance(self.angles, torch.Tensor):\n                self.angles = self.angles.cpu().numpy()\n        if radians is False:\n            self.angles = np.radians(self.angles)\n</code></pre>"},{"location":"reference/nect/sampling/#nect.sampling.Geometry.set_detector_binning","title":"set_detector_binning","text":"<pre><code>set_detector_binning(detector_binning: int)\n</code></pre> <p>Set the detector binning factor.</p> <p>Parameters:</p> Name Type Description Default <code>detector_binning</code> <code>int</code> <p>The binning factor of the detector</p> required Source code in <code>nect/sampling/geometry.py</code> <pre><code>def set_detector_binning(self, detector_binning: int):\n    \"\"\"\n    Set the detector binning factor.\n\n    Args:\n        detector_binning (int): The binning factor of the detector\"\"\"\n    self.detector_binning = detector_binning\n</code></pre>"},{"location":"reference/nect/sampling/#nect.sampling.Geometry.set_timesteps","title":"set_timesteps","text":"<pre><code>set_timesteps(timesteps: _list | None)\n</code></pre> <p>Set the timesteps for dynamic reconstruction.</p> <p>Parameters:</p> Name Type Description Default <code>timesteps</code> <code>list[float | int] | Tensor | ndarray | None</code> <p>An array of timesteps. Do not need to be normalized. If the order of the angles and corresponding projections does not equal the acqustition order, this parameter needs to be set to get the timesteps correct. Only important for dynamic reconstruction. Overrides the timestep of the Geometry if not <code>None</code>.</p> required Source code in <code>nect/sampling/geometry.py</code> <pre><code>def set_timesteps(self, timesteps: _list | None):\n    \"\"\"\n    Set the timesteps for dynamic reconstruction.\n\n    Args:\n        timesteps (list[float | int] | torch.Tensor | np.ndarray | None):\n            An array of timesteps. Do not need to be normalized.\n            If the order of the angles and corresponding projections does not equal the acqustition order, this parameter needs to be set to get the timesteps correct.\n            Only important for dynamic reconstruction. Overrides the timestep of the Geometry if not `None`.\n    \"\"\"\n    self.timesteps = timesteps\n</code></pre>"},{"location":"reference/nect/sampling/#nect.sampling.Projector","title":"Projector","text":"<pre><code>Projector(geometry: Geometry, points_per_batch: int, points_per_ray: int, device: device | str | int, random_offset_detector: float = 0.0, uniform_ray_spacing: bool = True)\n</code></pre> Source code in <code>nect/sampling/projector.py</code> <pre><code>def __init__(\n    self,\n    geometry: Geometry,\n    points_per_batch: int,\n    points_per_ray: int,\n    device: torch.device | str | int,\n    random_offset_detector: float = 0.0,\n    uniform_ray_spacing: bool = True,\n):\n    self.device = _check_and_return_cuda_device(device)\n    self.points_per_batch = points_per_batch\n    self.points_per_ray = points_per_ray\n    if points_per_ray &lt; 1:\n        raise ValueError(f\"points_per_ray ({points_per_ray}) must be greater than or equal to 1\")\n    if points_per_batch &lt; points_per_ray:\n        raise ValueError(\n            f\"points_per_batch ({points_per_batch}) must be greater than or equal to points_per_ray ({points_per_ray})\"\n        )\n    self.geometry = geometry\n    self.random_offset_detector = random_offset_detector\n    self.uniform_ray_spacing = uniform_ray_spacing\n    self.c_geometry = self.geometry.get_c_geometry()\n</code></pre>"},{"location":"reference/nect/sampling/#nect.sampling.Projector.__call__","title":"__call__","text":"<pre><code>__call__(batch_num: int, proj: Tensor)\n</code></pre> <p>Sample points along the rays, and return the points and the corresponding projection values.</p> Source code in <code>nect/sampling/projector.py</code> <pre><code>def __call__(self, batch_num: int, proj: torch.Tensor):\n    \"\"\"\n    Sample points along the rays, and return the points and the corresponding projection values.\n    \"\"\"\n    starting_ray_index = batch_num * self.batch_size\n    batch_size = self.batch_size\n    if (batch_num + 1) * batch_size &gt; self.random_indexes.size(0):\n        if batch_num == 0:\n            batch_size = self.random_indexes.size(0) % batch_size\n        else:\n            return None, None\n    ray_points, distances = nect.sampling.ct_sampling.sample(\n        random_ray_index=self.random_indexes,\n        geometry=self.c_geometry,\n        angle_rad=self.angle,\n        num_points_per_ray=self.points_per_ray,\n        num_rays=batch_size,\n        starting_ray_index=starting_ray_index,\n        max_ray_distance_per_point=self.distance_between_points,\n        uniform_ray_spacing=self.uniform_ray_spacing,\n        # random_detector_offset=False,\n        random_detector_offset=self.random_offset_detector,\n        device=self.device.index,\n    )\n    self.distances = distances\n    ray_points.clamp_(min=0, max=1)\n    return (\n        ray_points,\n        proj[self.random_indexes[starting_ray_index : starting_ray_index + batch_size]],\n    )\n</code></pre>"},{"location":"reference/nect/sampling/geometry/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> geometry","text":""},{"location":"reference/nect/sampling/geometry/#nect.sampling.geometry","title":"geometry","text":""},{"location":"reference/nect/sampling/geometry/#nect.sampling.geometry.Geometry","title":"Geometry","text":"<pre><code>Geometry(nDetector: _list_2_int, dDetector: _list_2_float, mode: str, DSD: float | None = None, DSO: float | None = None, nVoxel: _list_3_int | None = None, dVoxel: _list_3_float | None = None, radius: float | None = None, height: float | None = None, offOrigin: _list_3_float = (0.0, 0.0, 0.0), COR: float = 0.0, offDetector: _list_2_float = (0.0, 0.0), rotDetector: _list_3_float = (0.0, 0.0, 0.0), reconstruction_mode: str = 'voxel', detector_binning: int = 1, angles: _list_float | None = None, radians: bool = True, timesteps: _list | None = None)\n</code></pre> <p>Set up the geometry for the CT system.</p> <p>Parameters:</p> Name Type Description Default <code>nDetector</code> <code>tuple[int, int] | list[int] | Tensor | ndarray</code> <p>Number of pixels <code>[height, width]</code> of the detector</p> required <code>dDetector</code> <code>tuple[float, float] | list[float] | Tensor | ndarray</code> <p>Height and width of the detector (mm)</p> required <code>mode</code> <code>str</code> <p>Type of geometry. Supported modes are <code>cone</code> and <code>parallel</code></p> required <code>DSD</code> <code>float</code> <p>Distance Source Detector (mm)</p> <code>None</code> <code>DSO</code> <code>float</code> <p>Distance Source Origin (mm)</p> <code>None</code> <code>nVoxel</code> <code>tuple[int, int, int] | list[int] | Tensor | ndarray</code> <p>Number of voxels <code>[z, y, x]</code> of the volume</p> <code>None</code> <code>dVoxel</code> <code>tuple[float, float, float] | list[float] | Tensor | ndarray</code> <p>Size of a voxel <code>[z, y, x]</code> (mm)</p> <code>None</code> <code>radius</code> <code>float</code> <p>Radius of the object (mm)</p> <code>None</code> <code>height</code> <code>float</code> <p>Height of the object (mm)</p> <code>None</code> <code>offOrigin</code> <code>tuple[float, float, float] | list[float] | Tensor | ndarray</code> <p>Offset of the object from the origin <code>[z, y, x]</code> (mm)</p> <code>(0.0, 0.0, 0.0)</code> <code>COR</code> <code>float</code> <p>Center of rotation (mm)</p> <code>0.0</code> <code>offDetector</code> <code>tuple[float, float] | list[float] | Tensor | ndarray</code> <p>Offset of the detector from the center <code>[height, width]</code> (mm)</p> <code>(0.0, 0.0)</code> <code>rotDetector</code> <code>tuple[float, float, float] | list[float] | Tensor | ndarray</code> <p>Rotation of the detector <code>[roll, pitch, yaw]</code> (radians).</p> <code>(0.0, 0.0, 0.0)</code> <code>reconstruction_mode</code> <code>str</code> <p>Type of reconstruction. Supported modes are <code>'voxel'</code> and <code>'cylindrical'</code>. Default is <code>'voxel'</code></p> <code>'voxel'</code> <code>detector_binning</code> <code>int</code> <p>Binning factor of the detector. Default is 1</p> <code>1</code> <code>angles</code> <code>list[float] | Tensor | ndarray | None</code> <p>List of angles.</p> <code>None</code> <code>radians</code> <code>bool</code> <p>Unit of angles. If <code>True</code>, the unit is radians, if <code>False</code> the unit is degrees. Default is <code>True</code></p> <code>True</code> <code>timesteps</code> <code>list[float | int] | Tensor | ndarray | None</code> <p>An array of timesteps. Do not need to be normalized. If the order of the angles and corresponding projections does not equal the acqustition order, this parameter needs to be set to get the timesteps correct. Only important for dynamic reconstruction. Overrides the timestep of the Geometry if not <code>None</code>.</p> <code>None</code> Source code in <code>nect/sampling/geometry.py</code> <pre><code>def __init__(\n    self,\n    nDetector: _list_2_int,\n    dDetector: _list_2_float,\n    mode: str,\n    DSD: float | None = None,\n    DSO: float | None = None,\n    nVoxel: _list_3_int | None = None,\n    dVoxel: _list_3_float | None = None,\n    radius: float | None = None,\n    height: float | None = None,\n    offOrigin: _list_3_float = (0.0, 0.0, 0.0),\n    COR: float = 0.0,\n    offDetector: _list_2_float = (0.0, 0.0),\n    rotDetector: _list_3_float = (0.0, 0.0, 0.0),\n    reconstruction_mode: str = \"voxel\",\n    detector_binning: int = 1,\n    angles: _list_float | None = None,\n    radians: bool = True,\n    timesteps: _list | None = None,\n):\n    \"\"\"\n    Set up the geometry for the CT system.\n\n    Args:\n        nDetector (tuple[int, int] | list[int] | torch.Tensor | np.ndarray):\n            Number of pixels `[height, width]` of the detector\n        dDetector (tuple[float, float] | list[float] | torch.Tensor | np.ndarray):\n            Height and width of the detector (mm)\n        mode (str):\n            Type of geometry. Supported modes are `cone` and `parallel`\n        DSD (float):\n            Distance Source Detector (mm)\n        DSO (float):\n            Distance Source Origin (mm)\n        nVoxel (tuple[int, int, int] | list[int] | torch.Tensor | np.ndarray):\n            Number of voxels `[z, y, x]` of the volume\n        dVoxel (tuple[float, float, float] | list[float] | torch.Tensor | np.ndarray):\n            Size of a voxel `[z, y, x]` (mm)\n        radius (float):\n            Radius of the object (mm)\n        height (float):\n            Height of the object (mm)\n        offOrigin (tuple[float, float, float] | list[float] | torch.Tensor | np.ndarray):\n            Offset of the object from the origin `[z, y, x]` (mm)\n        COR (float):\n            Center of rotation (mm)\n        offDetector (tuple[float, float] | list[float] | torch.Tensor | np.ndarray):\n            Offset of the detector from the center `[height, width]` (mm)\n        rotDetector (tuple[float, float, float] | list[float] | torch.Tensor | np.ndarray):\n            Rotation of the detector `[roll, pitch, yaw]` (radians).\n        reconstruction_mode (str):\n            Type of reconstruction. Supported modes are `'voxel'` and `'cylindrical'`. Default is `'voxel'`\n        detector_binning (int):\n            Binning factor of the detector. Default is 1\n        angles (list[float] | torch.Tensor | np.ndarray | None):\n            List of angles.\n        radians (bool):\n            Unit of angles. If `True`, the unit is radians, if `False` the unit is degrees. Default is `True`\n        timesteps (list[float | int] | torch.Tensor | np.ndarray | None):\n            An array of timesteps. Do not need to be normalized.\n            If the order of the angles and corresponding projections does not equal the acqustition order, this parameter needs to be set to get the timesteps correct.\n            Only important for dynamic reconstruction. Overrides the timestep of the Geometry if not `None`.\n    \"\"\"\n    if mode not in [\"cone\", \"parallel\"]:\n        raise ValueError(f\"Unsupported mode '{mode}'\")\n    if mode == \"cone\":\n        if DSD is None:\n            raise ValueError(\"DSD is required for cone geometry\")\n        if DSO is None:\n            raise ValueError(\"DSO is required for cone geometry\")\n        self.DSD = float(DSD)\n        self.DSO = float(DSO)\n    else:\n        if DSD is not None:\n            warn(\"DSD is not required for parallel geometry. Ignoring the value of DSD\")\n        if DSO is not None:\n            warn(\"DSO is not required for parallel geometry. Ignoring the value of DSO\")\n        self.DSD = None\n        self.DSO = None\n    if reconstruction_mode not in [\"voxel\", \"cylindrical\"]:\n        raise ValueError(f\"Unsupported reconstruction mode '{reconstruction_mode}'\")\n    self.mode = mode\n    self.reconstruction_mode = reconstruction_mode\n    self.detector_binning = detector_binning\n    self.dDetector = cast(_tuple_2_float, self._check_and_return_float(dDetector, 2, \"dDetector\"))\n    self.nDetector = cast(_tuple_2_int, self._check_and_return_int(nDetector, 2, \"nDetector\"))\n    self.offDetector = cast(_tuple_2_float, self._check_and_return_float(offDetector, 2, \"offDetector\"))\n    self.rotDetector = cast(_tuple_3_float, self._check_and_return_float(rotDetector, 3, \"rotDetector\"))\n    self.COR = COR\n    self.set_angles(angles, radians)\n    self.set_timesteps(timesteps)\n    if radius is None or height is None:\n        self.nVoxel = cast(_tuple_3_int, self._check_and_return_int(nVoxel, 3, \"nVoxel\"))\n        self.dVoxel = cast(_tuple_3_float, self._check_and_return_float(dVoxel, 3, \"dVoxel\"))\n        self.sVoxel = (\n            self.nVoxel[0] * self.dVoxel[0],\n            self.nVoxel[1] * self.dVoxel[1],\n            self.nVoxel[2] * self.dVoxel[2],\n        )\n        self.offOrigin = cast(_tuple_3_float, self._check_and_return_float(offOrigin, 3, \"offOrigin\"))\n        if self.reconstruction_mode == \"cylindrical\":\n            if radius is None:\n                self.radius = max(self.sVoxel[1], self.sVoxel[2]) / 2\n            if height is None:\n                self.height = self.sVoxel[0]\n        else:\n            self.radius = None\n            self.height = None\n    else:\n        self.radius = radius\n        self.height = height\n        if nVoxel is not None:\n            self.nVoxel = cast(_tuple_3_int, self._check_and_return_int(nVoxel, 3, \"nVoxel\"))\n        else:\n            self.nVoxel = (self.nDetector[0], self.nDetector[1], self.nDetector[1])\n        if dVoxel is not None:\n            self.dVoxel = cast(_tuple_3_float, self._check_and_return_float(dVoxel, 3, \"dVoxel\"))\n        else:\n            self.dVoxel = (\n                self.nDetector[0] / height,\n                self.nDetector[1] / (2 * radius),\n                self.nDetector[1] / (2 * radius),\n            )\n        self.sVoxel = (\n            self.nVoxel[0] * self.dVoxel[0],\n            self.nVoxel[1] * self.dVoxel[1],\n            self.nVoxel[2] * self.dVoxel[2],\n        )\n    if self.mode == \"cone\":\n        if (nVoxel is None or dVoxel is None) and reconstruction_mode == \"cylindrical\":\n            max_length = self.radius * 2\n            triangle_theta = np.arctan((self.height / 2) / (self.DSO + max_length / 2))\n            self.max_distance_traveled = max_length / np.cos(triangle_theta)\n        else:\n            max_length = ((self.sVoxel[1] - self.dVoxel[1]) ** 2 + (self.sVoxel[2] - self.dVoxel[2]) ** 2) ** 0.5\n            triangle_theta = np.arctan(((self.sVoxel[0] - self.dVoxel[0]) / 2) / (self.DSO + max_length / 2))\n            self.max_distance_traveled = max_length / np.cos(triangle_theta)\n    else:\n        if self.reconstruction_mode == \"voxel\":\n            self.max_distance_traveled = (\n                (self.sVoxel[1] - self.dVoxel[1]) ** 2 + (self.sVoxel[2] - self.dVoxel[2]) ** 2\n            ) ** 0.5\n        elif self.reconstruction_mode == \"cylindrical\":\n            if nVoxel is None or dVoxel is None:\n                self.max_distance_traveled = self.radius * 2\n            else:\n                self.max_distance_traveled = max(self.sVoxel[1], self.sVoxel[2])\n    self.sDetector = (\n        self.nDetector[0] * self.dDetector[0],\n        self.nDetector[1] * self.dDetector[1],\n    )\n</code></pre>"},{"location":"reference/nect/sampling/geometry/#nect.sampling.geometry.Geometry.from_cfg","title":"from_cfg  <code>classmethod</code>","text":"<pre><code>from_cfg(cfg: GeometryCone | Geometry, reconstruction_mode: str = 'voxel', sample_outside: int = 0) -&gt; 'Geometry'\n</code></pre> <p>Load the geometry from a configuration object.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>GeometryCone | Geometry</code> <p>The configuration object.</p> required <code>reconstruction_mode</code> <code>str</code> <p>The reconstruction mode. Default is <code>'voxel'</code>.</p> <code>'voxel'</code> <code>sample_outside</code> <code>int</code> <p>The number of voxels to sample outside the object. Default is <code>0</code>.</p> <code>0</code> <p>Returns:</p> Name Type Description <code>Geometry</code> <code>'Geometry'</code> <p>The geometry object.</p> Source code in <code>nect/sampling/geometry.py</code> <pre><code>@classmethod\ndef from_cfg(\n    cls,\n    cfg: nect.config.GeometryCone | nect.config.Geometry,\n    reconstruction_mode: str = \"voxel\",\n    sample_outside: int = 0,\n) -&gt; \"Geometry\":\n    \"\"\"\n    Load the geometry from a configuration object.\n\n    Args:\n        cfg (nect.config.GeometryCone | nect.config.Geometry): The configuration object.\n        reconstruction_mode (str): The reconstruction mode. Default is `'voxel'`.\n        sample_outside (int): The number of voxels to sample outside the object. Default is `0`.\n\n    Returns:\n        Geometry: The geometry object.\n    \"\"\"\n    nVoxel = cfg.nVoxel\n    if sample_outside &gt; 0:\n        nVoxel = [s + 2 * sample_outside for s in nVoxel]\n    return cls(\n        nDetector=cfg.nDetector,\n        dDetector=cfg.dDetector,\n        mode=cfg.mode,\n        DSD=cfg.DSD if hasattr(cfg, \"DSD\") else None,\n        DSO=cfg.DSO if hasattr(cfg, \"DSO\") else None,\n        nVoxel=nVoxel,\n        dVoxel=cfg.dVoxel,\n        radius=cfg.radius,\n        height=cfg.height,\n        offOrigin=cfg.offOrigin,\n        COR=cfg.COR,\n        offDetector=cfg.offDetector,\n        rotDetector=cfg.rotDetector if cfg.rotDetector is not None else (0.0, 0.0, 0.0),\n        reconstruction_mode=reconstruction_mode,\n        detector_binning=1,\n        angles=cfg.angles,\n        timesteps=cfg.timesteps,\n    )\n</code></pre>"},{"location":"reference/nect/sampling/geometry/#nect.sampling.geometry.Geometry.from_yaml","title":"from_yaml  <code>classmethod</code>","text":"<pre><code>from_yaml(path: str | Path, reconstruction_mode: str | None = None) -&gt; 'Geometry'\n</code></pre> <p>Load the geometry from a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>The path to the YAML file.</p> required <code>reconstruction_mode</code> <code>str | None</code> <p>The reconstruction mode. Supported strings are <code>'voxel'</code> and <code>'cylindrical'</code>. Default is <code>None</code>.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Geometry</code> <code>'Geometry'</code> <p>The geometry object.</p> Source code in <code>nect/sampling/geometry.py</code> <pre><code>@classmethod\ndef from_yaml(cls, path: str | Path, reconstruction_mode: str | None = None) -&gt; \"Geometry\":\n    \"\"\"\n    Load the geometry from a YAML file.\n\n    Args:\n        path (str | Path): The path to the YAML file.\n        reconstruction_mode (str | None): The reconstruction mode. Supported strings are `'voxel'` and `'cylindrical'`. Default is `None`.\n\n    Returns:\n        Geometry: The geometry object.\"\"\"\n    with open(path, \"r\") as f:\n        cfg = yaml.safe_load(f)\n    if reconstruction_mode is None:\n        reconstruction_mode = cfg.get(\"reconstruction_mode\", \"voxel\")\n    elif reconstruction_mode not in (\"voxel\", \"cylindrical\"):\n        raise NotImplementedError(\n            f\"Only reconstruction mode 'voxel' and 'cylindrical' is implemented, got '{reconstruction_mode}'\"\n        )\n    return cls(\n        nDetector=cfg[\"nDetector\"],\n        dDetector=cfg[\"dDetector\"],\n        mode=cfg[\"mode\"],\n        DSD=cfg.get(\"DSD\", None),\n        DSO=cfg.get(\"DSO\", None),\n        nVoxel=cfg.get(\"nVoxel\", None),\n        dVoxel=cfg.get(\"dVoxel\", None),\n        radius=cfg.get(\"radius\", None),\n        height=cfg.get(\"height\", None),\n        offOrigin=cfg.get(\"offOrigin\", (0.0, 0.0, 0.0)),\n        COR=cfg.get(\"COR\", 0.0),\n        offDetector=cfg.get(\"offDetector\", (0.0, 0.0)),\n        rotDetector=cfg.get(\"rotDetector\", (0.0, 0.0, 0.0)),\n        reconstruction_mode=reconstruction_mode,\n        detector_binning=1,\n        angles=cfg.get(\"angles\", None),\n        timesteps=cfg.get(\"timesteps\", None),\n        radians=cfg.get(\"radians\", True),\n    )\n</code></pre>"},{"location":"reference/nect/sampling/geometry/#nect.sampling.geometry.Geometry.set_angles","title":"set_angles","text":"<pre><code>set_angles(angles: _list_float | None, radians: bool)\n</code></pre> <p>Set the angles for the geometry.</p> <p>Parameters:</p> Name Type Description Default <code>angles</code> <code>list[float] | Tensor | ndarray | None</code> <p>List of angles.</p> required <code>radians</code> <code>bool</code> <p>Unit of angles. If <code>True</code>, the unit is radians, if <code>False</code> the unit is degrees. Default is <code>True</code></p> required Source code in <code>nect/sampling/geometry.py</code> <pre><code>def set_angles(self, angles: _list_float | None, radians: bool):\n    \"\"\"\n    Set the angles for the geometry.\n\n    Args:\n        angles (list[float] | torch.Tensor | np.ndarray | None):\n            List of angles.\n        radians (bool):\n            Unit of angles. If `True`, the unit is radians, if `False` the unit is degrees. Default is `True`\"\"\"\n    self.angles = angles\n    if self.angles is not None:\n        if not isinstance(self.angles, np.ndarray):\n            if isinstance(self.angles, list):\n                self.angles = np.array(self.angles)\n            elif isinstance(self.angles, torch.Tensor):\n                self.angles = self.angles.cpu().numpy()\n        if radians is False:\n            self.angles = np.radians(self.angles)\n</code></pre>"},{"location":"reference/nect/sampling/geometry/#nect.sampling.geometry.Geometry.set_detector_binning","title":"set_detector_binning","text":"<pre><code>set_detector_binning(detector_binning: int)\n</code></pre> <p>Set the detector binning factor.</p> <p>Parameters:</p> Name Type Description Default <code>detector_binning</code> <code>int</code> <p>The binning factor of the detector</p> required Source code in <code>nect/sampling/geometry.py</code> <pre><code>def set_detector_binning(self, detector_binning: int):\n    \"\"\"\n    Set the detector binning factor.\n\n    Args:\n        detector_binning (int): The binning factor of the detector\"\"\"\n    self.detector_binning = detector_binning\n</code></pre>"},{"location":"reference/nect/sampling/geometry/#nect.sampling.geometry.Geometry.set_timesteps","title":"set_timesteps","text":"<pre><code>set_timesteps(timesteps: _list | None)\n</code></pre> <p>Set the timesteps for dynamic reconstruction.</p> <p>Parameters:</p> Name Type Description Default <code>timesteps</code> <code>list[float | int] | Tensor | ndarray | None</code> <p>An array of timesteps. Do not need to be normalized. If the order of the angles and corresponding projections does not equal the acqustition order, this parameter needs to be set to get the timesteps correct. Only important for dynamic reconstruction. Overrides the timestep of the Geometry if not <code>None</code>.</p> required Source code in <code>nect/sampling/geometry.py</code> <pre><code>def set_timesteps(self, timesteps: _list | None):\n    \"\"\"\n    Set the timesteps for dynamic reconstruction.\n\n    Args:\n        timesteps (list[float | int] | torch.Tensor | np.ndarray | None):\n            An array of timesteps. Do not need to be normalized.\n            If the order of the angles and corresponding projections does not equal the acqustition order, this parameter needs to be set to get the timesteps correct.\n            Only important for dynamic reconstruction. Overrides the timestep of the Geometry if not `None`.\n    \"\"\"\n    self.timesteps = timesteps\n</code></pre>"},{"location":"reference/nect/sampling/projector/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> projector","text":""},{"location":"reference/nect/sampling/projector/#nect.sampling.projector","title":"projector","text":""},{"location":"reference/nect/sampling/projector/#nect.sampling.projector.Projector","title":"Projector","text":"<pre><code>Projector(geometry: Geometry, points_per_batch: int, points_per_ray: int, device: device | str | int, random_offset_detector: float = 0.0, uniform_ray_spacing: bool = True)\n</code></pre> Source code in <code>nect/sampling/projector.py</code> <pre><code>def __init__(\n    self,\n    geometry: Geometry,\n    points_per_batch: int,\n    points_per_ray: int,\n    device: torch.device | str | int,\n    random_offset_detector: float = 0.0,\n    uniform_ray_spacing: bool = True,\n):\n    self.device = _check_and_return_cuda_device(device)\n    self.points_per_batch = points_per_batch\n    self.points_per_ray = points_per_ray\n    if points_per_ray &lt; 1:\n        raise ValueError(f\"points_per_ray ({points_per_ray}) must be greater than or equal to 1\")\n    if points_per_batch &lt; points_per_ray:\n        raise ValueError(\n            f\"points_per_batch ({points_per_batch}) must be greater than or equal to points_per_ray ({points_per_ray})\"\n        )\n    self.geometry = geometry\n    self.random_offset_detector = random_offset_detector\n    self.uniform_ray_spacing = uniform_ray_spacing\n    self.c_geometry = self.geometry.get_c_geometry()\n</code></pre>"},{"location":"reference/nect/sampling/projector/#nect.sampling.projector.Projector.__call__","title":"__call__","text":"<pre><code>__call__(batch_num: int, proj: Tensor)\n</code></pre> <p>Sample points along the rays, and return the points and the corresponding projection values.</p> Source code in <code>nect/sampling/projector.py</code> <pre><code>def __call__(self, batch_num: int, proj: torch.Tensor):\n    \"\"\"\n    Sample points along the rays, and return the points and the corresponding projection values.\n    \"\"\"\n    starting_ray_index = batch_num * self.batch_size\n    batch_size = self.batch_size\n    if (batch_num + 1) * batch_size &gt; self.random_indexes.size(0):\n        if batch_num == 0:\n            batch_size = self.random_indexes.size(0) % batch_size\n        else:\n            return None, None\n    ray_points, distances = nect.sampling.ct_sampling.sample(\n        random_ray_index=self.random_indexes,\n        geometry=self.c_geometry,\n        angle_rad=self.angle,\n        num_points_per_ray=self.points_per_ray,\n        num_rays=batch_size,\n        starting_ray_index=starting_ray_index,\n        max_ray_distance_per_point=self.distance_between_points,\n        uniform_ray_spacing=self.uniform_ray_spacing,\n        # random_detector_offset=False,\n        random_detector_offset=self.random_offset_detector,\n        device=self.device.index,\n    )\n    self.distances = distances\n    ray_points.clamp_(min=0, max=1)\n    return (\n        ray_points,\n        proj[self.random_indexes[starting_ray_index : starting_ray_index + batch_size]],\n    )\n</code></pre>"},{"location":"reference/nect/src/","title":"Index","text":""},{"location":"reference/nect/src/#nect.src","title":"src","text":""},{"location":"reference/nect/src/evaluation/","title":"Index","text":""},{"location":"reference/nect/src/evaluation/#nect.src.evaluation","title":"evaluation","text":""},{"location":"reference/nect/src/evaluation/config/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> config","text":""},{"location":"reference/nect/src/evaluation/config/#nect.src.evaluation.config","title":"config","text":""},{"location":"reference/nect/src/evaluation/evaluator/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> evaluator","text":""},{"location":"reference/nect/src/evaluation/evaluator/#nect.src.evaluation.evaluator","title":"evaluator","text":""},{"location":"reference/nect/src/evaluation/evaluator/#nect.src.evaluation.evaluator.Evaluator","title":"Evaluator","text":"<pre><code>Evaluator(metrics: list[str], metrics_config: dict = {}, spatial_dims: int = 2)\n</code></pre> <p>Evaluator class for evaluating metrics on tensors.</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>list[str]</code> <p>Defines which metrics to use for image quality evaluation.</p> required <code>metrics_config</code> <code>dict</code> <p>Optional configuration to override base configuration. Defaults to {}.</p> <code>{}</code> <code>spatial_dims</code> <code>int</code> <p>Determines whether to use 2D or 3D base configuration. Defaults to 2.</p> <code>2</code> Source code in <code>nect/src/evaluation/evaluator.py</code> <pre><code>def __init__(self, metrics: list[str], metrics_config: dict = {}, spatial_dims: int = 2) -&gt; None:\n    \"\"\"Evaluator class for evaluating metrics on tensors.\n\n    Args:\n        metrics (list[str]): Defines which metrics to use for image quality evaluation.\n        metrics_config (dict, optional): Optional configuration to override base configuration. Defaults to {}.\n        spatial_dims (int, optional): Determines whether to use 2D or 3D base configuration. Defaults to 2.\n    \"\"\"\n    self.metrics = [metric.capitalize().lower() for metric in metrics]\n    assert spatial_dims in [\n        2,\n        3,\n    ], f\"Spatial dimensions must be either 2 or 3, got {spatial_dims}\"\n    self.base_config = BASE_2D_EVALUATION_CONFIG if spatial_dims == 2 else BASE_3D_EVALUATION_CONFIG\n    self.config = self.update_config(metrics_config)\n    self.spatial_dims = spatial_dims\n</code></pre>"},{"location":"reference/nect/src/evaluation/evaluator/#nect.src.evaluation.evaluator.Evaluator.evaluate","title":"evaluate","text":"<pre><code>evaluate(X: Tensor, Y: Tensor) -&gt; dict\n</code></pre> <p>Evaluates the similarity between two tensors using the metrics defined in self.metrics. The optional configuration defined in self.config is used to override the base configuration.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> required <code>Y</code> <code>Tensor</code> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If a provided metric is not implemented.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing the results of the evaluation. The scores are accessible via the metric names.</p> Source code in <code>nect/src/evaluation/evaluator.py</code> <pre><code>def evaluate(self, X: torch.Tensor, Y: torch.Tensor) -&gt; dict:\n    \"\"\"Evaluates the similarity between two tensors using the metrics defined in self.metrics.\n    The optional configuration defined in self.config is used to override the base configuration.\n\n    Args:\n        X (torch.Tensor):\n        Y (torch.Tensor):\n\n    Raises:\n        ValueError: If a provided metric is not implemented.\n\n    Returns:\n        dict: Dictionary containing the results of the evaluation. The scores are accessible via the metric names.\n    \"\"\"\n    results = {}\n    for metric_name in self.metrics:\n        # print(f\"Evaluating {metric_name}...\")\n        if metric_name not in METRICS:\n            raise ValueError(f\"Metric {metric_name} not implemented\")\n        metric = METRICS[metric_name](**self.config[metric_name])\n        while self.spatial_dims == 3 and X.dim() &lt; 5:\n            X = X.unsqueeze(0)\n        while self.spatial_dims == 3 and Y.dim() &lt; 5:\n            Y = Y.unsqueeze(0)\n        while self.spatial_dims == 4 and X.dim() &lt; 6:\n            X = X.unsqueeze(0)\n        while self.spatial_dims == 4 and Y.dim() &lt; 6:\n            Y = Y.unsqueeze(0)\n        results[metric_name] = metric.forward(X, Y)\n    return results\n</code></pre>"},{"location":"reference/nect/src/evaluation/evaluator/#nect.src.evaluation.evaluator.Evaluator.update_config","title":"update_config","text":"<pre><code>update_config(new_config: dict) -&gt; None\n</code></pre> <p>Used to override the base configuration with the optional configuration defined in self.config.</p> Source code in <code>nect/src/evaluation/evaluator.py</code> <pre><code>def update_config(self, new_config: dict) -&gt; None:\n    \"\"\"Used to override the base configuration with the optional configuration defined in self.config.\"\"\"\n    for key, value in new_config.items():\n        for k, v in value.items():\n            self.base_config[key][k] = v\n    return self.base_config\n</code></pre>"},{"location":"reference/nect/src/phantom/","title":"Index","text":""},{"location":"reference/nect/src/phantom/#nect.src.phantom","title":"phantom","text":""},{"location":"reference/nect/src/phantom/demo_3d_phantom/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> demo_3d_phantom","text":""},{"location":"reference/nect/src/phantom/demo_3d_phantom/#nect.src.phantom.demo_3d_phantom","title":"demo_3d_phantom","text":"<p>The demo demonstrates how to create a 3D phantom and visualize it using Plotly. The 3D phantom can be used to verify the correctness of the forward model or the reconstruction algorithm. The demo creates a cuboid and a sphere that moves in a predefined trajectory.</p>"},{"location":"reference/nect/src/phantom/geometric/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> geometric","text":""},{"location":"reference/nect/src/phantom/geometric/#nect.src.phantom.geometric","title":"geometric","text":""},{"location":"reference/nect/src/phantom/geometric/#nect.src.phantom.geometric.Phantom","title":"Phantom","text":"<pre><code>Phantom(size: tuple[int, int], background_method: str = 'zeros')\n</code></pre> <p>Creates a phantom image with a given background. Phantom objects can be added to the phantom.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>tuple[int, int]</code> <p>The size of the phantom. Defined as [x, y, [z]]</p> required <code>background_method</code> <code>str</code> <p>How to create the background. Defaults to \"zeros\".</p> <code>'zeros'</code> Source code in <code>nect/src/phantom/geometric.py</code> <pre><code>def __init__(self, size: tuple[int, int], background_method: str = \"zeros\") -&gt; None:\n    \"\"\"Creates a phantom image with a given background. Phantom objects can be added to the phantom.\n\n    Args:\n        size (tuple[int,int]): The size of the phantom. Defined as [x, y, [z]]\n        background_method (str, optional): How to create the background. Defaults to \"zeros\".\n    \"\"\"\n    self.objects = []\n    self.size = size\n    assert len(size) &gt;= 2 or len(size) &lt;= 3, \"Size must be a tuple of length 2 or 3.\"\n    self.is_3d = len(size) == 3\n    self.create_background(size=size, method=background_method)\n</code></pre>"},{"location":"reference/nect/src/phantom/geometric/#nect.src.phantom.geometric.Phantom.create_video_as_tensor","title":"create_video_as_tensor","text":"<pre><code>create_video_as_tensor(time_steps: ndarray) -&gt; Tensor\n</code></pre> <p>Creates a video of the phantom as a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>time_steps</code> <code>ndarray</code> <p>The time steps of the video.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The video as a tensor. Has shape (time_steps, height, width).</p> Source code in <code>nect/src/phantom/geometric.py</code> <pre><code>def create_video_as_tensor(self, time_steps: np.ndarray) -&gt; torch.Tensor:\n    \"\"\"\n    Creates a video of the phantom as a tensor.\n\n    Args:\n        time_steps (np.ndarray): The time steps of the video.\n\n    Returns:\n        torch.Tensor: The video as a tensor. Has shape (time_steps, height, width).\"\"\"\n    vid = np.zeros(shape=(len(time_steps), *self.background.shape))\n    for i in tqdm(range(len(time_steps)), desc=\"Creating phantom images\"):\n        vid[i] = self.get_phantom(time_steps[i])\n    return torch.from_numpy(np.array(vid)).float()\n</code></pre>"},{"location":"reference/nect/src/phantom/geometric/#nect.src.phantom.geometric.Phantom.get_phantom","title":"get_phantom","text":"<pre><code>get_phantom(t: float) -&gt; Tensor\n</code></pre> <p>Gets the state of the phantom at time t.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>float</code> <p>The current time.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The background with the object intensities added.</p> Source code in <code>nect/src/phantom/geometric.py</code> <pre><code>def get_phantom(self, t: float) -&gt; torch.Tensor:\n    \"\"\"Gets the state of the phantom at time t.\n\n    Args:\n        t (float): The current time.\n\n    Returns:\n        torch.Tensor: The background with the object intensities added.\n    \"\"\"\n    phantom = self.background.clone()\n    t_ = t\n    if self.is_3d:\n        for obj in self.objects:\n            obj.update(t_)\n            top = int(max(0, obj.tlbr[0]))  # Top\n            left = int(max(0, obj.tlbr[1]))  # Left\n            high = int(max(0, obj.tlbr[2]))  # Upper\n            bottom = int(min(obj.tlbr[3], self.size[0]))\n            right = int(min(obj.tlbr[4], self.size[1]))\n            low = int(min(obj.tlbr[5], self.size[2]))\n            phantom[top:bottom, left:right, high:low] += obj.intensity_grid[\n                0 : bottom - top, 0 : right - left, 0 : low - high\n            ]\n    else:\n        for obj in self.objects:\n            obj.update(t_)\n            top = int(max(0, obj.tlbr[0]))\n            left = int(max(0, obj.tlbr[1]))\n            bottom = int(min(obj.tlbr[2], self.size[0]))\n            right = int(min(obj.tlbr[3], self.size[1]))\n            phantom[top:bottom, left:right] += obj.intensity_grid[0 : bottom - top, 0 : right - left]\n    return phantom.detach().cpu().numpy()\n</code></pre>"},{"location":"reference/nect/src/phantom/geometric/#nect.src.phantom.geometric.PhantomObject","title":"PhantomObject","text":"<pre><code>PhantomObject(eom: Callable, eoi: Callable, tl: Tensor, geometry, intensity: int | Tensor)\n</code></pre> <p>Creates an object in a phantom that moves according to the equation of motion (eom) and changes its intensity according to the equation of intensity (eoi).</p> <p>Parameters:</p> Name Type Description Default <code>eom</code> <code>Callable</code> <p>Takes the initial tlbr coordinates and the time t as input and returns the new tlbr coordinates.</p> required <code>eoi</code> <code>Callable</code> <p>Takes the initial intensity and the time t as input and returns the new intensity.</p> required <code>tl</code> <code>Tensor</code> <p>The initial tl (top-left) coordinates of the bounding box.</p> required <code>geometry</code> <p>An object containing a boolean mask of the shape of the object.</p> required <code>intensity</code> <code>int | Tensor</code> <p>The intensity of the object, either as a scalar or as a grid of the same shape as the geometry mask.</p> required Source code in <code>nect/src/phantom/geometric.py</code> <pre><code>def __init__(\n    self,\n    eom: Callable,\n    eoi: Callable,\n    tl: torch.Tensor,\n    geometry,\n    intensity: int | torch.Tensor,\n) -&gt; None:\n    \"\"\"Creates an object in a phantom that moves according to the equation of motion (eom) and changes its intensity according to the equation of intensity (eoi).\n\n    Args:\n        eom (Callable): Takes the initial tlbr coordinates and the time t as input and returns the new tlbr coordinates.\n        eoi (Callable): Takes the initial intensity and the time t as input and returns the new intensity.\n        tl (torch.Tensor): The initial tl (top-left) coordinates of the bounding box.\n        geometry (): An object containing a boolean mask of the shape of the object.\n        intensity (int | torch.Tensor): The intensity of the object, either as a scalar or as a grid of the same shape as the geometry mask.\n    \"\"\"\n    self.eom = eom  # equation of motion\n    self.eoi = eoi  # equation of intensity\n    assert len(tl) == len(\n        geometry.mask.shape\n    ), \"Initialization coordinates and geometry must have the same dimensionality.\"\n    self.tlbr_init = torch.cat(\n        (tl, tl + torch.tensor(geometry.mask.shape)), 0\n    )  # initial tlbr coordinates of bounding box\n    self.tlbr = self.tlbr_init  # tlbr coordinates of bounding box\n    self.geometry = geometry  # shape of object (boolean mask)\n    self.intensity = intensity  # intensity of object (scalar or grid)\n    self.intensity_grid = self.geometry.mask * self.intensity  # intensity grid of object\n</code></pre>"},{"location":"reference/nect/src/phantom/geometric/#nect.src.phantom.geometric.PhantomObject.update","title":"update","text":"<pre><code>update(t: float) -&gt; None\n</code></pre> <p>Updates the position and the intensity of the object.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>float</code> <p>The current time.</p> required Source code in <code>nect/src/phantom/geometric.py</code> <pre><code>def update(self, t: float) -&gt; None:\n    \"\"\"Updates the position and the intensity of the object.\n\n    Args:\n        t (float): The current time.\n    \"\"\"\n    self.tlbr = self.eom(self.tlbr_init, t)\n    self.intensity_grid = self.geometry.mask * self.eoi(self.intensity, t)\n</code></pre>"},{"location":"reference/nect/src/phantom/phantom_config/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> phantom_config","text":""},{"location":"reference/nect/src/phantom/phantom_config/#nect.src.phantom.phantom_config","title":"phantom_config","text":"<p>The file contains various phantom configurations that can be used for model testing and validation.</p>"},{"location":"reference/nect/src/phantom/phantom_config/#nect.src.phantom.phantom_config.moving_objects_constant_intensity_phantom","title":"moving_objects_constant_intensity_phantom","text":"<pre><code>moving_objects_constant_intensity_phantom(save_video=False) -&gt; Phantom\n</code></pre> <p>Creates a phantom with moving objects.</p> <p>Returns:</p> Name Type Description <code>Phantom</code> <code>Phantom</code> <p>The phantom with moving objects.</p> Source code in <code>nect/src/phantom/phantom_config.py</code> <pre><code>def moving_objects_constant_intensity_phantom(save_video=False) -&gt; Phantom:\n    \"\"\"Creates a phantom with moving objects.\n\n    Returns:\n        Phantom: The phantom with moving objects.\n    \"\"\"\n\n    phantom = Phantom(size=(300, 300), background_method=\"zeros\")\n    circle = Circle(300, 300, 150)\n    rectangle = Rectangle(40, 80)\n    triangle = Triangle(40, 80)\n    small_circle = Circle(20, 20, 10)\n    ellipse = Ellipse(40, 80, 10, 20)\n    random = create_random_closed_geometry(size=(50, 50))\n\n    def eom_horizontal(tlbr, t):\n        return tlbr + t * torch.tensor([0, 1, 0, 1])\n\n    def eom_vertical(tlbr, t):\n        return tlbr + t * torch.tensor([1, 0, 1, 0])\n\n    def eom_diagonal(tlbr, t):\n        return tlbr + t\n\n    phantom_circle = PhantomObject(\n        lambda tlbr, t: tlbr,\n        lambda intensity, t: intensity,\n        torch.tensor([0, 0]),\n        circle,\n        intensity=0.1,\n    )\n    phantom_rectangle = PhantomObject(\n        eom_horizontal,\n        lambda intensity, t: intensity,\n        torch.tensor([60, 60]),\n        rectangle,\n        intensity=1,\n    )\n    phantom_random = PhantomObject(\n        eom_diagonal,\n        lambda intensity, t: intensity,\n        torch.tensor([100, 100]),\n        random,\n        intensity=1,\n    )\n    phantom_ellipse = PhantomObject(\n        eom_vertical,\n        lambda intensity, t: intensity,\n        torch.tensor([200, 200]),\n        ellipse,\n        intensity=1,\n    )\n    phantom_small_circle = PhantomObject(\n        eom_diagonal,\n        lambda intensity, t: intensity,\n        torch.tensor([100, 200]),\n        small_circle,\n        intensity=1,\n    )\n    phantom_triangle = PhantomObject(\n        eom_diagonal,\n        lambda intensity, t: intensity,\n        torch.tensor([200, 100]),\n        triangle,\n        intensity=1,\n    )\n\n    phantom.add_phantom_object(\n        [\n            phantom_circle,\n            phantom_rectangle,\n            phantom_random,\n            phantom_ellipse,\n            phantom_small_circle,\n            phantom_triangle,\n        ]\n    )\n\n    if save_video:\n        # Save the phantom to a video\n        time_steps = np.arange(0, 30, 1)\n        phantom.save_video(time_steps, \"src/phantom/cfg_videos/moving_objects.mp4\")\n    return phantom\n</code></pre>"},{"location":"reference/nect/src/phantom/phantom_config/#nect.src.phantom.phantom_config.moving_objects_one_constant_changing_intensity_phantom","title":"moving_objects_one_constant_changing_intensity_phantom","text":"<pre><code>moving_objects_one_constant_changing_intensity_phantom(save_video=False) -&gt; Phantom\n</code></pre> <p>Creates a phantom with moving objects.</p> <p>Returns:</p> Name Type Description <code>Phantom</code> <code>Phantom</code> <p>The phantom with moving objects.</p> Source code in <code>nect/src/phantom/phantom_config.py</code> <pre><code>def moving_objects_one_constant_changing_intensity_phantom(save_video=False) -&gt; Phantom:\n    \"\"\"Creates a phantom with moving objects.\n\n    Returns:\n        Phantom: The phantom with moving objects.\n    \"\"\"\n\n    phantom = Phantom(size=(300, 300), background_method=\"zeros\")\n    circle = Circle(300, 300, 150)\n    rectangle = Rectangle(40, 80)\n    square = Rectangle(40, 40)\n    triangle = Triangle(40, 80)\n    small_circle = Circle(20, 20, 10)\n    ellipse = Ellipse(40, 80, 10, 20)\n    random = create_random_closed_geometry(size=(50, 50))\n\n    def eom_horizontal(tlbr, t):\n        return tlbr + t * torch.tensor([0, 1, 0, 1])\n\n    def eom_vertical(tlbr, t):\n        return tlbr + t * torch.tensor([1, 0, 1, 0])\n\n    def eom_diagonal(tlbr, t):\n        return tlbr + t\n\n    def eom_constant(tlbr, t):\n        return tlbr\n\n    def decreaseing_intensity(intensity, t):\n        return intensity - t / 30\n\n    def increasing_intensity(intensity, t):\n        return intensity + t / 15\n\n    phantom_circle = PhantomObject(\n        lambda tlbr, t: tlbr,\n        lambda intensity, t: intensity,\n        torch.tensor([0, 0]),\n        circle,\n        intensity=0.1,\n    )\n    phantom_rectangle = PhantomObject(\n        eom_horizontal,\n        lambda intensity, t: intensity,\n        torch.tensor([60, 60]),\n        rectangle,\n        intensity=1,\n    )\n    phantom_random = PhantomObject(\n        eom_diagonal,\n        lambda intensity, t: intensity,\n        torch.tensor([100, 100]),\n        random,\n        intensity=1,\n    )\n    phantom_ellipse = PhantomObject(\n        eom_vertical,\n        lambda intensity, t: intensity,\n        torch.tensor([200, 200]),\n        ellipse,\n        intensity=1,\n    )\n    phantom_small_circle = PhantomObject(\n        eom_diagonal,\n        lambda intensity, t: intensity,\n        torch.tensor([100, 200]),\n        small_circle,\n        intensity=1,\n    )\n    phantom_triangle = PhantomObject(\n        eom_diagonal,\n        decreaseing_intensity,\n        torch.tensor([200, 100]),\n        triangle,\n        intensity=1,\n    )\n    phantom_static_rectangle = PhantomObject(\n        eom_constant, increasing_intensity, torch.tensor([150, 50]), square, intensity=1\n    )\n\n    phantom.add_phantom_object(\n        [\n            phantom_circle,\n            phantom_rectangle,\n            phantom_random,\n            phantom_ellipse,\n            phantom_small_circle,\n            phantom_triangle,\n            phantom_static_rectangle,\n        ]\n    )\n\n    if save_video:\n        # Save the phantom to a video\n        time_steps = np.arange(0, 30, 1)\n        phantom.save_video(\n            time_steps,\n            \"src/phantom/cfg_videos/moving_objects_one_static_chaning_intensity.mp4\",\n        )\n    return phantom\n</code></pre>"},{"location":"reference/nect/src/phantom/phantom_config/#nect.src.phantom.phantom_config.moving_objects_one_constant_constant_intensity_phantom","title":"moving_objects_one_constant_constant_intensity_phantom","text":"<pre><code>moving_objects_one_constant_constant_intensity_phantom(save_video=False) -&gt; Phantom\n</code></pre> <p>Creates a phantom with moving objects.</p> <p>Returns:</p> Name Type Description <code>Phantom</code> <code>Phantom</code> <p>The phantom with moving objects.</p> Source code in <code>nect/src/phantom/phantom_config.py</code> <pre><code>def moving_objects_one_constant_constant_intensity_phantom(save_video=False) -&gt; Phantom:\n    \"\"\"Creates a phantom with moving objects.\n\n    Returns:\n        Phantom: The phantom with moving objects.\n    \"\"\"\n\n    phantom = Phantom(size=(300, 300), background_method=\"zeros\")\n    circle = Circle(300, 300, 150)\n    rectangle = Rectangle(40, 80)\n    square = Rectangle(40, 40)\n    triangle = Triangle(40, 80)\n    small_circle = Circle(20, 20, 10)\n    ellipse = Ellipse(40, 80, 10, 20)\n    random = create_random_closed_geometry(size=(50, 50))\n\n    def eom_horizontal(tlbr, t):\n        return tlbr + t * torch.tensor([0, 1, 0, 1])\n\n    def eom_vertical(tlbr, t):\n        return tlbr + t * torch.tensor([1, 0, 1, 0])\n\n    def eom_diagonal(tlbr, t):\n        return tlbr + t\n\n    def eom_constant(tlbr, t):\n        return tlbr\n\n    phantom_circle = PhantomObject(\n        lambda tlbr, t: tlbr,\n        lambda intensity, t: intensity,\n        torch.tensor([0, 0]),\n        circle,\n        intensity=0.1,\n    )\n    phantom_rectangle = PhantomObject(\n        eom_horizontal,\n        lambda intensity, t: intensity,\n        torch.tensor([60, 60]),\n        rectangle,\n        intensity=1,\n    )\n    phantom_random = PhantomObject(\n        eom_diagonal,\n        lambda intensity, t: intensity,\n        torch.tensor([100, 100]),\n        random,\n        intensity=1,\n    )\n    phantom_ellipse = PhantomObject(\n        eom_vertical,\n        lambda intensity, t: intensity,\n        torch.tensor([200, 200]),\n        ellipse,\n        intensity=1,\n    )\n    phantom_small_circle = PhantomObject(\n        eom_diagonal,\n        lambda intensity, t: intensity,\n        torch.tensor([100, 200]),\n        small_circle,\n        intensity=1,\n    )\n    phantom_triangle = PhantomObject(\n        eom_diagonal,\n        lambda intensity, t: intensity,\n        torch.tensor([200, 100]),\n        triangle,\n        intensity=1,\n    )\n    phantom_static_rectangle = PhantomObject(\n        eom_constant,\n        lambda intensity, t: intensity,\n        torch.tensor([150, 50]),\n        square,\n        intensity=1,\n    )\n\n    phantom.add_phantom_object(\n        [\n            phantom_circle,\n            phantom_rectangle,\n            phantom_random,\n            phantom_ellipse,\n            phantom_small_circle,\n            phantom_triangle,\n            phantom_static_rectangle,\n        ]\n    )\n\n    if save_video:\n        # Save the phantom to a video\n        time_steps = np.arange(0, 30, 1)\n        phantom.save_video(time_steps, \"src/phantom/cfg_videos/moving_objects_one_static.mp4\")\n    return phantom\n</code></pre>"},{"location":"reference/nect/src/phantom/pore/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> pore","text":""},{"location":"reference/nect/src/phantom/pore/#nect.src.phantom.pore","title":"pore","text":""},{"location":"reference/nect/src/phantom/pore/#nect.src.phantom.pore.BlinkingCube","title":"BlinkingCube","text":"<pre><code>BlinkingCube(cube_position: tuple[int, int, int], size: int, period: int)\n</code></pre> <p>Creates a blinking cube. The cube will blink between air and water with a given period.</p> <p>Parameters:</p> Name Type Description Default <code>cube_position</code> <code>tuple[int, int, int]</code> <p>The position of the cube in the image. (top-left corner, zyx)</p> required <code>size</code> <code>int</code> <p>The size of the cube.</p> required <code>period</code> <code>int</code> <p>The period of the blinking. If the period is 10, then the cube will be air for 5 projections and water for 5 projections.</p> required Source code in <code>nect/src/phantom/pore.py</code> <pre><code>def __init__(self, cube_position: tuple[int, int, int], size: int, period: int) -&gt; None:\n    \"\"\"Creates a blinking cube. The cube will blink between air and water with a given period.\n\n    Args:\n        cube_position (tuple[int, int, int]): The position of the cube in the image. (top-left corner, zyx)\n        size (int): The size of the cube.\n        period (int): The period of the blinking. If the period is 10, then the cube will be air for 5 projections and water for 5 projections.\n    \"\"\"\n\n    self.cube_position = cube_position\n    self.size = size\n    self.period = period\n    assert period % 2 == 0, \"The period must be an even number.\"\n    self.half_period = period // 2\n    self.projs_since_last_blink = 0\n    self.blinking = False\n    self.attenuation = IntensityConfig.AIR\n</code></pre>"},{"location":"reference/nect/src/phantom/pore/#nect.src.phantom.pore.BlinkingCube.place_cube","title":"place_cube","text":"<pre><code>place_cube(img: ndarray) -&gt; ndarray\n</code></pre> <p>Places the cube in the image.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>The image to place the cube in.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The image with the cube placed.</p> Source code in <code>nect/src/phantom/pore.py</code> <pre><code>def place_cube(self, img: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Places the cube in the image.\n\n    Args:\n        img (np.ndarray): The image to place the cube in.\n\n    Returns:\n        np.ndarray: The image with the cube placed.\n    \"\"\"\n    # print(f\"Since blink: {self.projs_since_last_blink}\")\n    if self.projs_since_last_blink == self.half_period:\n        # print(\"Blinking\")\n        self.projs_since_last_blink = 1\n        self.blinking = not self.blinking\n    else:\n        self.projs_since_last_blink += 1\n    if self.blinking:\n        self.attenuation = IntensityConfig.WATER\n        cube = np.ones((self.size, self.size, self.size)) * self.attenuation\n        z, y, x = self.cube_position\n        img[z : z + self.size, y : y + self.size, x : x + self.size] = cube\n    return img\n</code></pre>"},{"location":"reference/nect/src/phantom/pore/#nect.src.phantom.pore.EmptyCanvas","title":"EmptyCanvas","text":"<pre><code>EmptyCanvas(size: tuple[int, int, int], cubes: list[BlinkingCube])\n</code></pre> <p>Creates an empty canvas with blinking cubes.</p> <p>Parameters:</p> Name Type Description Default <code>cubes</code> <code>list[BlinkingCube]</code> <p>A list of blinking cubes.</p> required Source code in <code>nect/src/phantom/pore.py</code> <pre><code>def __init__(self, size: tuple[int, int, int], cubes: list[BlinkingCube]) -&gt; None:\n    \"\"\"Creates an empty canvas with blinking cubes.\n\n    Args:\n        cubes (list[BlinkingCube]): A list of blinking cubes.\n    \"\"\"\n    self.cubes = cubes\n    self.size = size\n    self.base_img = np.ones(size)\n</code></pre>"},{"location":"reference/nect/src/phantom/pore/#nect.src.phantom.pore.MultiScalePorousMedium","title":"MultiScalePorousMedium","text":"<pre><code>MultiScalePorousMedium(medium1: PorousMedium, medium2: PorousMedium)\n</code></pre> <p>Creates a porous medium geometry that is the intersection of two porous medium geometries.</p> <p>Parameters:</p> Name Type Description Default <code>medium1</code> <code>PorousMedium</code> <p>The first porous medium.</p> required <code>medium2</code> <code>PorousMedium</code> <p>The second porous medium.</p> required Source code in <code>nect/src/phantom/pore.py</code> <pre><code>def __init__(self, medium1: PorousMedium, medium2: PorousMedium) -&gt; None:\n    \"\"\"Creates a porous medium geometry that is the intersection of two porous medium geometries.\n\n    Args:\n        medium1 (PorousMedium): The first porous medium.\n        medium2 (PorousMedium): The second porous medium.\n    \"\"\"\n    self.mask = ~(~medium1.mask * medium2.mask)\n</code></pre>"},{"location":"reference/nect/src/phantom/pore/#nect.src.phantom.pore.PorousMedium","title":"PorousMedium","text":"<pre><code>PorousMedium(shape: ndarray = (50, 50, 50), blobiness: list[int] = [1], porosity: float = 0.5, divs: int = 1)\n</code></pre> <p>Creates a porous medium geometry. Calling with divs &gt; 1 processes the porous medium in parallel.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>ndarray</code> <p>The shape of the porous medium. Defaults to (50,50,50).</p> <code>(50, 50, 50)</code> <code>blobiness</code> <code>list[int]</code> <p>The blobiness of the porous medium. Defaults to [1].</p> <code>[1]</code> <code>porosity</code> <code>float</code> <p>The porosity of the porous medium. Defaults to 0.5.</p> <code>0.5</code> <code>divs</code> <code>int</code> <p>The number of divisions to use in parallel computing. Defaults to 1.</p> <code>1</code> Source code in <code>nect/src/phantom/pore.py</code> <pre><code>def __init__(\n    self,\n    shape: np.ndarray = (50, 50, 50),\n    blobiness: list[int] = [1],\n    porosity: float = 0.5,\n    divs: int = 1,\n) -&gt; None:\n    \"\"\"Creates a porous medium geometry. Calling with divs &gt; 1 processes the porous medium in parallel.\n\n    Args:\n        shape (np.ndarray, optional): The shape of the porous medium. Defaults to (50,50,50).\n        blobiness (list[int], optional): The blobiness of the porous medium. Defaults to [1].\n        porosity (float, optional): The porosity of the porous medium. Defaults to 0.5.\n        divs (int, optional): The number of divisions to use in parallel computing. Defaults to 1.\n    \"\"\"\n    self.mask = ps.generators.blobs(shape=shape, porosity=porosity, blobiness=blobiness, divs=divs)\n</code></pre>"},{"location":"reference/nect/src/phantom/pore/#nect.src.phantom.pore.PorousMediumPhantom","title":"PorousMediumPhantom","text":"<pre><code>PorousMediumPhantom(geometry: ndarray, inlet: str = 'xyt', cylindrical: bool = False, poisson_noise: bool = False, dynamic: bool = True)\n</code></pre> <p>Creates a porous medium phantom based on the geometry (shape=(z, y, x)). The inlet defines the direction of the flow. \"xyt\" should be read as \"in the direction of the xy-plane from the top\" and \"yb\" as \"in the direction of the y-axis from the bottom\".</p> <p>Parameters:</p> Name Type Description Default <code>geometry</code> <code>ndarray</code> <p>The porous medium geometry. Can be created with the PorousMedium class.</p> required <code>inlet</code> <code>str</code> <p>A string defining the direction of inlet flow. Defaults to \"xyt\".</p> <code>'xyt'</code> <code>cylindrical</code> <code>bool</code> <p>Whether to create a cylindrical phantom. Defaults to False.</p> <code>False</code> Source code in <code>nect/src/phantom/pore.py</code> <pre><code>def __init__(\n    self,\n    geometry: np.ndarray,\n    inlet: str = \"xyt\",\n    cylindrical: bool = False,\n    poisson_noise: bool = False,\n    dynamic: bool = True,\n) -&gt; None:\n    \"\"\"Creates a porous medium phantom based on the geometry (shape=(z, y, x)). The inlet defines the direction of the flow.\n    \"xyt\" should be read as \"in the direction of the xy-plane from the top\" and \"yb\" as \"in the direction of the y-axis from the bottom\".\n\n    Args:\n        geometry (np.ndarray): The porous medium geometry. Can be created with the PorousMedium class.\n        inlet (str, optional): A string defining the direction of inlet flow. Defaults to \"xyt\".\n        cylindrical (bool, optional): Whether to create a cylindrical phantom. Defaults to False.\n    \"\"\"\n    self.is_3d = len(geometry.shape) == 3\n    self.inlet = inlet\n    self.size = geometry.shape\n    self.geometry = geometry\n    self.bd = self.create_inlet(inlet=inlet)\n    self.target = self.get_target(\n        scale=True\n    )  # When scale is True, the timescale of porous medium filling will always be between 0 and 1.\n    self.base_img = np.zeros_like(self.target)\n    self.base_img[~geometry] = IntensityConfig.ROCK  # Setting rock to 2 for contrast\n    self.steps = np.sort(np.unique(self.target)[1:])\n    self.cylindrical = cylindrical\n    self.poisson_noise = poisson_noise\n    self.dynamic = dynamic\n    # self.base_img[~self.geometry] = IntensityConfig.ROCK\n    if cylindrical:\n        self.cylindrical_geometry = ps.generators.cylindrical_plug(shape=self.size, r=self.size[-1] // 2, axis=0)\n</code></pre>"},{"location":"reference/nect/src/phantom/pore/#nect.src.phantom.pore.PorousMediumPhantom.create_inlet","title":"create_inlet","text":"<pre><code>create_inlet(inlet: str = 'xyt') -&gt; ndarray\n</code></pre> <p>Creates the inlet boundary condition based on the \"inlet\" string. It should be read as \"in the direction of the xy-plane from the top\" for \"xyt\" or \"in the direction of the y-axis from the bottom\" for \"yb\".</p> <p>Parameters:</p> Name Type Description Default <code>inlet</code> <code>str</code> <p>String defining the orientation. Defaults to \"xyt\".</p> <code>'xyt'</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the inlet string is not defined.</p> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array of the same shape as the geometry with True values at the inlet.</p> Source code in <code>nect/src/phantom/pore.py</code> <pre><code>def create_inlet(self, inlet: str = \"xyt\") -&gt; np.ndarray:\n    \"\"\"Creates the inlet boundary condition based on the \"inlet\" string.\n    It should be read as \"in the direction of the xy-plane from the top\" for \"xyt\"\n    or \"in the direction of the y-axis from the bottom\" for \"yb\".\n\n    Args:\n        inlet (str, optional): String defining the orientation. Defaults to \"xyt\".\n\n    Raises:\n        NotImplementedError: If the inlet string is not defined.\n\n    Returns:\n        np.ndarray: An array of the same shape as the geometry with True values at the inlet.\n    \"\"\"\n    bd = np.zeros_like(self.geometry, dtype=bool)\n    if self.is_3d:\n        if inlet == \"xyt\":  # xy-plane top\n            bd[0, :, :] = 1\n        elif inlet == \"xyb\":  # xy-plane bottom\n            bd[-1, :, :] = 1\n        else:\n            raise NotImplementedError(f\"Inlet {inlet} is not implemented.\")\n    else:\n        if inlet == \"xl\":  # x-axis left\n            bd[:, 0] = 1\n        elif inlet == \"xr\":  # x-axis right\n            bd[:, -1] = 1\n        elif inlet == \"yt\":  # y-axis top\n            bd[0, :] = 1\n        elif inlet == \"yb\":  # y-axis bottom\n            bd[-1, :] = 1\n        else:\n            raise NotImplementedError(f\"Inlet {inlet} is not implemented.\")\n\n    bd *= self.geometry\n    return bd\n</code></pre>"},{"location":"reference/nect/src/phantom/pore/#nect.src.phantom.pore.PorousMediumPhantom.create_video_as_tensor","title":"create_video_as_tensor","text":"<pre><code>create_video_as_tensor(time_steps: ndarray, save_frames=None, save_video=None) -&gt; Tensor\n</code></pre> <p>Creates a video of the phantom as a tensor.</p> <p>Parameters:</p> Name Type Description Default <code>time_steps</code> <code>ndarray</code> <p>The time steps of the video.</p> required <code>save_frames</code> <code>str</code> <p>Path to a directory where the frames should be saved. Defaults to None.</p> <code>None</code> <code>save_video</code> <code>str</code> <p>Path to a file where the video should be saved. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The video as a tensor. Has shape (time_steps, height, width).</p> Source code in <code>nect/src/phantom/pore.py</code> <pre><code>def create_video_as_tensor(self, time_steps: np.ndarray, save_frames=None, save_video=None) -&gt; torch.Tensor:\n    \"\"\"\n    Creates a video of the phantom as a tensor.\n\n    Args:\n        time_steps (np.ndarray): The time steps of the video.\n        save_frames (str, optional): Path to a directory where the frames should be saved. Defaults to None.\n        save_video (str, optional): Path to a file where the video should be saved. Defaults to None.\n\n    Returns:\n        torch.Tensor: The video as a tensor. Has shape (time_steps, height, width).\"\"\"\n    vid = np.zeros(shape=(len(time_steps), *self.size))\n    for i in tqdm(range(len(time_steps)), desc=\"Creating phantom images\"):\n        vid[i] = self.get_phantom(time_steps[i])\n        if save_frames is not None:\n            np.save(Path(save_frames) / f\"frame_{i}\", vid[i])\n    if save_video is not None:\n        np.save(Path(save_video), vid)\n    return torch.from_numpy(np.array(vid)).float()\n</code></pre>"},{"location":"reference/nect/src/phantom/pore/#nect.src.phantom.pore.PorousMediumPhantom.get_phantom","title":"get_phantom","text":"<pre><code>get_phantom(t: float, poisson_noise: bool = False, scaled: bool = True, blinking_cubes: list[BlinkingCube | RandomCube] | None = None) -&gt; ndarray\n</code></pre> <p>Fills the pores with fluid according to simulation results at time t.</p> <p>Parameters:</p> Name Type Description Default <code>t</code> <code>float</code> <p>Simulation time</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array with pores filled according to simulation results at time t.</p> Source code in <code>nect/src/phantom/pore.py</code> <pre><code>def get_phantom(\n    self,\n    t: float,\n    poisson_noise: bool = False,\n    scaled: bool = True,\n    blinking_cubes: list[BlinkingCube | RandomCube] | None = None,\n) -&gt; np.ndarray:\n    \"\"\"Fills the pores with fluid according to simulation results at time t.\n\n    Args:\n        t (float): Simulation time\n\n    Returns:\n        np.ndarray: An array with pores filled according to simulation results at time t.\n    \"\"\"\n    dynamic_img = self.base_img.copy()\n    # if self.dynamic:\n    if not self.dynamic:\n        t = 0\n    dynamic_img[np.logical_and(self.target &lt; t, self.target &gt; 0)] = (\n        IntensityConfig.WATER\n    )  # All cells with a value less than t are filled at time t\n    dynamic_img[~self.geometry] = IntensityConfig.ROCK  # Set the rock to 2 for contrast\n    if self.cylindrical:\n        dynamic_img *= self.cylindrical_geometry\n    if poisson_noise or self.poisson_noise:\n        noise_mask = np.random.poisson(\n            dynamic_img\n        )  # Poisson noise is not additive, but depends on the signal strength. This should be the correct way.\n        dynamic_img = dynamic_img + noise_mask\n\n    if blinking_cubes is not None:\n        for cube in blinking_cubes:\n            dynamic_img = cube.place_cube(dynamic_img)\n\n    if scaled:\n        dynamic_img += dynamic_img.min()\n        dynamic_img /= dynamic_img.max()\n    return dynamic_img\n</code></pre>"},{"location":"reference/nect/src/phantom/pore/#nect.src.phantom.pore.PorousMediumPhantom.get_target","title":"get_target","text":"<pre><code>get_target(scale=False) -&gt; ndarray\n</code></pre> <p>Returns an array which specifies when a portion of space should be filled. See https://porespy.org/examples/simulations/tutorials/drainage_with_gravity_advanced.html for details. Returns an array of saturation between 0 and 1. If there is unfilled pores in the phantom, the maximum saturation will be less than 1. Scaling between 0 and 1 can be forced by passing scale=True to the function.</p> Source code in <code>nect/src/phantom/pore.py</code> <pre><code>def get_target(self, scale=False) -&gt; np.ndarray:\n    \"\"\"Returns an array which specifies when a portion of space should be filled.\n    See https://porespy.org/examples/simulations/tutorials/drainage_with_gravity_advanced.html for details.\n    Returns an array of saturation between 0 and 1. If there is unfilled pores in the phantom, the maximum\n    saturation will be less than 1. Scaling between 0 and 1 can be forced by passing scale=True to the function.\"\"\"\n    # self.geometry = ps.filters.trim_disconnected_blobs(im=self.geometry, inlets=self.bd)  # Removes disconnected blobs, but we probably don't want this\n    out = ps.filters.ibip(im=self.geometry, inlets=self.bd, maxiter=15000)\n    inv_seq = out.inv_sequence\n    inv_satn = ps.filters.seq_to_satn(seq=inv_seq)\n    if scale:\n        inv_satn = inv_satn / np.max(inv_satn)\n    return np.around(inv_satn, decimals=3)\n</code></pre>"},{"location":"reference/nect/src/phantom/pore/#nect.src.phantom.pore.RandomCube","title":"RandomCube","text":"<pre><code>RandomCube(cube_position: tuple[int, int, int], size: int, start_time: int, on_time)\n</code></pre> <p>Creates a blinking cube. The cube will blink between air and water with a given period.</p> <p>Parameters:</p> Name Type Description Default <code>cube_position</code> <code>tuple[int, int, int]</code> <p>The position of the cube in the image. (top-left corner, zyx)</p> required <code>size</code> <code>int</code> <p>The size of the cube.</p> required <code>start_time</code> <code>int</code> <p>The time when the cube starts blinking.</p> required <code>on_time</code> <code>int</code> <p>The time the cube is on.</p> required Source code in <code>nect/src/phantom/pore.py</code> <pre><code>def __init__(self, cube_position: tuple[int, int, int], size: int, start_time: int, on_time) -&gt; None:\n    \"\"\"Creates a blinking cube. The cube will blink between air and water with a given period.\n\n    Args:\n        cube_position (tuple[int, int, int]): The position of the cube in the image. (top-left corner, zyx)\n        size (int): The size of the cube.\n        start_time (int): The time when the cube starts blinking.\n        on_time (int): The time the cube is on.\n    \"\"\"\n\n    self.cube_position = cube_position\n    self.size = size\n    self.attenuation = IntensityConfig.AIR\n    self.start_time = start_time\n    self.on_time = on_time\n    self.time = 0\n</code></pre>"},{"location":"reference/nect/src/phantom/pore/#nect.src.phantom.pore.RandomCube.place_cube","title":"place_cube","text":"<pre><code>place_cube(img: ndarray) -&gt; ndarray\n</code></pre> <p>Places the cube in the image.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>ndarray</code> <p>The image to place the cube in.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The image with the cube placed.</p> Source code in <code>nect/src/phantom/pore.py</code> <pre><code>def place_cube(self, img: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Places the cube in the image.\n\n    Args:\n        img (np.ndarray): The image to place the cube in.\n\n    Returns:\n        np.ndarray: The image with the cube placed.\n    \"\"\"\n    self.attenuation = IntensityConfig.AIR\n\n    if self.time &gt;= self.start_time and self.time &lt; self.start_time + self.on_time:\n        self.attenuation = IntensityConfig.WATER\n    self.time += 1\n    cube = np.ones((self.size, self.size, self.size)) * self.attenuation\n    z, y, x = self.cube_position\n    img[z : z + self.size, y : y + self.size, x : x + self.size] = cube\n    return img\n</code></pre>"},{"location":"reference/nect/src/phantom/utils/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> utils","text":""},{"location":"reference/nect/src/phantom/utils/#nect.src.phantom.utils","title":"utils","text":""},{"location":"reference/nect/src/phantom/utils/#nect.src.phantom.utils.create_random_closed_geometry","title":"create_random_closed_geometry","text":"<pre><code>create_random_closed_geometry(size=(100, 100)) -&gt; CustomGeometry\n</code></pre> <p>Creates a random closed geometry.</p> <p>Parameters:</p> Name Type Description Default <code>size</code> <code>tuple[int, int]</code> <p>The size of the geometry. Defaults to (100,100).</p> <code>(100, 100)</code> <p>Returns:</p> Name Type Description <code>CustomGeometry</code> <code>CustomGeometry</code> <p>The random closed geometry.</p> Source code in <code>nect/src/phantom/utils.py</code> <pre><code>def create_random_closed_geometry(size=(100, 100)) -&gt; CustomGeometry:\n    \"\"\"Creates a random closed geometry.\n\n    Args:\n        size (tuple[int,int], optional): The size of the geometry. Defaults to (100,100).\n\n    Returns:\n        CustomGeometry: The random closed geometry.\n    \"\"\"\n    # create a random geometry with more 0 than 1\n    geometry = np.random.choice([0, 1], size=size, p=[0.7, 0.3])\n    # close the geometry\n    geometry = ndimage.binary_closing(geometry, border_value=0, iterations=1)\n    # create a custom geometry\n    return CustomGeometry(mask=geometry)\n</code></pre>"},{"location":"reference/nect/src/reconstruction/","title":"Index","text":""},{"location":"reference/nect/src/reconstruction/#nect.src.reconstruction","title":"reconstruction","text":""},{"location":"reference/nect/src/reconstruction/leap/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> leap","text":""},{"location":"reference/nect/src/reconstruction/leap/#nect.src.reconstruction.leap","title":"leap","text":""},{"location":"reference/nect/src/reconstruction/reconstructor/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> reconstructor","text":""},{"location":"reference/nect/src/reconstruction/reconstructor/#nect.src.reconstruction.reconstructor","title":"reconstructor","text":""},{"location":"reference/nect/src/reconstruction/reconstructor/#nect.src.reconstruction.reconstructor.Reconstructor","title":"Reconstructor","text":"<pre><code>Reconstructor(n_projs_per_frame: int, method: str = 'fbp_skimage', sample_size=tuple[int], *args, **kwargs)\n</code></pre> <p>A wrapper class for all reconstruction methods. Reconstruction method specific arguments can be passed in *args and **kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>n_projs_per_frame</code> <code>int</code> <p>How many projections to use for each reconstructed frame.</p> required <code>method</code> <code>str</code> <p>The reconstruction method. Defaults to \"fbp_skimage\".</p> <code>'fbp_skimage'</code> <code>sample_size</code> <code>tuple[int]</code> <p>The size/shape of the phantom. Defaults to tuple[int].</p> <code>tuple[int]</code> Source code in <code>nect/src/reconstruction/reconstructor.py</code> <pre><code>def __init__(\n    self,\n    n_projs_per_frame: int,\n    method: str = \"fbp_skimage\",\n    sample_size=tuple[int],\n    *args,\n    **kwargs,\n):\n    \"\"\"A wrapper class for all reconstruction methods. Reconstruction method specific arguments can be passed in *args and **kwargs.\n\n    Args:\n        n_projs_per_frame (int): How many projections to use for each reconstructed frame.\n        method (str, optional): The reconstruction method. Defaults to \"fbp_skimage\".\n        sample_size (tuple[int], optional): The size/shape of the phantom. Defaults to tuple[int].\n    \"\"\"\n    self.supported_frameworks = [\"skimage\", \"tigre\", \"leaptorch\"]\n    self.framework = self.resolve_framework(method)\n    self.supported_methods = [\n        \"fbp_skimage\",\n        \"fdk_tigre\",\n        \"ossart_tigre\",\n        \"fbp_leaptorch\",\n    ]\n    self.method = self.resolve_method(method)\n    self.n_projs_per_frame = n_projs_per_frame\n    self.sample_size = sample_size\n    self.args = args\n    self.kwargs = kwargs\n</code></pre>"},{"location":"reference/nect/src/reconstruction/reconstructor/#nect.src.reconstruction.reconstructor.Reconstructor.reconstruct","title":"reconstruct","text":"<pre><code>reconstruct(sinogram, theta) -&gt; ndarray\n</code></pre> <p>Divides the sinogram into frames and reconstructs each frame using the specified reconstruction method.</p> <p>Parameters:</p> Name Type Description Default <code>sinogram</code> <code>ndarray | Tensor</code> <p>The sinogram to reconstruct. Shape=[nprojs, ...]</p> required <code>theta</code> <code>ndarray | Tensor</code> <p>The angles of the sinogram.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The reconstructed time series.</p> Source code in <code>nect/src/reconstruction/reconstructor.py</code> <pre><code>def reconstruct(self, sinogram, theta) -&gt; np.ndarray:\n    \"\"\"Divides the sinogram into frames and reconstructs each frame using the specified reconstruction method.\n\n    Args:\n        sinogram (np.ndarray | torch.Tensor): The sinogram to reconstruct. Shape=[nprojs, ...]\n        theta (np.ndarray | torch.Tensor): The angles of the sinogram.\n\n    Returns:\n        np.ndarray: The reconstructed time series.\n    \"\"\"\n    steps = np.arange(0, sinogram.shape[0], self.n_projs_per_frame)\n    if (np.max(steps) + self.n_projs_per_frame) &gt; sinogram.shape[0]:\n        print(\n            f\"The {(np.max(steps) + self.n_projs_per_frame) - sinogram.shape[0]} last projections will be ignored. Consider adjusting the number of projections per frame.\"\n        )\n        reconstruction = np.zeros((len(steps) - 1, *self.sample_size))\n        steps = steps[:-1]\n    else:\n        reconstruction = np.zeros((len(steps), *self.sample_size))\n    if self.framework == \"skimage\":  # Get sinogram on skimage format: [..., nprojs]\n        sinogram = np.transpose(sinogram, axes=(1, 0))\n        for i, step in enumerate(steps):\n            reconstruction[i] = self.method(\n                sinogram[..., step : step + self.n_projs_per_frame],\n                theta[step : step + self.n_projs_per_frame],\n                *self.args,\n                **self.kwargs,\n            )\n    else:\n        for i, step in enumerate(steps):\n            reconstruction[i] = self.method(\n                sinogram[step : step + self.n_projs_per_frame, ...],\n                theta[step : step + self.n_projs_per_frame],\n                *self.args,\n                **self.kwargs,\n            )\n    return reconstruction\n</code></pre>"},{"location":"reference/nect/src/reconstruction/reconstructor/#nect.src.reconstruction.reconstructor.Reconstructor.resolve_method","title":"resolve_method","text":"<pre><code>resolve_method(method: str)\n</code></pre> <p>Returns the correct method based on the provided method name.</p> Source code in <code>nect/src/reconstruction/reconstructor.py</code> <pre><code>def resolve_method(self, method: str):\n    \"\"\"Returns the correct method based on the provided method name.\"\"\"\n    if method == \"fbp_skimage\":\n        return scikit_image.fbp\n    elif method == \"fdk_tigre\":\n        return tigre_toolbox.fdk\n    elif method == \"ossart_tigre\":\n        return tigre_toolbox.ossart\n    elif method == \"fbp_leaptorch\":\n        return leap.fbp\n    else:\n        raise NotImplementedError(f\"Method {method} not supported. Supported methods are: {self.supported_methods}\")\n</code></pre>"},{"location":"reference/nect/src/reconstruction/scikit_image/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> scikit_image","text":""},{"location":"reference/nect/src/reconstruction/scikit_image/#nect.src.reconstruction.scikit_image","title":"scikit_image","text":""},{"location":"reference/nect/src/reconstruction/tigre_toolbox/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> tigre_toolbox","text":""},{"location":"reference/nect/src/reconstruction/tigre_toolbox/#nect.src.reconstruction.tigre_toolbox","title":"tigre_toolbox","text":""},{"location":"reference/nect/src/reconstruction/tigre_toolbox/#nect.src.reconstruction.tigre_toolbox.fdk","title":"fdk","text":"<pre><code>fdk(sinogram, theta, geo=None, *args, **kwargs) -&gt; ndarray\n</code></pre> <p>FDK reconstruction using the TIGRE toolbox.</p> <p>Parameters:</p> Name Type Description Default <code>sinogram</code> <code>ndarray</code> <p>A sinogram.</p> required <code>geo</code> <code>Geometry</code> <p>A TIGRE Geometry object.</p> <code>None</code> <code>theta</code> <code>ndarray</code> <p>The angles of the sinogram.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The reconstructed image.</p> Source code in <code>nect/src/reconstruction/tigre_toolbox.py</code> <pre><code>def fdk(sinogram, theta, geo=None, *args, **kwargs) -&gt; np.ndarray:\n    \"\"\"FDK reconstruction using the TIGRE toolbox.\n\n    Args:\n        sinogram (np.ndarray): A sinogram.\n        geo (Geometry): A TIGRE Geometry object.\n        theta (np.ndarray): The angles of the sinogram.\n\n    Returns:\n        np.ndarray: The reconstructed image.\n    \"\"\"\n    if geo is None:\n        raise ValueError(\"Geometry is None\")\n    elif isinstance(geo, TigreGeometry):\n        geo = geo.geo\n    return algs.fdk(sinogram, geo, angles=theta, *args, **kwargs)\n</code></pre>"},{"location":"reference/nect/src/reconstruction/tigre_toolbox/#nect.src.reconstruction.tigre_toolbox.ossart","title":"ossart","text":"<pre><code>ossart(sinogram, geo, theta, niter, *args, **kwargs) -&gt; ndarray\n</code></pre> <p>OSSART reconstruction using the TIGRE toolbox.</p> <p>Parameters:</p> Name Type Description Default <code>sinogram</code> <code>ndarray</code> <p>A sinogram.</p> required <code>geo</code> <code>Geometry</code> <p>A TIGRE Geometry object.</p> required <code>theta</code> <code>ndarray</code> <p>The angles of the sinogram.</p> required <code>niter</code> <code>int</code> <p>Number of iterations.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The reconstructed image.</p> Source code in <code>nect/src/reconstruction/tigre_toolbox.py</code> <pre><code>def ossart(sinogram, geo, theta, niter, *args, **kwargs) -&gt; np.ndarray:\n    \"\"\"OSSART reconstruction using the TIGRE toolbox.\n\n    Args:\n        sinogram (np.ndarray): A sinogram.\n        geo (Geometry): A TIGRE Geometry object.\n        theta (np.ndarray): The angles of the sinogram.\n        niter (int): Number of iterations.\n\n    Returns:\n        np.ndarray: The reconstructed image.\n    \"\"\"\n    if geo is None:\n        raise ValueError(\"Geometry is None\")\n    elif isinstance(geo, TigreGeometry):\n        geo = geo.geo\n    return algs.ossart(sinogram, geo, angles=theta, niter=niter, *args, **kwargs)\n</code></pre>"},{"location":"reference/nect/src/sampling/","title":"Index","text":""},{"location":"reference/nect/src/sampling/#nect.src.sampling","title":"sampling","text":""},{"location":"reference/nect/src/sampling/leap/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> leap","text":""},{"location":"reference/nect/src/sampling/leap/#nect.src.sampling.leap","title":"leap","text":""},{"location":"reference/nect/src/sampling/leap/#nect.src.sampling.leap.dynamic_equidistant_sampling","title":"dynamic_equidistant_sampling","text":"<pre><code>dynamic_equidistant_sampling(dynamic_image, scheduler, nprojs: int, nrevs: int, radians: bool = False, proj: LeapGeometry = None, device: device = None, *args, **kwargs) -&gt; ndarray\n</code></pre> <p>Sample a dynamic image using the TIGRE toolbox and the equidistant sampling.</p> <p>Parameters:</p> Name Type Description Default <code>dynamic_image</code> <code>DynamicImage</code> <p>Dynamic image.</p> required <code>scheduler</code> <code>Scheduler</code> <p>Scheduler object.</p> required <code>nprojs</code> <code>int</code> <p>Number of projections.</p> required <code>nrevs</code> <code>int</code> <p>Number of revolutions.</p> required <code>radians</code> <code>bool</code> <p>Wheter to use radians. Defaults to True.</p> <code>False</code> <code>proj</code> <code>LeapGeometry</code> <p>Geometry object from LEAP. Defaults to None.</p> <code>None</code> <code>device</code> <code>device</code> <p>Device to use. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array containing the sinogram.</p> Source code in <code>nect/src/sampling/leap.py</code> <pre><code>def dynamic_equidistant_sampling(\n    dynamic_image,\n    scheduler,\n    nprojs: int,\n    nrevs: int,\n    radians: bool = False,\n    proj: LeapGeometry = None,\n    device: torch.device = None,\n    *args,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"Sample a dynamic image using the TIGRE toolbox and the equidistant sampling.\n\n    Args:\n        dynamic_image (DynamicImage): Dynamic image.\n        scheduler (Scheduler): Scheduler object.\n        nprojs (int): Number of projections.\n        nrevs (int): Number of revolutions.\n        radians (bool, optional): Wheter to use radians. Defaults to True.\n        proj (LeapGeometry, optional): Geometry object from LEAP. Defaults to None.\n        device (torch.device, optional): Device to use. Defaults to None.\n\n    Returns:\n        np.ndarray: An array containing the sinogram.\n    \"\"\"\n    print(\"[WARNING] LeapTorch arrays are shifted by -n_projs//4. This might change in the future\")\n    time_series = np.zeros((nrevs * nprojs, proj.numRows, proj.numCols))  # Initialize sinogram\n    time = 0\n    theta = torch.from_numpy(equidistant(nprojs=nprojs, nrevs=nrevs, radians=radians)).float()\n    for i in tqdm(range(len(theta)), desc=\"Sampling\"):\n        phantom = torch.from_numpy(dynamic_image.get_phantom(float(time))).to(device).unsqueeze(0).float()\n        proj.update_phi(theta[[i]])\n        time_series[i, ...] = proj(phantom).squeeze(0).detach().cpu().numpy()\n        time = scheduler.rotate(theta[i], theta[i + 1]) if i &lt; len(theta) - 1 else time\n    return time_series, theta\n</code></pre>"},{"location":"reference/nect/src/sampling/leap/#nect.src.sampling.leap.dynamic_hybrid_golden_angle_sampling","title":"dynamic_hybrid_golden_angle_sampling","text":"<pre><code>dynamic_hybrid_golden_angle_sampling(dynamic_image, scheduler, nprojs: int, nrevs: int, radians: bool = False, proj: LeapGeometry = None, device: device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu'), *args, **kwargs) -&gt; ndarray\n</code></pre> <p>Sample a dynamic image using the TIGRE toolbox and the golden ratio sampling.</p> <p>Parameters:</p> Name Type Description Default <code>dynamic_image</code> <code>DynamicImage</code> <p>Dynamic image.</p> required <code>scheduler</code> <code>Scheduler</code> <p>Scheduler object.</p> required <code>nprojs</code> <code>int</code> <p>Number of projections.</p> required <code>nrevs</code> <code>int</code> <p>Number of revolutions</p> required <code>proj</code> <code>Geometry</code> <p>Geometry object from LEAP.</p> <code>None</code> <code>radians</code> <code>bool</code> <p>Wheter to use radians. Defaults to True.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array containing the sinogram.</p> Source code in <code>nect/src/sampling/leap.py</code> <pre><code>def dynamic_hybrid_golden_angle_sampling(\n    dynamic_image,\n    scheduler,\n    nprojs: int,\n    nrevs: int,\n    radians: bool = False,\n    proj: LeapGeometry = None,\n    device: torch.device = torch.device(\"cuda\") if torch.cuda.is_available else torch.device(\"cpu\"),\n    *args,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"Sample a dynamic image using the TIGRE toolbox and the golden ratio sampling.\n\n    Args:\n        dynamic_image (DynamicImage): Dynamic image.\n        scheduler (Scheduler): Scheduler object.\n        nprojs (int): Number of projections.\n        nrevs (int): Number of revolutions\n        proj (Geometry): Geometry object from LEAP.\n        radians (bool, optional): Wheter to use radians. Defaults to True.\n\n    Returns:\n        np.ndarray: An array containing the sinogram.\n    \"\"\"\n    print(\"[WARNING] LeapTorch arrays are shifted by -n_projs//4. This might change in the future\")\n    time_series = np.zeros((nrevs * nprojs, proj.numRows, proj.numCols))  # Initialize sinogram\n    time = 0\n    theta = torch.from_numpy(hybrid_golden_angle(nprojs=nprojs, nrevs=nrevs, radians=radians)).float()\n    for i in tqdm(range(len(theta)), desc=\"Sampling: \"):\n        phantom = torch.from_numpy(dynamic_image.get_phantom(float(time))).to(device).unsqueeze(0).float()\n        proj.update_phi(theta[[i]])\n        time_series[i, ...] = proj(phantom).squeeze(0).detach().cpu().numpy()\n        time = scheduler.rotate(theta[i], theta[i + 1]) if i &lt; len(theta) - 1 else time\n    return time_series, theta.detach().cpu().numpy()\n</code></pre>"},{"location":"reference/nect/src/sampling/leap/#nect.src.sampling.leap.dynamic_hybrid_golden_angle_sampling_linear_time","title":"dynamic_hybrid_golden_angle_sampling_linear_time","text":"<pre><code>dynamic_hybrid_golden_angle_sampling_linear_time(dynamic_image, scheduler, nprojs: int, nrevs: int, radians: bool = False, proj: LeapGeometry = None, device: device = torch.device('cuda') if torch.cuda.is_available else torch.device('cpu'), *args, **kwargs) -&gt; ndarray\n</code></pre> <p>Sample a dynamic image using the TIGRE toolbox and the golden ratio sampling.</p> <p>Parameters:</p> Name Type Description Default <code>dynamic_image</code> <code>DynamicImage</code> <p>Dynamic image.</p> required <code>scheduler</code> <code>Scheduler</code> <p>Scheduler object.</p> required <code>nprojs</code> <code>int</code> <p>Number of projections.</p> required <code>nrevs</code> <code>int</code> <p>Number of revolutions</p> required <code>proj</code> <code>Geometry</code> <p>Geometry object from LEAP.</p> <code>None</code> <code>radians</code> <code>bool</code> <p>Wheter to use radians. Defaults to True.</p> <code>False</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array containing the sinogram.</p> Source code in <code>nect/src/sampling/leap.py</code> <pre><code>def dynamic_hybrid_golden_angle_sampling_linear_time(\n    dynamic_image,\n    scheduler,\n    nprojs: int,\n    nrevs: int,\n    radians: bool = False,\n    proj: LeapGeometry = None,\n    device: torch.device = torch.device(\"cuda\") if torch.cuda.is_available else torch.device(\"cpu\"),\n    *args,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"Sample a dynamic image using the TIGRE toolbox and the golden ratio sampling.\n\n    Args:\n        dynamic_image (DynamicImage): Dynamic image.\n        scheduler (Scheduler): Scheduler object.\n        nprojs (int): Number of projections.\n        nrevs (int): Number of revolutions\n        proj (Geometry): Geometry object from LEAP.\n        radians (bool, optional): Wheter to use radians. Defaults to True.\n\n    Returns:\n        np.ndarray: An array containing the sinogram.\n    \"\"\"\n    print(\"[WARNING] LeapTorch arrays are shifted by -n_projs//4. This might change in the future\")\n    time_series = np.zeros((nrevs * nprojs, proj.numRows, proj.numCols))  # Initialize sinogram\n    time = np.linspace(0, 1, num=nprojs * nrevs, endpoint=True)\n    theta = torch.from_numpy(hybrid_golden_angle(nprojs=nprojs, nrevs=nrevs, radians=radians)).float()\n    for i in tqdm(range(len(theta)), desc=\"Sampling: \"):\n        phantom = (\n            torch.from_numpy(dynamic_image.get_phantom(float(time[i]), blinking_cubes=kwargs.get(\"blinking_cubes\")))\n            .to(device)\n            .unsqueeze(0)\n            .float()\n        )\n        proj.update_phi(theta[[i]])\n        time_series[i, ...] = proj(phantom).squeeze(0).detach().cpu().numpy()\n    return time_series, theta.detach().cpu().numpy()\n</code></pre>"},{"location":"reference/nect/src/sampling/leap/#nect.src.sampling.leap.equidistant_sampling","title":"equidistant_sampling","text":"<pre><code>equidistant_sampling(image, nprojs: int, nrevs: int, proj: Projector = None, radians=False, device=torch.device('cpu'), t: float = 0, *args, **kwargs) -&gt; ndarray\n</code></pre> <p>Sample a static image using the TIGRE toolbox.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Static 3D image.</p> required <code>nprojs</code> <code>int</code> <p>Number of projections.</p> required <code>nrevs</code> <code>int</code> <p>Number of revolutions.</p> required <code>radians</code> <code>bool</code> <p>Wheter to use radians. Defaults to True.</p> <code>False</code> <code>proj</code> <code>Projector</code> <p>Projector object from LEAP. Defaults to None.</p> <code>None</code> <code>device</code> <code>device</code> <p>Device to use. Defaults to torch.device(\"cpu\").</p> <code>device('cpu')</code> <code>t</code> <code>float</code> <p>Time of the image. Defaults to 0.</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array containing the sinogram.</p> Source code in <code>nect/src/sampling/leap.py</code> <pre><code>def equidistant_sampling(\n    image,\n    nprojs: int,\n    nrevs: int,\n    proj: Projector = None,\n    radians=False,\n    device=torch.device(\"cpu\"),\n    t: float = 0,\n    *args,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"Sample a static image using the TIGRE toolbox.\n\n    Args:\n        image (np.ndarray): Static 3D image.\n        nprojs (int): Number of projections.\n        nrevs (int): Number of revolutions.\n        radians (bool, optional): Wheter to use radians. Defaults to True.\n        proj (Projector, optional): Projector object from LEAP. Defaults to None.\n        device (torch.device, optional): Device to use. Defaults to torch.device(\"cpu\").\n        t (float, optional): Time of the image. Defaults to 0.\n\n    Returns:\n        np.ndarray: An array containing the sinogram.\n    \"\"\"\n    print(\"[WARNING] LeapTorch arrays are shifted by -n_projs//4. This might change in the future\")\n    theta = torch.from_numpy(equidistant(nprojs=nprojs, nrevs=nrevs, radians=radians)).float()\n    proj.update_phi(theta)\n    img = torch.from_numpy(image.get_phantom(t)).to(device).unsqueeze(0).float()\n    return proj(img).squeeze(0).detach().cpu().numpy(), theta.detach().cpu().numpy()\n</code></pre>"},{"location":"reference/nect/src/sampling/methods/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> methods","text":""},{"location":"reference/nect/src/sampling/methods/#nect.src.sampling.methods","title":"methods","text":""},{"location":"reference/nect/src/sampling/methods/#nect.src.sampling.methods.equidistant","title":"equidistant","text":"<pre><code>equidistant(nprojs: int, nrevs: int, radians: bool = True) -&gt; ndarray\n</code></pre> <p>Generates a set of angles using the equidistant method</p> <p>Parameters:</p> Name Type Description Default <code>nprojs</code> <code>int</code> <p>The number of projections per revolution.</p> required <code>nrevs</code> <code>int</code> <p>The number of revolutions.</p> required <code>radians</code> <code>bool</code> <p>Whether to return angles in radians. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The equiditant angles.</p> Source code in <code>nect/src/sampling/methods.py</code> <pre><code>def equidistant(nprojs: int, nrevs: int, radians: bool = True) -&gt; np.ndarray:\n    \"\"\"Generates a set of angles using the equidistant method\n\n    Args:\n        nprojs (int): The number of projections per revolution.\n        nrevs (int): The number of revolutions.\n        radians (bool, optional): Whether to return angles in radians. Defaults to True.\n\n    Returns:\n        np.ndarray: The equiditant angles.\n    \"\"\"\n    start = 0\n    end = 360 * nrevs\n    angles = np.linspace(start, end, nprojs * nrevs, endpoint=False)\n\n    if radians:\n        return angles * np.pi / 180\n    else:\n        return angles\n</code></pre>"},{"location":"reference/nect/src/sampling/methods/#nect.src.sampling.methods.golden_angle","title":"golden_angle","text":"<pre><code>golden_angle(nprojs: int, radians: bool = True) -&gt; ndarray\n</code></pre> <p>Generates a set of angles using the golden angle method</p> <p>Parameters:</p> Name Type Description Default <code>nprojs</code> <code>int</code> <p>The number of projections per revolution.</p> required <code>radians</code> <code>bool</code> <p>Whether to return angles in radians. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The angles sampled by the golden angle method.</p> Source code in <code>nect/src/sampling/methods.py</code> <pre><code>def golden_angle(nprojs: int, radians: bool = True) -&gt; np.ndarray:\n    \"\"\"Generates a set of angles using the golden angle method\n\n    Args:\n        nprojs (int): The number of projections per revolution.\n        radians (bool, optional): Whether to return angles in radians. Defaults to True.\n\n    Returns:\n        np.ndarray: The angles sampled by the golden angle method.\n    \"\"\"\n    angles = np.arange(nprojs) * 360 / scipy.constants.golden_ratio\n    if radians:\n        return angles * np.pi / 180\n    else:\n        return angles\n</code></pre>"},{"location":"reference/nect/src/sampling/sampler/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> sampler","text":""},{"location":"reference/nect/src/sampling/sampler/#nect.src.sampling.sampler","title":"sampler","text":""},{"location":"reference/nect/src/sampling/sampler/#nect.src.sampling.sampler.Sampler","title":"Sampler","text":"<pre><code>Sampler(method: str, scheduler: Scheduler, reset_time: bool = True, ct_noise: bool = False, *args, **kwargs)\n</code></pre> <p>A wrapper class for sampling a phantom. Sampling method specific arguments can be passed in *args and **kwargs.</p> <p>Parameters:</p> Name Type Description Default <code>method</code> <code>str</code> <p>The sampling method.</p> required <code>scheduler</code> <code>Scheduler</code> <p>A Scheduler defining how time and thus the phantom evolves during sampling.</p> required <code>reset_time</code> <code>bool</code> <p>Whether to reset the internal scheduler time between each sampling. Defaults to True.</p> <code>True</code> Source code in <code>nect/src/sampling/sampler.py</code> <pre><code>def __init__(\n    self,\n    method: str,\n    scheduler: Scheduler,\n    reset_time: bool = True,\n    ct_noise: bool = False,\n    *args,\n    **kwargs,\n):\n    \"\"\"A wrapper class for sampling a phantom. Sampling method specific arguments can be passed in *args and **kwargs.\n\n    Args:\n        method (str): The sampling method.\n        scheduler (Scheduler): A Scheduler defining how time and thus the phantom evolves during sampling.\n        reset_time (bool, optional): Whether to reset the internal scheduler time between each sampling. Defaults to True.\n    \"\"\"\n    self.supported_frameworks = [\"skimage\", \"tigre\", \"leaptorch\"]\n    self.framework = self.resolve_framework(method)\n    self.supported_sampling_methods = [\n        \"dynamic_equidistant_skimage\",\n        \"dynamic_golden_angle_skimage\",\n        \"equidistant_skimage\",\n        \"golden_angle_skimage\",\n        \"dynamic_equidistant_tigre\",\n        \"dynamic_golden_angle_tigre\",\n        \"equidistant_tigre\",\n        \"golden_angle_tigre\",\n        \"dynamic_equidistant_leaptorch\",\n        \"equidistant_leaptorch\",\n        \"dynamic_hybrid_golden_angle_leaptorch\",\n    ]\n    self.method = self.resolve_method(method)\n    self.scheduler = scheduler\n    self.reset_time = reset_time\n    self.ct_noise = ct_noise\n    self.args = args\n    self.kwargs = kwargs\n</code></pre>"},{"location":"reference/nect/src/sampling/sampler/#nect.src.sampling.sampler.Sampler.resolve_method","title":"resolve_method","text":"<pre><code>resolve_method(method: str)\n</code></pre> <p>Returns the correct method based on the provided method name.</p> Source code in <code>nect/src/sampling/sampler.py</code> <pre><code>def resolve_method(self, method: str):\n    \"\"\"Returns the correct method based on the provided method name.\"\"\"\n    # Skimage methods (2D)\n    if method == \"dynamic_equidistant_skimage\":\n        return scikit_image.dynamic_equidistant_sampling\n    elif method == \"dynamic_golden_angle_skimage\":\n        return scikit_image.dynamic_golden_angle_sampling\n    elif method == \"equidistant_skimage\":\n        return scikit_image.equidistant_sampling\n    elif method == \"golden_angle_skimage\":\n        return scikit_image.golden_angle_sampling\n\n    # Tigre methods (3D)\n    elif method == \"dynamic_equidistant_tigre\":\n        return tigre_toolbox.dynamic_equidistant_sampling\n    elif method == \"dynamic_golden_angle_tigre\":\n        return tigre_toolbox.dynamic_golden_angle_sampling\n    elif method == \"equidistant_tigre\":\n        return tigre_toolbox.equidistant_sampling\n    elif method == \"golden_angle_tigre\":\n        return tigre_toolbox.golden_angle_sampling\n\n    elif method == \"dynamic_equidistant_leaptorch\":\n        return leap.dynamic_equidistant_sampling\n    elif method == \"equidistant_leaptorch\":\n        return leap.equidistant_sampling\n    elif method == \"dynamic_hybrid_golden_angle_leaptorch\":\n        return leap.dynamic_hybrid_golden_angle_sampling\n    elif method == \"dynamic_hybrid_golden_angle_linear_time_leaptorch\":\n        return leap.dynamic_hybrid_golden_angle_sampling_linear_time\n    else:\n        raise NotImplementedError(\n            f\"Method {method} not supported. Supported methods are: {self.supported_sampling_methods}\"\n        )\n</code></pre>"},{"location":"reference/nect/src/sampling/sampler/#nect.src.sampling.sampler.Sampler.sample","title":"sample","text":"<pre><code>sample(phantom)\n</code></pre> <p>Samples the phantom using the specified sampling method. Returns the sinogram on the format [nproj, ...]</p> Source code in <code>nect/src/sampling/sampler.py</code> <pre><code>def sample(self, phantom):\n    \"\"\"Samples the phantom using the specified sampling method. Returns the sinogram on the format [nproj, ...]\"\"\"\n    if self.reset_time:\n        self.scheduler.reset_time()\n    sinogram, theta = self.method(phantom, scheduler=self.scheduler, *self.args, **self.kwargs)\n    if self.framework == \"skimage\":  # skimage uses the convension [shape, nprojs]. We want [nprojs, shape]\n        sinogram = np.transpose(sinogram, axes=(1, 0))  # skimage sinograms are always 2D\n    if self.ct_noise:\n        sinogram = self.apply_ct_noise(sinogram.astype(np.float32))\n    return sinogram, theta\n</code></pre>"},{"location":"reference/nect/src/sampling/scikit_image/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> scikit_image","text":""},{"location":"reference/nect/src/sampling/scikit_image/#nect.src.sampling.scikit_image","title":"scikit_image","text":""},{"location":"reference/nect/src/sampling/scikit_image/#nect.src.sampling.scikit_image.equidistant_sampling","title":"equidistant_sampling","text":"<pre><code>equidistant_sampling(image, nprojs, nrevs, radians=False, *args, **kwargs)\n</code></pre> <p>Equidistant sampling of the image. :param image: 2D image :param n_samples: number of samples. Use at least 360 for \"full\" sampling. :return: sinogram of sampled image</p> Source code in <code>nect/src/sampling/scikit_image.py</code> <pre><code>def equidistant_sampling(image, nprojs, nrevs, radians=False, *args, **kwargs):\n    \"\"\"\n    Equidistant sampling of the image.\n    :param image: 2D image\n    :param n_samples: number of samples. Use at least 360 for \"full\" sampling.\n    :return: sinogram of sampled image\n    \"\"\"\n    theta = equidistant(nprojs=nprojs, nrevs=nrevs, radians=radians)\n    return radon(image, theta=theta, circle=True), theta\n</code></pre>"},{"location":"reference/nect/src/sampling/scikit_image/#nect.src.sampling.scikit_image.golden_angle_sampling","title":"golden_angle_sampling","text":"<pre><code>golden_angle_sampling(image, nprojs, radians=False, *args, **kwargs)\n</code></pre> <p>Golden angle sampling of the image. :param image: 2D image :param n_samples: number of samples. :return: sinogram of sampled image</p> Source code in <code>nect/src/sampling/scikit_image.py</code> <pre><code>def golden_angle_sampling(image, nprojs, radians=False, *args, **kwargs):\n    \"\"\"\n    Golden angle sampling of the image.\n    :param image: 2D image\n    :param n_samples: number of samples.\n    :return: sinogram of sampled image\n    \"\"\"\n    theta = golden_angle(nprojs=nprojs, radians=radians)\n    return radon(image, theta=theta, circle=True), theta\n</code></pre>"},{"location":"reference/nect/src/sampling/tigre_toolbox/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> tigre_toolbox","text":""},{"location":"reference/nect/src/sampling/tigre_toolbox/#nect.src.sampling.tigre_toolbox","title":"tigre_toolbox","text":""},{"location":"reference/nect/src/sampling/tigre_toolbox/#nect.src.sampling.tigre_toolbox.dynamic_equidistant_sampling","title":"dynamic_equidistant_sampling","text":"<pre><code>dynamic_equidistant_sampling(dynamic_image, scheduler, nprojs: int, nrevs: int, geo: Geometry = None, radians=True, *args, **kwargs) -&gt; ndarray\n</code></pre> <p>Sample a dynamic image using the TIGRE toolbox and the equidistant sampling.</p> <p>Parameters:</p> Name Type Description Default <code>dynamic_image</code> <code>DynamicImage</code> <p>Dynamic image.</p> required <code>scheduler</code> <code>Scheduler</code> <p>Scheduler object.</p> required <code>nprojs</code> <code>int</code> <p>Number of projections.</p> required <code>nrevs</code> <code>int</code> <p>Number of revolutions.</p> required <code>geo</code> <code>Geometry</code> <p>Geometry object from TIGRE.</p> <code>None</code> <code>radians</code> <code>bool</code> <p>Wheter to use radians. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array containing the sinogram.</p> Source code in <code>nect/src/sampling/tigre_toolbox.py</code> <pre><code>def dynamic_equidistant_sampling(\n    dynamic_image,\n    scheduler,\n    nprojs: int,\n    nrevs: int,\n    geo: Geometry = None,\n    radians=True,\n    *args,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"Sample a dynamic image using the TIGRE toolbox and the equidistant sampling.\n\n    Args:\n        dynamic_image (DynamicImage): Dynamic image.\n        scheduler (Scheduler): Scheduler object.\n        nprojs (int): Number of projections.\n        nrevs (int): Number of revolutions.\n        geo (Geometry): Geometry object from TIGRE.\n        radians (bool, optional): Wheter to use radians. Defaults to True.\n\n    Returns:\n        np.ndarray: An array containing the sinogram.\n    \"\"\"\n    if isinstance(geo, TigreGeometry):\n        geo = (\n            geo.geo\n        )  # If the user forgets to pass the actual Tigre geometry, but rather the object, set geo to the geometry\n    time_series = np.zeros((nrevs * nprojs, *dynamic_image.size[:-1]))  # Initialize sinogram\n    theta = equidistant(nprojs=nprojs, nrevs=nrevs, radians=radians)\n    time = 0\n    for i in tqdm(range(len(theta)), desc=\"Sampling\"):\n        phantom = dynamic_image.get_phantom(time)\n        time_series[i, ...] = tigre.Ax(phantom.astype(np.float32), geo, angles=np.array([theta[i]]), **kwargs).squeeze(\n            axis=0\n        )\n        time = scheduler.rotate(theta[i], theta[i + 1]) if i &lt; len(theta) - 1 else time\n    return time_series, theta\n</code></pre>"},{"location":"reference/nect/src/sampling/tigre_toolbox/#nect.src.sampling.tigre_toolbox.dynamic_golden_angle_sampling","title":"dynamic_golden_angle_sampling","text":"<pre><code>dynamic_golden_angle_sampling(dynamic_image, scheduler, nprojs: int, geo: Geometry = None, radians=True, *args, **kwargs) -&gt; ndarray\n</code></pre> <p>Sample a dynamic image using the TIGRE toolbox and the golden ratio sampling.</p> <p>Parameters:</p> Name Type Description Default <code>dynamic_image</code> <code>DynamicImage</code> <p>Dynamic image.</p> required <code>scheduler</code> <code>Scheduler</code> <p>Scheduler object.</p> required <code>nprojs</code> <code>int</code> <p>Number of projections.</p> required <code>geo</code> <code>Geometry</code> <p>Geometry object from TIGRE.</p> <code>None</code> <code>radians</code> <code>bool</code> <p>Wheter to use radians. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array containing the sinogram.</p> Source code in <code>nect/src/sampling/tigre_toolbox.py</code> <pre><code>def dynamic_golden_angle_sampling(\n    dynamic_image,\n    scheduler,\n    nprojs: int,\n    geo: Geometry = None,\n    radians=True,\n    *args,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"Sample a dynamic image using the TIGRE toolbox and the golden ratio sampling.\n\n    Args:\n        dynamic_image (DynamicImage): Dynamic image.\n        scheduler (Scheduler): Scheduler object.\n        nprojs (int): Number of projections.\n        geo (Geometry): Geometry object from TIGRE.\n        radians (bool, optional): Wheter to use radians. Defaults to True.\n\n    Returns:\n        np.ndarray: An array containing the sinogram.\n    \"\"\"\n    if isinstance(geo, TigreGeometry):\n        geo = (\n            geo.geo\n        )  # If the user forgets to pass the actual Tigre geometry, but rather the object, set geo to the geometry\n    time_series = np.zeros((nprojs, *dynamic_image.size[:-1]))  # Initialize sinogram\n    theta = golden_angle(nprojs=nprojs, radians=radians)\n    time = 0\n    for i in tqdm(range(len(theta)), desc=\"Sampling\"):\n        phantom = dynamic_image.get_phantom(time)\n        time_series[i, ...] = tigre.Ax(phantom.astype(np.float32), geo, angles=np.array([theta[i]]), **kwargs).squeeze(\n            axis=0\n        )\n        time = scheduler.rotate(theta[i], theta[i + 1]) if i &lt; len(theta) - 1 else time\n    return time_series, theta\n</code></pre>"},{"location":"reference/nect/src/sampling/tigre_toolbox/#nect.src.sampling.tigre_toolbox.equidistant_sampling","title":"equidistant_sampling","text":"<pre><code>equidistant_sampling(image, nprojs: int, nrevs: int, geo: Geometry = None, radians=True, *args, **kwargs) -&gt; ndarray\n</code></pre> <p>Sample a static image using the TIGRE toolbox.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Static 3D image.</p> required <code>nprojs</code> <code>int</code> <p>Number of projections.</p> required <code>nrevs</code> <code>int</code> <p>Number of revolutions.</p> required <code>geo</code> <code>Geometry</code> <p>Geometry object from TIGRE.</p> <code>None</code> <code>radians</code> <code>bool</code> <p>Wheter to use radians. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array containing the sinogram.</p> Source code in <code>nect/src/sampling/tigre_toolbox.py</code> <pre><code>def equidistant_sampling(\n    image, nprojs: int, nrevs: int, geo: Geometry = None, radians=True, *args, **kwargs\n) -&gt; np.ndarray:\n    \"\"\"Sample a static image using the TIGRE toolbox.\n\n    Args:\n        image (np.ndarray): Static 3D image.\n        nprojs (int): Number of projections.\n        nrevs (int): Number of revolutions.\n        geo (Geometry): Geometry object from TIGRE.\n        radians (bool, optional): Wheter to use radians. Defaults to True.\n\n    Returns:\n        np.ndarray: An array containing the sinogram.\n    \"\"\"\n    if isinstance(geo, TigreGeometry):\n        geo = (\n            geo.geo\n        )  # If the user forgets to pass the actual Tigre geometry, but rather the object, set geo to the geometry\n    theta = equidistant(nprojs=nprojs, nrevs=nrevs, radians=radians)\n    return tigre.Ax(image.get_phantom(0).astype(np.float32), geo, theta, **kwargs), theta\n</code></pre>"},{"location":"reference/nect/src/sampling/tigre_toolbox/#nect.src.sampling.tigre_toolbox.golden_angle_sampling","title":"golden_angle_sampling","text":"<pre><code>golden_angle_sampling(image, nprojs: int, geo: Geometry = None, radians=True, *args, **kwargs) -&gt; ndarray\n</code></pre> <p>Sample a static image using the TIGRE toolbox and the golden ratio sampling.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Static 3D image.</p> required <code>nprojs</code> <code>int</code> <p>Number of projections.</p> required <code>geo</code> <code>Geometry</code> <p>Geometry object from TIGRE.</p> <code>None</code> <code>radians</code> <code>bool</code> <p>Wheter to use radians. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: An array containing the sinogram.</p> Source code in <code>nect/src/sampling/tigre_toolbox.py</code> <pre><code>def golden_angle_sampling(image, nprojs: int, geo: Geometry = None, radians=True, *args, **kwargs) -&gt; np.ndarray:\n    \"\"\"Sample a static image using the TIGRE toolbox and the golden ratio sampling.\n\n    Args:\n        image (np.ndarray): Static 3D image.\n        nprojs (int): Number of projections.\n        geo (Geometry): Geometry object from TIGRE.\n        radians (bool, optional): Wheter to use radians. Defaults to True.\n\n    Returns:\n        np.ndarray: An array containing the sinogram.\n    \"\"\"\n    if isinstance(geo, TigreGeometry):\n        geo = (\n            geo.geo\n        )  # If the user forgets to pass the actual Tigre geometry, but rather the object, set geo to the geometry\n    theta = golden_angle(nprojs=nprojs, radians=radians)\n    return tigre.Ax(image.astype(np.float32), geo, theta, **kwargs), theta\n</code></pre>"},{"location":"reference/nect/src/simulator/","title":"Index","text":""},{"location":"reference/nect/src/simulator/#nect.src.simulator","title":"simulator","text":""},{"location":"reference/nect/src/simulator/scheduler/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> scheduler","text":""},{"location":"reference/nect/src/simulator/scheduler/#nect.src.simulator.scheduler","title":"scheduler","text":""},{"location":"reference/nect/src/simulator/scheduler/#nect.src.simulator.scheduler.Scheduler","title":"Scheduler","text":"<pre><code>Scheduler(s_per_degree, exp_t_per_proj: float = 0, n_proj_avg: float = 0)\n</code></pre> Source code in <code>nect/src/simulator/scheduler.py</code> <pre><code>def __init__(self, s_per_degree, exp_t_per_proj: float = 0, n_proj_avg: float = 0):\n    self.s_per_degree = s_per_degree\n    self.exp_t_per_proj = exp_t_per_proj\n    self.n_proj_avg = n_proj_avg\n    self.current_t = 0\n</code></pre>"},{"location":"reference/nect/src/simulator/scheduler/#nect.src.simulator.scheduler.Scheduler.frame_avg","title":"frame_avg","text":"<pre><code>frame_avg() -&gt; float\n</code></pre> <p>Updates the current time to the next time.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The next time.</p> Source code in <code>nect/src/simulator/scheduler.py</code> <pre><code>def frame_avg(self) -&gt; float:\n    \"\"\"Updates the current time to the next time.\n\n    Returns:\n        float: The next time.\n    \"\"\"\n    self.current_t += self.exp_t_per_proj * self.n_proj_avg\n    return self.current_t\n</code></pre>"},{"location":"reference/nect/src/simulator/scheduler/#nect.src.simulator.scheduler.Scheduler.project","title":"project","text":"<pre><code>project() -&gt; float\n</code></pre> <p>Updates the current time to the next time.</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The next time.</p> Source code in <code>nect/src/simulator/scheduler.py</code> <pre><code>def project(self) -&gt; float:\n    \"\"\"Updates the current time to the next time.\n\n    Returns:\n        float: The next time.\n    \"\"\"\n    self.current_t += self.exp_t_per_proj\n    return self.current_t\n</code></pre>"},{"location":"reference/nect/src/simulator/scheduler/#nect.src.simulator.scheduler.Scheduler.rotate","title":"rotate","text":"<pre><code>rotate(current_angle: float, next_angle: float) -&gt; float\n</code></pre> <p>Updates the current time and angle to the next time and angle.</p> <p>Parameters:</p> Name Type Description Default <code>current_angle</code> <code>float</code> <p>The current angle.</p> required <code>next_angle</code> <code>float</code> <p>The next angle.</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The next time.</p> Source code in <code>nect/src/simulator/scheduler.py</code> <pre><code>def rotate(self, current_angle: float, next_angle: float) -&gt; float:\n    \"\"\"Updates the current time and angle to the next time and angle.\n\n    Args:\n        current_angle (float): The current angle.\n        next_angle (float): The next angle.\n\n    Returns:\n        float: The next time.\n    \"\"\"\n    self.current_t += self.s_per_degree * np.abs(next_angle - current_angle)\n    return self.current_t\n</code></pre>"},{"location":"reference/nect/src/simulator/configuration/","title":"Index","text":""},{"location":"reference/nect/src/simulator/configuration/#nect.src.simulator.configuration","title":"configuration","text":""},{"location":"reference/nect/src/simulator/configuration/config/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> config","text":""},{"location":"reference/nect/src/simulator/configuration/config/#nect.src.simulator.configuration.config","title":"config","text":""},{"location":"reference/nect/src/simulator/configuration/config/#nect.src.simulator.configuration.config.LeapGeometry","title":"LeapGeometry","text":"<pre><code>LeapGeometry(default: bool = True, device: device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device('cpu'), forward_project: bool = True, use_static: bool = False, *cfg, **kwargs)\n</code></pre> Source code in <code>nect/src/simulator/configuration/config.py</code> <pre><code>def __init__(\n    self,\n    default: bool = True,\n    device: torch.device = torch.device(\"cuda:0\")\n    if torch.cuda.is_available()\n    else torch.device(\"cpu\"),  # TODO: Update to \"cuda\" when supported by LEAP\n    forward_project: bool = True,\n    use_static: bool = False,\n    *cfg,\n    **kwargs,\n):\n    self.valid_keys = [\n        \"mode\",\n        \"numX\",\n        \"numY\",\n        \"numZ\",\n        \"width\",\n        \"height\",\n        \"offsetX\",\n        \"offsetY\",\n        \"offsetZ\",\n        \"numAngles\",\n        \"numRows\",\n        \"numCols\",\n        \"pixelWidth\",\n        \"pixelHeight\",\n        \"centerRow\",\n        \"centerCol\",\n        \"arange\",\n        \"phis\",\n        \"sod\",\n        \"sdd\",\n        \"default\",\n        \"device\",\n    ]\n    self.device = device\n    self.use_gpu = False if self.device.type == \"cpu\" else True\n    self.forward_project = forward_project\n    self.use_static = use_static\n    if default:\n        self.default_projector()\n    else:\n        self.proj = Projector(\n            use_gpu=self.use_gpu,\n            gpu_device=self.device,\n            forward_project=self.forward_project,\n            use_static=self.use_static,\n        )\n\n    # Check if input names are valid\n    for dictionary in cfg:\n        for key in dictionary:\n            if self.is_valid_key(key):\n                setattr(self, key, dictionary[key])\n    for key in kwargs:\n        if self.is_valid_key(key):\n            setattr(self, key, kwargs[key])\n\n    self.update_projector()\n</code></pre>"},{"location":"reference/nect/src/simulator/configuration/config/#nect.src.simulator.configuration.config.LeapGeometry.update_phi","title":"update_phi","text":"<pre><code>update_phi(phi: Tensor) -&gt; None\n</code></pre> <p>Phi should be a tensor and in degrees</p> Source code in <code>nect/src/simulator/configuration/config.py</code> <pre><code>def update_phi(self, phi: torch.Tensor) -&gt; None:\n    \"\"\"Phi should be a tensor and in degrees\"\"\"\n    if len(phi.shape) == 0:\n        numAngles = 1\n    else:\n        numAngles = int(phi.shape[0])\n    self.proj.set_conebeam(\n        numAngles=numAngles,\n        numRows=self.numRows,\n        numCols=self.numCols,\n        pixelHeight=self.pixelHeight,\n        pixelWidth=self.pixelWidth,\n        centerRow=self.centerRow,\n        centerCol=self.centerCol,\n        phis=phi,\n        sod=self.sod,\n        sdd=self.sdd,\n    )\n</code></pre>"},{"location":"reference/nect/src/simulator/configuration/config/#nect.src.simulator.configuration.config.TigreGeometry","title":"TigreGeometry","text":"<pre><code>TigreGeometry(default=True, *cfg, **kwargs)\n</code></pre> Source code in <code>nect/src/simulator/configuration/config.py</code> <pre><code>def __init__(self, default=True, *cfg, **kwargs):\n    self.valid_keys = [\n        \"mode\",\n        \"nVoxel\",\n        \"DSD\",\n        \"DSO\",\n        \"nDetector\",\n        \"dDetector\",\n        \"sDetector\",\n        \"dVoxel\",\n        \"sVoxel\",\n        \"offOrigin\",\n        \"offDetector\",\n        \"accuracy\",\n        \"COR\",\n        \"rotDetector\",\n        \"default\",\n    ]\n    self.geo = tigre.geometry()\n    if default:\n        self.set_default_geometry()\n    for dictionary in cfg:\n        for key in dictionary:\n            if self.is_valid_key(key):\n                setattr(self.geo, key, dictionary[key])\n    for key in kwargs:\n        if self.is_valid_key(key):\n            setattr(self.geo, key, kwargs[key])\n</code></pre>"},{"location":"reference/nect/src/test/","title":"Index","text":""},{"location":"reference/nect/src/test/#nect.src.test","title":"test","text":""},{"location":"reference/nect/src/test/test_conv/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> test_conv","text":""},{"location":"reference/nect/src/test/test_conv/#nect.src.test.test_conv","title":"test_conv","text":""},{"location":"reference/nect/src/test/test_conv/#nect.src.test.test_conv.ConvTranspose3d","title":"ConvTranspose3d","text":"<pre><code>ConvTranspose3d(in_channels: int, out_channels: int, kernel_size: _size_3_t, stride: _size_3_t = 1, padding: _size_3_t = 0, output_padding: _size_3_t = 0, groups: int = 1, bias: bool = True, dilation: _size_3_t = 1, padding_mode: str = 'zeros', device=None, dtype=None)\n</code></pre> <p>               Bases: <code>_ConvTransposeNd</code></p> <p>From https://github.com/matheusja/ConvTranspose4d-PyTorch</p> Source code in <code>nect/src/test/test_conv.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_3_t,\n    stride: _size_3_t = 1,\n    padding: _size_3_t = 0,\n    output_padding: _size_3_t = 0,\n    groups: int = 1,\n    bias: bool = True,\n    dilation: _size_3_t = 1,\n    padding_mode: str = \"zeros\",\n    device=None,\n    dtype=None,\n) -&gt; None:\n    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n    kernel_size = _triple(kernel_size)\n    stride = _triple(stride)\n    padding = _triple(padding)\n    dilation = _triple(dilation)\n    output_padding = _triple(output_padding)\n    super().__init__(\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        dilation,\n        True,\n        output_padding,\n        groups,\n        bias,\n        padding_mode,\n        **factory_kwargs,\n    )\n</code></pre>"},{"location":"reference/nect/src/test/test_conv/#nect.src.test.test_conv.conv3d","title":"conv3d","text":"<pre><code>conv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)\n</code></pre> <p>Applies a 2D convolution over an input signal composed of several input planes.</p> Source code in <code>nect/src/test/test_conv.py</code> <pre><code>def conv3d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1):\n    \"\"\"Applies a 2D convolution over an input signal composed of several input\n    planes.\n    \"\"\"\n    # Define shortcut names for dimensions of input and kernel\n    (Batch, _, l_i, h_i, w_i) = tuple(input.shape)\n    (l_k, h_k, w_k) = (weight.size(2), weight.size(3), weight.size(4))\n    (l_p, h_p, w_p) = padding\n    (l_d, h_d, w_d) = dilation\n    (l_s, h_s, w_s) = stride\n\n    # Compute the size of the output tensor based on the zero padding\n    l_o = (l_i + 2 * l_p - (l_k) - (l_k - 1) * (l_d - 1)) // l_s + 1\n    w_o = (w_i + 2 * w_p - (w_k) - (w_k - 1) * (w_d - 1)) // w_s + 1\n    h_o = (h_i + 2 * h_p - (h_k) - (h_k - 1) * (h_d - 1)) // h_s + 1\n    out_channels = weight.size(1)\n    # Pre-define output tensors\n    out = torch.zeros(Batch, out_channels, l_o, h_o, w_o).to(input.device)\n\n    # Convolve each kernel frame i with each input frame j\n    for i in range(l_k):\n        # Calculate the zero-offset of kernel frame i\n        zero_offset = -l_p + (i * l_d)\n        # Calculate the range of input frame j corresponding to kernel frame i\n        j_start = max(zero_offset % l_s, zero_offset)\n        j_end = min(l_i, l_i + l_p - (l_k - i - 1) * l_d)\n        # Convolve each kernel frame i with corresponding input frame j\n        for j in range(j_start, j_end, l_s):\n            # Calculate the output frame\n            out_frame = (j - zero_offset) // l_s\n            # Add results to this output frame\n            out[:, :, out_frame, :, :] += F.conv2d(\n                input[:, :, j, :, :],\n                weight[:, :, i, :, :],\n                bias=None,\n                stride=stride[1::],\n                padding=padding[1::],\n                dilation=dilation[1::],\n                groups=groups,\n            )\n\n    # Add bias to output\n    if bias is not None:\n        out = out + bias.view(1, -1, 1, 1, 1)\n\n    return out\n</code></pre>"},{"location":"reference/nect/src/test/test_pooling/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> test_pooling","text":""},{"location":"reference/nect/src/test/test_pooling/#nect.src.test.test_pooling","title":"test_pooling","text":""},{"location":"reference/nect/src/test/test_sampling/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> test_sampling","text":""},{"location":"reference/nect/src/test/test_sampling/#nect.src.test.test_sampling","title":"test_sampling","text":""},{"location":"reference/nect/src/test/test_sampling/#nect.src.test.test_sampling.test_equidistant_sampling","title":"test_equidistant_sampling","text":"<pre><code>test_equidistant_sampling(nprojs: int, nrevs: int, radians: bool)\n</code></pre> <p>Checks that the correct number of angles is reveived and that they are equidistantly spaced. Also checks the boundary values.</p> <p>Parameters:</p> Name Type Description Default <code>nprojs</code> <code>int</code> <p>How many projections per revolution.</p> required <code>nrevs</code> <code>int</code> <p>Number of revolutions.</p> required <code>radians</code> <code>bool</code> <p>Whether to return angles in radians or degrees.</p> required Source code in <code>nect/src/test/test_sampling.py</code> <pre><code>@pytest.mark.parametrize(\n    \"nprojs, nrevs, radians\",\n    [(13, 1, True), (13, 1, False), (100, 2, False)],\n    ids=[\"Radians\", \"Degrees\", \"100 projections and 2 revolutions\"],\n)\ndef test_equidistant_sampling(nprojs: int, nrevs: int, radians: bool):\n    \"\"\"Checks that the correct number of angles is reveived and that they are equidistantly spaced.\n    Also checks the boundary values.\n\n    Args:\n        nprojs (int): How many projections per revolution.\n        nrevs (int): Number of revolutions.\n        radians (bool): Whether to return angles in radians or degrees.\n    \"\"\"\n    angles = equidistant(nproj=nprojs, nrevs=nrevs, radians=radians)\n    assert len(angles) == nprojs * nrevs\n    assert angles[0] == 0\n    assert np.min(angles) &gt;= 0\n    if radians:\n        assert np.max(angles) &lt;= 2 * np.pi * nrevs\n    else:\n        assert np.max(angles) &lt;= 360 * nrevs\n    diffs = np.diff(angles)\n    assert np.allclose(diffs, diffs[0])\n</code></pre>"},{"location":"reference/nect/src/test/test_st_ms_ssim/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> test_st_ms_ssim","text":""},{"location":"reference/nect/src/test/test_st_ms_ssim/#nect.src.test.test_st_ms_ssim","title":"test_st_ms_ssim","text":""},{"location":"reference/nect/src/utils/","title":"Index","text":""},{"location":"reference/nect/src/utils/#nect.src.utils","title":"utils","text":""},{"location":"reference/nect/src/utils/nsi_to_inr_yaml/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> nsi_to_inr_yaml","text":""},{"location":"reference/nect/src/utils/nsi_to_inr_yaml/#nect.src.utils.nsi_to_inr_yaml","title":"nsi_to_inr_yaml","text":""},{"location":"reference/nect/src/utils/video/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> video","text":""},{"location":"reference/nect/src/utils/video/#nect.src.utils.video","title":"video","text":""},{"location":"reference/nect/trainers/","title":"Index","text":""},{"location":"reference/nect/trainers/#nect.trainers","title":"trainers","text":""},{"location":"reference/nect/trainers/#nect.trainers.BaseTrainer","title":"BaseTrainer","text":"<pre><code>BaseTrainer(config: Config, output_directory: str | Path | None = None, checkpoint: str | Path | None = None, save_ckpt: bool = True, save_last: bool = True, save_optimizer: bool = True, verbose: bool = True, log: bool = True, cancel_at: str | None = None, keep_two: bool = True, prune: bool = True)\n</code></pre> Source code in <code>nect/trainers/base_trainer.py</code> <pre><code>def __init__(\n    self,\n    config: Config,\n    output_directory: str | Path | None = None,\n    checkpoint: str | Path | None = None,\n    save_ckpt: bool = True,\n    save_last: bool = True,\n    save_optimizer: bool = True,\n    verbose: bool = True,\n    log: bool = True,\n    cancel_at: str | None = None,\n    keep_two: bool = True,\n    prune: bool = True,\n):\n    setup_logger(level=logging.INFO if verbose else logging.WARNING)\n    if checkpoint:\n        output_directory = Path(checkpoint).parent.parent.parent\n    elif output_directory is not None:\n        specific_run = datetime.datetime.now().replace(microsecond=0).isoformat().replace(\":\",\"-\")\n        output_directory = os.path.join(output_directory, config.get_str(), specific_run)\n    self.use_prior = config.use_prior\n    self.save_optimizer = save_optimizer\n    self.keep_two = keep_two\n    if self.use_prior:\n        prior_path = Path(config.img_path)\n        self.prior = np.load(prior_path).astype(np.float32)\n    L.seed_everything(42)\n    self.logger = logger\n    self.logger.info(\"-------------------\")\n    self.logger.info(f\"SEEING NUMBER OF GPUS {torch.cuda.device_count()}\")\n    self.logger.info(\"-------------------\")\n    torch.set_float32_matmul_precision(\"high\")\n    self.cancel_at = None\n    self.prune = prune\n    if cancel_at is not None:\n        try:\n            self.cancel_at = datetime.datetime.fromisoformat(cancel_at)\n        except ValueError as e:\n            raise ValueError(f\"Tried set cancel of job at '{cancel_at}', which is an invalid ISO-datetime.\")\n    self.config = config\n    self.setup_dataset()\n    self.dataloader = torch.utils.data.DataLoader(dataset=self.dataset, batch_size=1, shuffle=True, num_workers=config.num_workers)\n    if isinstance(self.config.epochs, str):\n        fraction = self.config.epochs.split(\"x\")\n        self.config.epochs = math.ceil(49 / len(self.dataset) * max(config.geometry.nDetector))\n        if len(fraction) == 2:\n            self.config.epochs = math.ceil(self.config.epochs * float(fraction[0]))\n    self.loss_fn = config.get_loss_fn()\n\n    if output_directory is None:\n        if save_ckpt or save_last or log:\n            raise ValueError(\"Output directory must be provided if logging or saving checkpoints\")\n    if log:\n        if output_directory is None:\n            # raise a error that is due to coding error and not the user\n            raise ValueError(\"Output directory must be provided if logging or saving checkpoints. This is a bug in the code.\")\n        tensorboard_logger = TensorBoardLogger(root_dir=output_directory, name=\"logs\")\n    else:\n        self.config.image_interval = -1\n        tensorboard_logger = None\n    self.verbose = verbose\n    if verbose is False:\n        self.tqdm = lambda x, *args, **kwargs: x\n    else:\n        self.tqdm = tqdm\n    self.fabric = L.Fabric(\n        accelerator=\"cuda\",\n        devices=\"auto\",\n        strategy=\"auto\",\n        precision=\"16-mixed\",\n        loggers=tensorboard_logger,\n    )\n\n    # supress warnings from self.fabric.launch()\n    warnings.filterwarnings(\"ignore\")\n    self.fabric.launch()\n    warnings.resetwarnings()\n    self.model = config.get_model()\n    self.optim = config.get_optimizer(self.model)\n    (self.lr_scheduler_warmup, self.lr_scheduler, self.lr_scheduler_warmup_downsample,) = config.get_lr_schedulers(self.optim)\n    self.current_epoch = 0\n    self.current_angle = 0\n    self.angle = 0.0\n    self.current_projection = 0\n    if config.s3im:\n        self.s3im_loss = config.get_s3im_loss()\n\n    self.downsample_detector_factor = config.downsampling_detector.start\n    self.points_per_ray = config.points_per_ray.start\n    self.geometry = nect.Geometry.from_cfg(config.geometry, reconstruction_mode=config.reconstruction_mode, sample_outside=config.sample_outside,)\n    if config.points_per_batch == \"auto\":\n        raise ValueError(\"`points_per_batch` should already have been calculated at this point.\")\n    self.projector = nect.sampling.Projector(\n        geometry=self.geometry,\n        points_per_batch=config.points_per_batch // 2,\n        points_per_ray=self.points_per_ray,\n        device=self.fabric.device,\n        uniform_ray_spacing=config.uniform_ray_spacing,\n    )\n\n    self.model, self.optim = self.fabric.setup(self.model, self.optim)\n    if hasattr(self.model, \"encoder\"):\n        self.model.encoder.B = self.fabric.to_device(self.model.encoder.B)\n\n    self.dataloader = cast(torch.utils.data.DataLoader, self.fabric.setup_dataloaders(self.dataloader))\n    if config.points_per_ray.end == \"auto\":\n        config.points_per_ray.end = math.ceil(max(config.geometry.nDetector) * 1.5)\n    elif isinstance(config.points_per_ray.end, str):\n        end_str = config.points_per_ray.end.split(\"x\")\n        if len(end_str) == 2:\n            config.points_per_ray.end = math.ceil(max(config.geometry.nDetector) * float(end_str[0]))\n        else:\n            raise ValueError(\"Invalid format for `points_per_ray.end`\")\n    if isinstance(config.points_per_ray.update_interval, str):\n        if config.points_per_ray.update_interval == \"auto\":\n            factor = 0.9\n        else:\n            update_interval = config.points_per_ray.update_interval.split(\"x\")\n            if len(update_interval) == 2:\n                factor = float(update_interval[0])\n            else:\n                raise ValueError(f\"Invalid format for `points_per_ray.update_interval`, got '{config.points_per_ray.update_interval}'\")\n\n        projections = len(self.dataset)\n        epochs = self.config.epochs * factor\n        total_updates = projections * epochs / torch.cuda.device_count()\n        number_of_updates_needed = config.points_per_ray.end - config.points_per_ray.start\n        config.points_per_ray.update_interval = math.ceil(total_updates / number_of_updates_needed)\n\n    if self.fabric.is_global_zero and output_directory is not None:\n        self.checkpoint_directory_base, self.image_directory_base = create_sub_folders(output_directory)\n        self.epoch_loss_log_path = Path(self.checkpoint_directory_base).parent / \"epoch_losses.txt\"\n        self.initial_state_path = Path(self.checkpoint_directory_base).parent / \"initial_state.txt\"\n    else:\n        self.checkpoint_directory_base = \"needs_to_be_defined_but_not_used\"  # must be defined\n        self.image_directory_base = \"needs_to_be_defined_but_not_used\"\n        self.epoch_loss_log_path = None\n        self.initial_state_path = None\n\n    self._initial_state_saved = False\n    self.last_checkpoint_time = time.perf_counter()\n    self.last_image_time = time.perf_counter()\n    self.last_evaluation_time = time.perf_counter()\n    self.save = save_ckpt\n    self.save_last = save_last\n    self.use_checkpoint = False\n    self.downsample_warmup_iteration = 0\n    if checkpoint:\n        self.logger.info(\"LOADING CHEKPOINT\")\n        checkpoint_data = self.fabric.load(checkpoint, strict=True)\n        self.current_angle = checkpoint_data[\"angle\"]\n        self.current_epoch = checkpoint_data[\"epoch\"]\n        self.model.load_state_dict(checkpoint_data[\"model\"])\n        self.optim.load_state_dict(checkpoint_data[\"optim\"])\n        self.logger.info(f\"Starting from epoch {self.current_epoch} and angle number {self.current_angle}\")\n        self.dataloader._num_iter_calls = self.current_epoch\n        self.use_checkpoint = True\n        self.current_projection = self.current_epoch * config.geometry.num_angles + self.current_angle\n        self.downsample_detector_factor = max(1, self.config.downsampling_detector.start // 2 ** (self.current_projection // self.config.downsampling_detector.update_interval)) \n        if self.config.points_per_ray.linear:\n            self.points_per_ray = min(self.config.points_per_ray.end, self.config.points_per_ray.start + self.current_projection // self.config.points_per_ray.update_interval)\n        else:\n            self.points_per_ray = min(self.config.points_per_ray.end, self.config.points_per_ray.start * 2 ** (self.current_projection // self.config.points_per_ray.update_interval))\n        self.projector.update(\n            angle=self.angle,\n            detector_binning=self.downsample_detector_factor,\n            points_per_ray=self.points_per_ray,\n            random_offset_detector=0.5 if self.downsample_detector_factor &gt; 1 else 0,\n        )\n        self.step = 0\n        self.lr_scheduler.load_state_dict(checkpoint_data[\"lr_scheduler\"])\n    self.setup_evaluator()\n    self.batch_per_proj = config.batch_per_proj\n    if self.batch_per_proj == \"all\":\n        self.batch_per_proj = 1000000000\n\n    if config.mode == \"static\" and log:\n        model_summary = summary(self.model, input_size=((1, 3)))\n        # save the model summary to a txt file\n        with open(f\"{Path(self.image_directory_base).parent/'model_summary.txt'}\", \"w\") as file:\n            file.write(str(model_summary))\n\n    if self.fabric.is_global_zero and checkpoint is None and output_directory is not None:\n        config.save(output_directory)\n\n    self.outputdir = output_directory\n</code></pre>"},{"location":"reference/nect/trainers/#nect.trainers.BaseTrainer.save_epoch_checkpoint","title":"save_epoch_checkpoint","text":"<pre><code>save_epoch_checkpoint()\n</code></pre> <p>Save full model + optimizer every N epochs into outputs/run_name/checkpoints/epoch_xxxx.ckpt</p> Source code in <code>nect/trainers/base_trainer.py</code> <pre><code>def save_epoch_checkpoint(self):\n    \"\"\"Save full model + optimizer every N epochs into outputs/run_name/checkpoints/epoch_xxxx.ckpt\"\"\"\n    run_name = getattr(self.config, \"model\", \"default_run\")\n    base_dir = Path(\"outputs\") / run_name / \"checkpoints\"\n    base_dir.mkdir(parents=True, exist_ok=True)\n\n    filename = base_dir / f\"epoch_{self.current_epoch:04d}.ckpt\"\n\n    state = {\n        \"epoch\": self.current_epoch,\n        \"model_state_dict\": self.model.state_dict(),\n        \"optimizer_state_dict\": self.optim.state_dict() if self.save_optimizer else None,\n        \"lr_scheduler_state_dict\": self.lr_scheduler.state_dict() if hasattr(self, \"lr_scheduler\") else None,\n        \"config\": self.config,\n    }\n\n    torch.save(state, filename)\n    self.logger.info(f\"Saved checkpoint: {filename}\")\n</code></pre>"},{"location":"reference/nect/trainers/#nect.trainers.BaseTrainer.set_projection","title":"set_projection","text":"<pre><code>set_projection(projection)\n</code></pre> <p>Set the projection. This is used to update the downsampling factor and points per ray.</p> <p>Parameters:</p> Name Type Description Default <code>projection</code> <code>int</code> <p>current projection</p> required Source code in <code>nect/trainers/base_trainer.py</code> <pre><code>def set_projection(self, projection):\n    \"\"\"\n    Set the projection. This is used to update the downsampling factor and points per ray.\n\n    Args:\n        projection (int): current projection\n    \"\"\"\n    if isinstance(self.config.points_per_ray.end, str):\n        raise ValueError(\"`points_per_ray.end` should already have been calculated at this point.\")\n    self.projection = projection\n    if (\n        projection != 0\n        and projection % self.config.downsampling_detector.update_interval == 0\n        and self.downsample_detector_factor &gt; self.config.downsampling_detector.end\n    ):\n        self.downsample_detector_factor //= 2\n    if (\n        projection != 0\n        and projection % self.config.points_per_ray.update_interval == 0\n        and self.points_per_ray &lt; self.config.points_per_ray.end\n        and self.config.points_per_ray.linear is False\n    ):\n        self.points_per_ray *= 2\n    elif (\n        projection != 0\n        and projection % self.config.points_per_ray.update_interval == 0\n        and self.points_per_ray &lt; self.config.points_per_ray.end\n        and self.config.points_per_ray.linear is True\n    ):\n        self.points_per_ray += 1\n\n    self.projector.update(\n        angle=self.angle,\n        detector_binning=self.downsample_detector_factor,\n        points_per_ray=self.points_per_ray,\n        random_offset_detector=0.5 if self.downsample_detector_factor &gt; 1 else 0,\n    )\n</code></pre>"},{"location":"reference/nect/trainers/#nect.trainers.BaseTrainer.warmup_w0_only","title":"warmup_w0_only","text":"<pre><code>warmup_w0_only(steps: int = 1500, lr_mult: float = 2.0, include_b0: bool = True)\n</code></pre> <p>Short warm-up training that updates ONLY the first MLP layer's weights (W0) and optionally its bias (b0). Everything else (encoders and deeper MLP layers) is frozen via a gradient mask on the flat tcnn 'net.params' tensor.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>number of optimizer updates to run in this warm-up (not epochs)</p> <code>1500</code> <code>lr_mult</code> <code>float</code> <p>learning-rate multiplier relative to self.config.base_lr</p> <code>2.0</code> <code>include_b0</code> <code>bool</code> <p>if True, also train the first layer bias b0</p> <code>True</code> Source code in <code>nect/trainers/base_trainer.py</code> <pre><code>def warmup_w0_only(self, steps: int = 1500, lr_mult: float = 2.0, include_b0: bool = True):\n    \"\"\"\n    Short warm-up training that updates ONLY the first MLP layer's weights (W0)\n    and optionally its bias (b0). Everything else (encoders and deeper MLP layers)\n    is frozen via a gradient mask on the flat tcnn 'net.params' tensor.\n\n    Args:\n        steps: number of optimizer updates to run in this warm-up (not epochs)\n        lr_mult: learning-rate multiplier relative to self.config.base_lr\n        include_b0: if True, also train the first layer bias b0\n    \"\"\"\n    self.logger.info(f\"[W0 warm-up] steps={steps}, lr_mult={lr_mult}, include_b0={include_b0}\")\n\n    # ---- Helpers to compute MLP splits (TCNN-compatible; folds padding into W0) ----\n    def _mlp_layer_splits(in_dim: int, net_cfg) -&gt; list[int]:\n        # H, L + concrete dict\n        if hasattr(net_cfg, \"n_neurons\"):\n            H = int(net_cfg.n_neurons)\n            L = int(net_cfg.n_hidden_layers)\n            net_conf = net_cfg.get_network_config()\n        else:\n            net_conf = net_cfg.get_network_config()\n            H = int(net_conf[\"n_neurons\"])\n            L = int(net_conf[\"n_hidden_layers\"])\n        D_in = int(in_dim)\n        D_out = 1  # single scalar output\n\n        splits: list[int] = []\n        splits += [H * D_in, H]                 # W0, b0\n        for _ in range(L - 1):\n            splits += [H * H, H]                # Wk, bk\n        splits += [D_out * H, D_out]            # W_out, b_out\n\n        # Validate vs dummy and fold any padding into W0\n        enc = {\"otype\": \"Identity\", \"n_dims_to_encode\": D_in}\n        dummy = tcnn.NetworkWithInputEncoding(\n            n_input_dims=D_in,\n            n_output_dims=D_out,\n            encoding_config=enc,\n            network_config=net_conf,\n        )\n        flat = dummy.state_dict().get(\"net.params\", dummy.state_dict().get(\"params\"))\n        assert flat is not None, \"TCNN dummy state_dict missing 'net.params'/'params'.\"\n        diff = flat.numel() - sum(splits)\n        if diff != 0:\n            splits[0] += diff\n        assert sum(splits) == flat.numel(), (\n            f\"MLP split mismatch even after padding W0: sum={sum(splits)} vs tcnn={flat.numel()}\"\n        )\n        return splits\n\n    def _encoded_width_quadcubes_from_cfg(cfg) -&gt; int:\n        # No include_identity handling as requested\n        L = cfg.encoder.n_levels\n        F = cfg.encoder.n_features_per_level\n        return 4 * (L * F)\n\n    # ---- Locate the flat TCNN parameter and build a W0/b0 mask ----\n    # Find the flat parameter (usually named 'net.params')\n    name_to_param = dict(self.model.named_parameters())\n    flat_name = None\n    for k in name_to_param:\n        if k.endswith(\"net.params\") or k == \"params\" or k.endswith(\".params\"):\n            flat_name = k\n            break\n    if flat_name is None:\n        raise RuntimeError(\"Could not find TCNN flat parameter ('net.params') in model.named_parameters().\")\n\n    flat_param: torch.nn.Parameter = name_to_param[flat_name]\n    total_len = flat_param.numel()\n\n    # Compute MLP splits using encoded input width from cfg\n    in_dim = _encoded_width_quadcubes_from_cfg(self.config)\n    splits = _mlp_layer_splits(in_dim, self.config.net)\n\n    mlp_total = sum(splits)\n    enc_total = total_len - mlp_total\n    if enc_total &lt; 0:\n        raise RuntimeError(f\"Computed negative encoder size: total={total_len}, mlp={mlp_total}\")\n\n    # Offsets inside MLP tail\n    def _prefix_offsets(szs: list[int]) -&gt; list[int]:\n        offs = [0]\n        for s in szs:\n            offs.append(offs[-1] + s)\n        return offs\n\n    off = _prefix_offsets(splits)\n    W0_lo, W0_hi = enc_total + off[0], enc_total + off[1]   # first weight block\n    b0_lo, b0_hi = enc_total + off[1], enc_total + off[2]   # first bias block\n\n    # Build mask: True for trainable indices, False elsewhere\n    mask = torch.zeros(total_len, dtype=torch.bool, device=flat_param.device)\n    mask[W0_lo:W0_hi] = True\n    if include_b0:\n        mask[b0_lo:b0_hi] = True\n\n    self.logger.info(\n        f\"[W0 warm-up] total={total_len}, enc_total={enc_total}, \"\n        f\"W0=[{W0_lo},{W0_hi}), b0=[{b0_lo},{b0_hi}) trainable={int(mask.sum().item())}\"\n    )\n\n    # Register grad mask hook on the flat param\n    def _grad_mask_hook(grad: torch.Tensor) -&gt; torch.Tensor:\n        g = grad\n        if g.is_sparse:\n            g = g.to_dense()\n        g = g.masked_fill(~mask, 0)\n        return g\n\n    hook_handle = flat_param.register_hook(_grad_mask_hook)\n\n    # Temporary optimizer for the flat param only\n    base_lr = getattr(self.config, \"base_lr\", 1e-3)\n    lr = float(base_lr) * float(lr_mult)\n    # Try to read beta settings from config; fall back to (0.9, 0.95)\n    beta1 = getattr(getattr(self.config, \"optimizer\", object()), \"beta1\", 0.9)\n    beta2 = getattr(getattr(self.config, \"optimizer\", object()), \"beta2\", 0.95)\n    wd = getattr(getattr(self.config, \"optimizer\", object()), \"weight_decay\", 0.0)\n    warmup_optim = torch.optim.Adam([flat_param], lr=lr, betas=(beta1, beta2), weight_decay=wd)\n\n    # ---- Run a minimal inner loop for `steps` optimizer updates ----\n    self.model.train()\n    steps_done = 0\n    dataloader_iter = iter(self.dataloader)\n\n    start_t = time.perf_counter()\n    while steps_done &lt; steps:\n        try:\n            proj, angle, timestep = next(dataloader_iter)\n        except StopIteration:\n            dataloader_iter = iter(self.dataloader)\n            proj, angle, timestep = next(dataloader_iter)\n\n        # Optional downsampling, then flatten\n        if self.downsample_detector_factor != 1:\n            proj = F.avg_pool2d(\n                proj.unsqueeze(0),\n                kernel_size=self.downsample_detector_factor,\n                stride=self.downsample_detector_factor,\n            ).squeeze(0)\n        proj = proj.flatten()\n\n        # Ensure projector has per-angle sizing (sets batch_size, distances, batch_per_epoch, etc.)\n        self.angle = float(angle) if torch.is_tensor(angle) else float(angle)\n        self.projector.update(\n            angle=self.angle,\n            detector_binning=self.downsample_detector_factor,\n            points_per_ray=self.points_per_ray,\n            random_offset_detector=0.5 if self.downsample_detector_factor &gt; 1 else 0,\n        )\n\n        # Exactly one projector batch per step (config.batch_per_proj == 1)\n        batch_num = 0\n        warmup_optim.zero_grad(set_to_none=True)\n\n        points, y = self.projector(batch_num=batch_num, proj=proj)\n        if points is None or y is None:\n            continue\n\n        zero_points_mask = torch.all(points.view(-1, 3) == 0, dim=-1)\n        points_shape = points.size()\n        if points_shape[1] == 0:\n            continue\n\n        pts_flat = points.view(-1, 3)[~zero_points_mask]\n        if pts_flat.size(0) == 0:\n            continue\n\n        # Forward chunking for TCNN comfort (~5M points per chunk)\n        atten_hats = []\n        ppb = 5_000_000\n        for p0 in range(0, pts_flat.size(0), ppb):\n            if self.config.mode == \"dynamic\":\n                atten_hat = self.model(pts_flat[p0:p0+ppb], float(timestep)).squeeze(0)\n            else:\n                atten_hat = self.model(pts_flat[p0:p0+ppb]).squeeze(0)\n            atten_hats.append(atten_hat)\n\n        atten_hat = torch.cat(atten_hats) if atten_hats else torch.empty(0, device=self.fabric.device)\n\n        processed = torch.zeros(\n            (points_shape[0], points_shape[1], 1),\n            dtype=torch.float32,\n            device=self.fabric.device,\n        ).view(-1, 1)\n        processed[~zero_points_mask] = atten_hat\n        atten_hat = processed.view(points_shape[0], points_shape[1])\n\n        y_pred = torch.sum(atten_hat, dim=1) * (\n            self.projector.distances / (self.geometry.max_distance_traveled)\n        )\n        loss = self.loss_fn(y_pred, y, 0)\n\n        self.fabric.backward(loss)\n        if getattr(self.config, \"clip_grad_value\", None) is not None:\n            torch.nn.utils.clip_grad_value_(self.model.parameters(), self.config.clip_grad_value)\n\n        warmup_optim.step()\n        steps_done += 1\n\n        if self.fabric.is_global_zero and steps_done % 50 == 0:\n            self.fabric.log_dict(\n                {\n                    \"w0_warmup/loss\": loss.detach(),\n                    \"w0_warmup/steps_done\": steps_done,\n                    \"w0_warmup/lr\": lr,\n                },\n                step=steps_done,\n            )\n\n    dt = time.perf_counter() - start_t\n    self.logger.info(f\"[W0 warm-up] finished {steps_done} steps in {dt:.1f}s\")\n\n    # Clean up hook &amp; tmp optimizer\n    hook_handle.remove()\n    for pg in warmup_optim.param_groups:\n        pg[\"lr\"] = 0.0\n    del warmup_optim\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"reference/nect/trainers/base_trainer/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> base_trainer","text":""},{"location":"reference/nect/trainers/base_trainer/#nect.trainers.base_trainer","title":"base_trainer","text":""},{"location":"reference/nect/trainers/base_trainer/#nect.trainers.base_trainer.BaseTrainer","title":"BaseTrainer","text":"<pre><code>BaseTrainer(config: Config, output_directory: str | Path | None = None, checkpoint: str | Path | None = None, save_ckpt: bool = True, save_last: bool = True, save_optimizer: bool = True, verbose: bool = True, log: bool = True, cancel_at: str | None = None, keep_two: bool = True, prune: bool = True)\n</code></pre> Source code in <code>nect/trainers/base_trainer.py</code> <pre><code>def __init__(\n    self,\n    config: Config,\n    output_directory: str | Path | None = None,\n    checkpoint: str | Path | None = None,\n    save_ckpt: bool = True,\n    save_last: bool = True,\n    save_optimizer: bool = True,\n    verbose: bool = True,\n    log: bool = True,\n    cancel_at: str | None = None,\n    keep_two: bool = True,\n    prune: bool = True,\n):\n    setup_logger(level=logging.INFO if verbose else logging.WARNING)\n    if checkpoint:\n        output_directory = Path(checkpoint).parent.parent.parent\n    elif output_directory is not None:\n        specific_run = datetime.datetime.now().replace(microsecond=0).isoformat().replace(\":\",\"-\")\n        output_directory = os.path.join(output_directory, config.get_str(), specific_run)\n    self.use_prior = config.use_prior\n    self.save_optimizer = save_optimizer\n    self.keep_two = keep_two\n    if self.use_prior:\n        prior_path = Path(config.img_path)\n        self.prior = np.load(prior_path).astype(np.float32)\n    L.seed_everything(42)\n    self.logger = logger\n    self.logger.info(\"-------------------\")\n    self.logger.info(f\"SEEING NUMBER OF GPUS {torch.cuda.device_count()}\")\n    self.logger.info(\"-------------------\")\n    torch.set_float32_matmul_precision(\"high\")\n    self.cancel_at = None\n    self.prune = prune\n    if cancel_at is not None:\n        try:\n            self.cancel_at = datetime.datetime.fromisoformat(cancel_at)\n        except ValueError as e:\n            raise ValueError(f\"Tried set cancel of job at '{cancel_at}', which is an invalid ISO-datetime.\")\n    self.config = config\n    self.setup_dataset()\n    self.dataloader = torch.utils.data.DataLoader(dataset=self.dataset, batch_size=1, shuffle=True, num_workers=config.num_workers)\n    if isinstance(self.config.epochs, str):\n        fraction = self.config.epochs.split(\"x\")\n        self.config.epochs = math.ceil(49 / len(self.dataset) * max(config.geometry.nDetector))\n        if len(fraction) == 2:\n            self.config.epochs = math.ceil(self.config.epochs * float(fraction[0]))\n    self.loss_fn = config.get_loss_fn()\n\n    if output_directory is None:\n        if save_ckpt or save_last or log:\n            raise ValueError(\"Output directory must be provided if logging or saving checkpoints\")\n    if log:\n        if output_directory is None:\n            # raise a error that is due to coding error and not the user\n            raise ValueError(\"Output directory must be provided if logging or saving checkpoints. This is a bug in the code.\")\n        tensorboard_logger = TensorBoardLogger(root_dir=output_directory, name=\"logs\")\n    else:\n        self.config.image_interval = -1\n        tensorboard_logger = None\n    self.verbose = verbose\n    if verbose is False:\n        self.tqdm = lambda x, *args, **kwargs: x\n    else:\n        self.tqdm = tqdm\n    self.fabric = L.Fabric(\n        accelerator=\"cuda\",\n        devices=\"auto\",\n        strategy=\"auto\",\n        precision=\"16-mixed\",\n        loggers=tensorboard_logger,\n    )\n\n    # supress warnings from self.fabric.launch()\n    warnings.filterwarnings(\"ignore\")\n    self.fabric.launch()\n    warnings.resetwarnings()\n    self.model = config.get_model()\n    self.optim = config.get_optimizer(self.model)\n    (self.lr_scheduler_warmup, self.lr_scheduler, self.lr_scheduler_warmup_downsample,) = config.get_lr_schedulers(self.optim)\n    self.current_epoch = 0\n    self.current_angle = 0\n    self.angle = 0.0\n    self.current_projection = 0\n    if config.s3im:\n        self.s3im_loss = config.get_s3im_loss()\n\n    self.downsample_detector_factor = config.downsampling_detector.start\n    self.points_per_ray = config.points_per_ray.start\n    self.geometry = nect.Geometry.from_cfg(config.geometry, reconstruction_mode=config.reconstruction_mode, sample_outside=config.sample_outside,)\n    if config.points_per_batch == \"auto\":\n        raise ValueError(\"`points_per_batch` should already have been calculated at this point.\")\n    self.projector = nect.sampling.Projector(\n        geometry=self.geometry,\n        points_per_batch=config.points_per_batch // 2,\n        points_per_ray=self.points_per_ray,\n        device=self.fabric.device,\n        uniform_ray_spacing=config.uniform_ray_spacing,\n    )\n\n    self.model, self.optim = self.fabric.setup(self.model, self.optim)\n    if hasattr(self.model, \"encoder\"):\n        self.model.encoder.B = self.fabric.to_device(self.model.encoder.B)\n\n    self.dataloader = cast(torch.utils.data.DataLoader, self.fabric.setup_dataloaders(self.dataloader))\n    if config.points_per_ray.end == \"auto\":\n        config.points_per_ray.end = math.ceil(max(config.geometry.nDetector) * 1.5)\n    elif isinstance(config.points_per_ray.end, str):\n        end_str = config.points_per_ray.end.split(\"x\")\n        if len(end_str) == 2:\n            config.points_per_ray.end = math.ceil(max(config.geometry.nDetector) * float(end_str[0]))\n        else:\n            raise ValueError(\"Invalid format for `points_per_ray.end`\")\n    if isinstance(config.points_per_ray.update_interval, str):\n        if config.points_per_ray.update_interval == \"auto\":\n            factor = 0.9\n        else:\n            update_interval = config.points_per_ray.update_interval.split(\"x\")\n            if len(update_interval) == 2:\n                factor = float(update_interval[0])\n            else:\n                raise ValueError(f\"Invalid format for `points_per_ray.update_interval`, got '{config.points_per_ray.update_interval}'\")\n\n        projections = len(self.dataset)\n        epochs = self.config.epochs * factor\n        total_updates = projections * epochs / torch.cuda.device_count()\n        number_of_updates_needed = config.points_per_ray.end - config.points_per_ray.start\n        config.points_per_ray.update_interval = math.ceil(total_updates / number_of_updates_needed)\n\n    if self.fabric.is_global_zero and output_directory is not None:\n        self.checkpoint_directory_base, self.image_directory_base = create_sub_folders(output_directory)\n        self.epoch_loss_log_path = Path(self.checkpoint_directory_base).parent / \"epoch_losses.txt\"\n        self.initial_state_path = Path(self.checkpoint_directory_base).parent / \"initial_state.txt\"\n    else:\n        self.checkpoint_directory_base = \"needs_to_be_defined_but_not_used\"  # must be defined\n        self.image_directory_base = \"needs_to_be_defined_but_not_used\"\n        self.epoch_loss_log_path = None\n        self.initial_state_path = None\n\n    self._initial_state_saved = False\n    self.last_checkpoint_time = time.perf_counter()\n    self.last_image_time = time.perf_counter()\n    self.last_evaluation_time = time.perf_counter()\n    self.save = save_ckpt\n    self.save_last = save_last\n    self.use_checkpoint = False\n    self.downsample_warmup_iteration = 0\n    if checkpoint:\n        self.logger.info(\"LOADING CHEKPOINT\")\n        checkpoint_data = self.fabric.load(checkpoint, strict=True)\n        self.current_angle = checkpoint_data[\"angle\"]\n        self.current_epoch = checkpoint_data[\"epoch\"]\n        self.model.load_state_dict(checkpoint_data[\"model\"])\n        self.optim.load_state_dict(checkpoint_data[\"optim\"])\n        self.logger.info(f\"Starting from epoch {self.current_epoch} and angle number {self.current_angle}\")\n        self.dataloader._num_iter_calls = self.current_epoch\n        self.use_checkpoint = True\n        self.current_projection = self.current_epoch * config.geometry.num_angles + self.current_angle\n        self.downsample_detector_factor = max(1, self.config.downsampling_detector.start // 2 ** (self.current_projection // self.config.downsampling_detector.update_interval)) \n        if self.config.points_per_ray.linear:\n            self.points_per_ray = min(self.config.points_per_ray.end, self.config.points_per_ray.start + self.current_projection // self.config.points_per_ray.update_interval)\n        else:\n            self.points_per_ray = min(self.config.points_per_ray.end, self.config.points_per_ray.start * 2 ** (self.current_projection // self.config.points_per_ray.update_interval))\n        self.projector.update(\n            angle=self.angle,\n            detector_binning=self.downsample_detector_factor,\n            points_per_ray=self.points_per_ray,\n            random_offset_detector=0.5 if self.downsample_detector_factor &gt; 1 else 0,\n        )\n        self.step = 0\n        self.lr_scheduler.load_state_dict(checkpoint_data[\"lr_scheduler\"])\n    self.setup_evaluator()\n    self.batch_per_proj = config.batch_per_proj\n    if self.batch_per_proj == \"all\":\n        self.batch_per_proj = 1000000000\n\n    if config.mode == \"static\" and log:\n        model_summary = summary(self.model, input_size=((1, 3)))\n        # save the model summary to a txt file\n        with open(f\"{Path(self.image_directory_base).parent/'model_summary.txt'}\", \"w\") as file:\n            file.write(str(model_summary))\n\n    if self.fabric.is_global_zero and checkpoint is None and output_directory is not None:\n        config.save(output_directory)\n\n    self.outputdir = output_directory\n</code></pre>"},{"location":"reference/nect/trainers/base_trainer/#nect.trainers.base_trainer.BaseTrainer.save_epoch_checkpoint","title":"save_epoch_checkpoint","text":"<pre><code>save_epoch_checkpoint()\n</code></pre> <p>Save full model + optimizer every N epochs into outputs/run_name/checkpoints/epoch_xxxx.ckpt</p> Source code in <code>nect/trainers/base_trainer.py</code> <pre><code>def save_epoch_checkpoint(self):\n    \"\"\"Save full model + optimizer every N epochs into outputs/run_name/checkpoints/epoch_xxxx.ckpt\"\"\"\n    run_name = getattr(self.config, \"model\", \"default_run\")\n    base_dir = Path(\"outputs\") / run_name / \"checkpoints\"\n    base_dir.mkdir(parents=True, exist_ok=True)\n\n    filename = base_dir / f\"epoch_{self.current_epoch:04d}.ckpt\"\n\n    state = {\n        \"epoch\": self.current_epoch,\n        \"model_state_dict\": self.model.state_dict(),\n        \"optimizer_state_dict\": self.optim.state_dict() if self.save_optimizer else None,\n        \"lr_scheduler_state_dict\": self.lr_scheduler.state_dict() if hasattr(self, \"lr_scheduler\") else None,\n        \"config\": self.config,\n    }\n\n    torch.save(state, filename)\n    self.logger.info(f\"Saved checkpoint: {filename}\")\n</code></pre>"},{"location":"reference/nect/trainers/base_trainer/#nect.trainers.base_trainer.BaseTrainer.set_projection","title":"set_projection","text":"<pre><code>set_projection(projection)\n</code></pre> <p>Set the projection. This is used to update the downsampling factor and points per ray.</p> <p>Parameters:</p> Name Type Description Default <code>projection</code> <code>int</code> <p>current projection</p> required Source code in <code>nect/trainers/base_trainer.py</code> <pre><code>def set_projection(self, projection):\n    \"\"\"\n    Set the projection. This is used to update the downsampling factor and points per ray.\n\n    Args:\n        projection (int): current projection\n    \"\"\"\n    if isinstance(self.config.points_per_ray.end, str):\n        raise ValueError(\"`points_per_ray.end` should already have been calculated at this point.\")\n    self.projection = projection\n    if (\n        projection != 0\n        and projection % self.config.downsampling_detector.update_interval == 0\n        and self.downsample_detector_factor &gt; self.config.downsampling_detector.end\n    ):\n        self.downsample_detector_factor //= 2\n    if (\n        projection != 0\n        and projection % self.config.points_per_ray.update_interval == 0\n        and self.points_per_ray &lt; self.config.points_per_ray.end\n        and self.config.points_per_ray.linear is False\n    ):\n        self.points_per_ray *= 2\n    elif (\n        projection != 0\n        and projection % self.config.points_per_ray.update_interval == 0\n        and self.points_per_ray &lt; self.config.points_per_ray.end\n        and self.config.points_per_ray.linear is True\n    ):\n        self.points_per_ray += 1\n\n    self.projector.update(\n        angle=self.angle,\n        detector_binning=self.downsample_detector_factor,\n        points_per_ray=self.points_per_ray,\n        random_offset_detector=0.5 if self.downsample_detector_factor &gt; 1 else 0,\n    )\n</code></pre>"},{"location":"reference/nect/trainers/base_trainer/#nect.trainers.base_trainer.BaseTrainer.warmup_w0_only","title":"warmup_w0_only","text":"<pre><code>warmup_w0_only(steps: int = 1500, lr_mult: float = 2.0, include_b0: bool = True)\n</code></pre> <p>Short warm-up training that updates ONLY the first MLP layer's weights (W0) and optionally its bias (b0). Everything else (encoders and deeper MLP layers) is frozen via a gradient mask on the flat tcnn 'net.params' tensor.</p> <p>Parameters:</p> Name Type Description Default <code>steps</code> <code>int</code> <p>number of optimizer updates to run in this warm-up (not epochs)</p> <code>1500</code> <code>lr_mult</code> <code>float</code> <p>learning-rate multiplier relative to self.config.base_lr</p> <code>2.0</code> <code>include_b0</code> <code>bool</code> <p>if True, also train the first layer bias b0</p> <code>True</code> Source code in <code>nect/trainers/base_trainer.py</code> <pre><code>def warmup_w0_only(self, steps: int = 1500, lr_mult: float = 2.0, include_b0: bool = True):\n    \"\"\"\n    Short warm-up training that updates ONLY the first MLP layer's weights (W0)\n    and optionally its bias (b0). Everything else (encoders and deeper MLP layers)\n    is frozen via a gradient mask on the flat tcnn 'net.params' tensor.\n\n    Args:\n        steps: number of optimizer updates to run in this warm-up (not epochs)\n        lr_mult: learning-rate multiplier relative to self.config.base_lr\n        include_b0: if True, also train the first layer bias b0\n    \"\"\"\n    self.logger.info(f\"[W0 warm-up] steps={steps}, lr_mult={lr_mult}, include_b0={include_b0}\")\n\n    # ---- Helpers to compute MLP splits (TCNN-compatible; folds padding into W0) ----\n    def _mlp_layer_splits(in_dim: int, net_cfg) -&gt; list[int]:\n        # H, L + concrete dict\n        if hasattr(net_cfg, \"n_neurons\"):\n            H = int(net_cfg.n_neurons)\n            L = int(net_cfg.n_hidden_layers)\n            net_conf = net_cfg.get_network_config()\n        else:\n            net_conf = net_cfg.get_network_config()\n            H = int(net_conf[\"n_neurons\"])\n            L = int(net_conf[\"n_hidden_layers\"])\n        D_in = int(in_dim)\n        D_out = 1  # single scalar output\n\n        splits: list[int] = []\n        splits += [H * D_in, H]                 # W0, b0\n        for _ in range(L - 1):\n            splits += [H * H, H]                # Wk, bk\n        splits += [D_out * H, D_out]            # W_out, b_out\n\n        # Validate vs dummy and fold any padding into W0\n        enc = {\"otype\": \"Identity\", \"n_dims_to_encode\": D_in}\n        dummy = tcnn.NetworkWithInputEncoding(\n            n_input_dims=D_in,\n            n_output_dims=D_out,\n            encoding_config=enc,\n            network_config=net_conf,\n        )\n        flat = dummy.state_dict().get(\"net.params\", dummy.state_dict().get(\"params\"))\n        assert flat is not None, \"TCNN dummy state_dict missing 'net.params'/'params'.\"\n        diff = flat.numel() - sum(splits)\n        if diff != 0:\n            splits[0] += diff\n        assert sum(splits) == flat.numel(), (\n            f\"MLP split mismatch even after padding W0: sum={sum(splits)} vs tcnn={flat.numel()}\"\n        )\n        return splits\n\n    def _encoded_width_quadcubes_from_cfg(cfg) -&gt; int:\n        # No include_identity handling as requested\n        L = cfg.encoder.n_levels\n        F = cfg.encoder.n_features_per_level\n        return 4 * (L * F)\n\n    # ---- Locate the flat TCNN parameter and build a W0/b0 mask ----\n    # Find the flat parameter (usually named 'net.params')\n    name_to_param = dict(self.model.named_parameters())\n    flat_name = None\n    for k in name_to_param:\n        if k.endswith(\"net.params\") or k == \"params\" or k.endswith(\".params\"):\n            flat_name = k\n            break\n    if flat_name is None:\n        raise RuntimeError(\"Could not find TCNN flat parameter ('net.params') in model.named_parameters().\")\n\n    flat_param: torch.nn.Parameter = name_to_param[flat_name]\n    total_len = flat_param.numel()\n\n    # Compute MLP splits using encoded input width from cfg\n    in_dim = _encoded_width_quadcubes_from_cfg(self.config)\n    splits = _mlp_layer_splits(in_dim, self.config.net)\n\n    mlp_total = sum(splits)\n    enc_total = total_len - mlp_total\n    if enc_total &lt; 0:\n        raise RuntimeError(f\"Computed negative encoder size: total={total_len}, mlp={mlp_total}\")\n\n    # Offsets inside MLP tail\n    def _prefix_offsets(szs: list[int]) -&gt; list[int]:\n        offs = [0]\n        for s in szs:\n            offs.append(offs[-1] + s)\n        return offs\n\n    off = _prefix_offsets(splits)\n    W0_lo, W0_hi = enc_total + off[0], enc_total + off[1]   # first weight block\n    b0_lo, b0_hi = enc_total + off[1], enc_total + off[2]   # first bias block\n\n    # Build mask: True for trainable indices, False elsewhere\n    mask = torch.zeros(total_len, dtype=torch.bool, device=flat_param.device)\n    mask[W0_lo:W0_hi] = True\n    if include_b0:\n        mask[b0_lo:b0_hi] = True\n\n    self.logger.info(\n        f\"[W0 warm-up] total={total_len}, enc_total={enc_total}, \"\n        f\"W0=[{W0_lo},{W0_hi}), b0=[{b0_lo},{b0_hi}) trainable={int(mask.sum().item())}\"\n    )\n\n    # Register grad mask hook on the flat param\n    def _grad_mask_hook(grad: torch.Tensor) -&gt; torch.Tensor:\n        g = grad\n        if g.is_sparse:\n            g = g.to_dense()\n        g = g.masked_fill(~mask, 0)\n        return g\n\n    hook_handle = flat_param.register_hook(_grad_mask_hook)\n\n    # Temporary optimizer for the flat param only\n    base_lr = getattr(self.config, \"base_lr\", 1e-3)\n    lr = float(base_lr) * float(lr_mult)\n    # Try to read beta settings from config; fall back to (0.9, 0.95)\n    beta1 = getattr(getattr(self.config, \"optimizer\", object()), \"beta1\", 0.9)\n    beta2 = getattr(getattr(self.config, \"optimizer\", object()), \"beta2\", 0.95)\n    wd = getattr(getattr(self.config, \"optimizer\", object()), \"weight_decay\", 0.0)\n    warmup_optim = torch.optim.Adam([flat_param], lr=lr, betas=(beta1, beta2), weight_decay=wd)\n\n    # ---- Run a minimal inner loop for `steps` optimizer updates ----\n    self.model.train()\n    steps_done = 0\n    dataloader_iter = iter(self.dataloader)\n\n    start_t = time.perf_counter()\n    while steps_done &lt; steps:\n        try:\n            proj, angle, timestep = next(dataloader_iter)\n        except StopIteration:\n            dataloader_iter = iter(self.dataloader)\n            proj, angle, timestep = next(dataloader_iter)\n\n        # Optional downsampling, then flatten\n        if self.downsample_detector_factor != 1:\n            proj = F.avg_pool2d(\n                proj.unsqueeze(0),\n                kernel_size=self.downsample_detector_factor,\n                stride=self.downsample_detector_factor,\n            ).squeeze(0)\n        proj = proj.flatten()\n\n        # Ensure projector has per-angle sizing (sets batch_size, distances, batch_per_epoch, etc.)\n        self.angle = float(angle) if torch.is_tensor(angle) else float(angle)\n        self.projector.update(\n            angle=self.angle,\n            detector_binning=self.downsample_detector_factor,\n            points_per_ray=self.points_per_ray,\n            random_offset_detector=0.5 if self.downsample_detector_factor &gt; 1 else 0,\n        )\n\n        # Exactly one projector batch per step (config.batch_per_proj == 1)\n        batch_num = 0\n        warmup_optim.zero_grad(set_to_none=True)\n\n        points, y = self.projector(batch_num=batch_num, proj=proj)\n        if points is None or y is None:\n            continue\n\n        zero_points_mask = torch.all(points.view(-1, 3) == 0, dim=-1)\n        points_shape = points.size()\n        if points_shape[1] == 0:\n            continue\n\n        pts_flat = points.view(-1, 3)[~zero_points_mask]\n        if pts_flat.size(0) == 0:\n            continue\n\n        # Forward chunking for TCNN comfort (~5M points per chunk)\n        atten_hats = []\n        ppb = 5_000_000\n        for p0 in range(0, pts_flat.size(0), ppb):\n            if self.config.mode == \"dynamic\":\n                atten_hat = self.model(pts_flat[p0:p0+ppb], float(timestep)).squeeze(0)\n            else:\n                atten_hat = self.model(pts_flat[p0:p0+ppb]).squeeze(0)\n            atten_hats.append(atten_hat)\n\n        atten_hat = torch.cat(atten_hats) if atten_hats else torch.empty(0, device=self.fabric.device)\n\n        processed = torch.zeros(\n            (points_shape[0], points_shape[1], 1),\n            dtype=torch.float32,\n            device=self.fabric.device,\n        ).view(-1, 1)\n        processed[~zero_points_mask] = atten_hat\n        atten_hat = processed.view(points_shape[0], points_shape[1])\n\n        y_pred = torch.sum(atten_hat, dim=1) * (\n            self.projector.distances / (self.geometry.max_distance_traveled)\n        )\n        loss = self.loss_fn(y_pred, y, 0)\n\n        self.fabric.backward(loss)\n        if getattr(self.config, \"clip_grad_value\", None) is not None:\n            torch.nn.utils.clip_grad_value_(self.model.parameters(), self.config.clip_grad_value)\n\n        warmup_optim.step()\n        steps_done += 1\n\n        if self.fabric.is_global_zero and steps_done % 50 == 0:\n            self.fabric.log_dict(\n                {\n                    \"w0_warmup/loss\": loss.detach(),\n                    \"w0_warmup/steps_done\": steps_done,\n                    \"w0_warmup/lr\": lr,\n                },\n                step=steps_done,\n            )\n\n    dt = time.perf_counter() - start_t\n    self.logger.info(f\"[W0 warm-up] finished {steps_done} steps in {dt:.1f}s\")\n\n    # Clean up hook &amp; tmp optimizer\n    hook_handle.remove()\n    for pg in warmup_optim.param_groups:\n        pg[\"lr\"] = 0.0\n    del warmup_optim\n    torch.cuda.empty_cache()\n</code></pre>"},{"location":"reference/nect/trainers/continous_scanning_trainer/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> continous_scanning_trainer","text":""},{"location":"reference/nect/trainers/continous_scanning_trainer/#nect.trainers.continous_scanning_trainer","title":"continous_scanning_trainer","text":""},{"location":"reference/nect/trainers/initrainer/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> initrainer","text":""},{"location":"reference/nect/trainers/initrainer/#nect.trainers.initrainer","title":"initrainer","text":""},{"location":"reference/nect/trainers/porous_medium_trainer/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> porous_medium_trainer","text":""},{"location":"reference/nect/trainers/porous_medium_trainer/#nect.trainers.porous_medium_trainer","title":"porous_medium_trainer","text":""},{"location":"reference/nect/trainers/projections_loaded_trainer/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> projections_loaded_trainer","text":""},{"location":"reference/nect/trainers/projections_loaded_trainer/#nect.trainers.projections_loaded_trainer","title":"projections_loaded_trainer","text":""},{"location":"reference/nect/trainers/scivis_trainer/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> scivis_trainer","text":""},{"location":"reference/nect/trainers/scivis_trainer/#nect.trainers.scivis_trainer","title":"scivis_trainer","text":""},{"location":"reference/torch_extra/","title":"Index","text":""},{"location":"reference/torch_extra/#torch_extra","title":"torch_extra","text":""},{"location":"reference/torch_extra/typing/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> typing","text":""},{"location":"reference/torch_extra/typing/#torch_extra.typing","title":"typing","text":""},{"location":"reference/torch_extra/nn/","title":"Index","text":""},{"location":"reference/torch_extra/nn/#torch_extra.nn","title":"nn","text":""},{"location":"reference/torch_extra/nn/#torch_extra.nn.ConvTranspose4d","title":"ConvTranspose4d","text":"<pre><code>ConvTranspose4d(in_channels: int, out_channels: int, kernel_size: _size_4_t, stride: _size_4_t = 1, padding: _size_4_t = 0, output_padding: _size_4_t = 0, groups: int = 1, bias: bool = True, dilation: _size_4_t = 1, padding_mode: str = 'zeros', device=None, dtype=None)\n</code></pre> <p>               Bases: <code>_ConvTransposeNd</code></p> <p>From https://github.com/matheusja/ConvTranspose4d-PyTorch</p> Source code in <code>torch_extra/nn/modules/conv.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_4_t,\n    stride: _size_4_t = 1,\n    padding: _size_4_t = 0,\n    output_padding: _size_4_t = 0,\n    groups: int = 1,\n    bias: bool = True,\n    dilation: _size_4_t = 1,\n    padding_mode: str = \"zeros\",\n    device=None,\n    dtype=None,\n) -&gt; None:\n    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n    kernel_size = _quadruple(kernel_size)\n    stride = _quadruple(stride)\n    padding = _quadruple(padding)\n    dilation = _quadruple(dilation)\n    if dilation != (1, 1, 1, 1):\n        raise NotImplementedError(\"Dilation other than 1 not yet implemented!\")\n    output_padding = _quadruple(output_padding)\n    super().__init__(\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        dilation,\n        True,\n        output_padding,\n        groups,\n        bias,\n        padding_mode,\n        **factory_kwargs,\n    )\n</code></pre>"},{"location":"reference/torch_extra/nn/#torch_extra.nn.FourierRingCorrelation","title":"FourierRingCorrelation","text":"<pre><code>FourierRingCorrelation(shape: Tuple[int, ...], size_average: bool = True, delta: int = 1)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Fourier Ring Correlation (FRC) loss function.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>tuple[int, ...]</code> <p>shape of the input tensors.</p> required <code>size_average</code> <code>bool</code> <p>if size_average=True, FRC will be averaged over radii, channel and batch. Else, FRC will be averaged over channel and batch. Default: True.</p> <code>True</code> <code>delta</code> <code>int</code> <p>delta for the ring size. Default: 1.</p> <code>1</code> Source code in <code>torch_extra/nn/modules/loss.py</code> <pre><code>def __init__(self, shape: Tuple[int, ...], size_average: bool = True, delta: int = 1) -&gt; None:\n    super(FourierRingCorrelation, self).__init__()\n    self.size_average = size_average\n    self.radial_masks = F_extra._create_radial_masks(shape, delta=delta, dims=2)\n</code></pre>"},{"location":"reference/torch_extra/nn/#torch_extra.nn.FourierShellCorrelation","title":"FourierShellCorrelation","text":"<pre><code>FourierShellCorrelation(shape: Tuple[int, ...], size_average: bool = True, delta: int = 1)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Fourier Shell Correlation (FSC) loss function.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>tuple[int, ...]</code> <p>shape of the input tensors.</p> required <code>size_average</code> <code>bool</code> <p>if size_average=True, FSC will be averaged over radii, channel and batch. Else, FSC will be averaged over channel and batch. Default: True.</p> <code>True</code> <code>delta</code> <code>int</code> <p>delta for the shell size. Default: 1.</p> <code>1</code> Source code in <code>torch_extra/nn/modules/loss.py</code> <pre><code>def __init__(self, shape: Tuple[int, ...], size_average: bool = True, delta: int = 1) -&gt; None:\n    super(FourierShellCorrelation, self).__init__()\n    self.size_average = size_average\n    self.radial_masks = F_extra._create_radial_masks(shape, delta=delta, dims=3)\n</code></pre>"},{"location":"reference/torch_extra/nn/#torch_extra.nn.MS_SSIM","title":"MS_SSIM","text":"<pre><code>MS_SSIM(data_range: float = 255, size_average: bool = True, win_size: int = 11, win_sigma: float = 1.5, channel: int = 3, spatial_dims: int = 2, weights: Optional[List[float]] = None, K: Union[Tuple[float, float], List[float]] = (0.01, 0.03), stride: int = 1)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>class for ms-ssim Args:     data_range (float or int, optional): value range of input images. (usually 1.0 or 255)     size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar     win_size: (int, optional): the size of gauss kernel     win_sigma: (float, optional): sigma of normal distribution     channel (int, optional): input channels (default: 3)     weights (list, optional): weights for different levels     K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.     stride (int, optional): stride of the sliding window</p> Source code in <code>torch_extra/nn/modules/loss.py</code> <pre><code>def __init__(\n    self,\n    data_range: float = 255,\n    size_average: bool = True,\n    win_size: int = 11,\n    win_sigma: float = 1.5,\n    channel: int = 3,\n    spatial_dims: int = 2,\n    weights: Optional[List[float]] = None,\n    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n    stride: int = 1,\n) -&gt; None:\n    r\"\"\"class for ms-ssim\n    Args:\n        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n        win_size: (int, optional): the size of gauss kernel\n        win_sigma: (float, optional): sigma of normal distribution\n        channel (int, optional): input channels (default: 3)\n        weights (list, optional): weights for different levels\n        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n        stride (int, optional): stride of the sliding window\n    \"\"\"\n\n    super(MS_SSIM, self).__init__()\n    self.win_size = win_size\n    self.win = F_extra._fspecial_gauss_1d(win_size, win_sigma).repeat([channel, 1] + [1] * spatial_dims)\n    self.size_average = size_average\n    self.data_range = data_range\n    self.weights = weights\n    self.K = K\n    self.stride = stride\n</code></pre>"},{"location":"reference/torch_extra/nn/#torch_extra.nn.SSIM","title":"SSIM","text":"<pre><code>SSIM(data_range: float = 255, size_average: bool = True, win_size: int = 11, win_sigma: float = 1.5, channel: int = 3, spatial_dims: int = 2, K: Union[Tuple[float, float], List[float]] = (0.01, 0.03), win: Optional[Tensor] = None, nonnegative_ssim: bool = False, stride: int = 1, device: Optional[device | int | str] = None)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>class for ssim Args:     data_range (float or int, optional): value range of input images. (usually 1.0 or 255)     size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar     win_size: (int, optional): the size of gauss kernel     win_sigma: (float, optional): sigma of normal distribution     channel (int, optional): input channels (default: 3)     K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.     win (Tensor, optional): window for ssim. The size should be 1xwin_size     nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu.     stride (int, optional): stride of the sliding window     device (torch.device, int or str, optional): the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (default: None). If provided, it will send kernel to device before forward.</p> Source code in <code>torch_extra/nn/modules/loss.py</code> <pre><code>def __init__(\n    self,\n    data_range: float = 255,\n    size_average: bool = True,\n    win_size: int = 11,\n    win_sigma: float = 1.5,\n    channel: int = 3,\n    spatial_dims: int = 2,\n    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n    win: Optional[Tensor] = None,\n    nonnegative_ssim: bool = False,\n    stride: int = 1,\n    device: Optional[torch.device | int | str] = None,\n) -&gt; None:\n    r\"\"\"class for ssim\n    Args:\n        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n        win_size: (int, optional): the size of gauss kernel\n        win_sigma: (float, optional): sigma of normal distribution\n        channel (int, optional): input channels (default: 3)\n        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n        win (Tensor, optional): window for ssim. The size should be 1xwin_size\n        nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu.\n        stride (int, optional): stride of the sliding window\n        device (torch.device, int or str, optional): the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (default: None). If provided, it will send kernel to device before forward.\n    \"\"\"\n\n    super(SSIM, self).__init__()\n    self.win_size = win_size\n    if win is not None:\n        if len(win.size()) == 1:\n            win = win.repeat([channel, 1] + [1] * spatial_dims)\n        self.win = win\n    else:\n        self.win = F_extra._fspecial_gauss_1d(win_size, win_sigma).repeat([channel, 1] + [1] * spatial_dims)\n    if device:\n        self.win = self.win.to(device=device, dtype=torch.float32)\n    self.size_average = size_average\n    self.data_range = data_range\n    self.K = K\n    self.nonnegative_ssim = nonnegative_ssim\n    self.stride = stride\n</code></pre>"},{"location":"reference/torch_extra/nn/#torch_extra.nn.ST_MS_SSIM","title":"ST_MS_SSIM","text":"<pre><code>ST_MS_SSIM(data_range: float = 255, size_average: bool = True, win_size: int = 11, win_sigma: float = 1.5, channel: int = 3, spatial_dims: int = 2, weights: Optional[List[float]] = None, K: Union[Tuple[float, float], List[float]] = (0.01, 0.03), temporal_win_size: Optional[int] = None)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>class for ms-ssim Args:     data_range (float or int, optional): value range of input images. (usually 1.0 or 255)     size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar     win_size: (int, optional): the size of gauss kernel     win_sigma: (float, optional): sigma of normal distribution     channel (int, optional): input channels (default: 3)     weights (list, optional): weights for different levels     K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.     temporal_win_size (int, optional): the size of gauss kernel for temporal dimension</p> Source code in <code>torch_extra/nn/modules/loss.py</code> <pre><code>def __init__(\n    self,\n    data_range: float = 255,\n    size_average: bool = True,\n    win_size: int = 11,\n    win_sigma: float = 1.5,\n    channel: int = 3,\n    spatial_dims: int = 2,\n    weights: Optional[List[float]] = None,\n    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n    temporal_win_size: Optional[int] = None,\n) -&gt; None:\n    r\"\"\"class for ms-ssim\n    Args:\n        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n        win_size: (int, optional): the size of gauss kernel\n        win_sigma: (float, optional): sigma of normal distribution\n        channel (int, optional): input channels (default: 3)\n        weights (list, optional): weights for different levels\n        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n        temporal_win_size (int, optional): the size of gauss kernel for temporal dimension\n    \"\"\"\n\n    super(ST_MS_SSIM, self).__init__()\n    self.win_size = win_size\n    self.temporal_win_size = temporal_win_size\n    temporal_dim = 1 if temporal_win_size is not None else 0\n    self.win = F_extra._fspecial_gauss_1d(win_size, win_sigma).repeat(\n        [channel, 1] + [1] * (spatial_dims + temporal_dim)\n    )\n    if temporal_dim:\n        self.temporal_win = (\n            F_extra._fspecial_gauss_1d(temporal_win_size, win_sigma)\n            .repeat([channel, 1] + [1] * (spatial_dims + temporal_dim))\n            .transpose(-1, 2)\n        )\n    else:\n        self.temporal_win = None\n    self.size_average = size_average\n    self.data_range = data_range\n    self.K = K\n    self.weights = weights\n</code></pre>"},{"location":"reference/torch_extra/nn/#torch_extra.nn.ST_SSIM","title":"ST_SSIM","text":"<pre><code>ST_SSIM(data_range: float = 255, size_average: bool = True, win_size: int = 11, win_sigma: float = 1.5, channel: int = 3, spatial_dims: int = 2, K: Union[Tuple[float, float], List[float]] = (0.01, 0.03), nonnegative_ssim: bool = False, temporal_win_size: Optional[int] = None, stride: int = 1)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>class for ssim Args:     data_range (float or int, optional): value range of input images. (usually 1.0 or 255)     size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar     win_size: (int, optional): the size of gauss kernel     win_sigma: (float, optional): sigma of normal distribution     channel (int, optional): input channels (default: 3)     K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.     nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu.     temporal_win_size (int, optional): the size of gauss kernel for temporal dimension     stride (int, optional): stride of the sliding window</p> Source code in <code>torch_extra/nn/modules/loss.py</code> <pre><code>def __init__(\n    self,\n    data_range: float = 255,\n    size_average: bool = True,\n    win_size: int = 11,\n    win_sigma: float = 1.5,\n    channel: int = 3,\n    spatial_dims: int = 2,\n    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n    nonnegative_ssim: bool = False,\n    temporal_win_size: Optional[int] = None,\n    stride: int = 1,\n) -&gt; None:\n    r\"\"\"class for ssim\n    Args:\n        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n        win_size: (int, optional): the size of gauss kernel\n        win_sigma: (float, optional): sigma of normal distribution\n        channel (int, optional): input channels (default: 3)\n        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n        nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu.\n        temporal_win_size (int, optional): the size of gauss kernel for temporal dimension\n        stride (int, optional): stride of the sliding window\n    \"\"\"\n\n    super(ST_SSIM, self).__init__()\n    self.win_size = win_size\n    self.temporal_win_size = temporal_win_size\n    temporal_dim = 1 if temporal_win_size is not None else 0\n    self.win = F_extra._fspecial_gauss_1d(win_size, win_sigma).repeat(\n        [channel, 1] + [1] * (spatial_dims + temporal_dim)\n    )\n    if temporal_dim:\n        self.temporal_win = (\n            F_extra._fspecial_gauss_1d(temporal_win_size, win_sigma)\n            .repeat([channel, 1] + [1] * (spatial_dims + temporal_dim))\n            .transpose(-1, 2)\n        )\n    else:\n        self.temporal_win = None\n    self.size_average = size_average\n    self.data_range = data_range\n    self.K = K\n    self.nonnegative_ssim = nonnegative_ssim\n    self.stride = stride\n</code></pre>"},{"location":"reference/torch_extra/nn/common_types/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> common_types","text":""},{"location":"reference/torch_extra/nn/common_types/#torch_extra.nn.common_types","title":"common_types","text":""},{"location":"reference/torch_extra/nn/functional/","title":"Index","text":""},{"location":"reference/torch_extra/nn/functional/#torch_extra.nn.functional","title":"functional","text":""},{"location":"reference/torch_extra/nn/functional/#torch_extra.nn.functional.conv4d","title":"conv4d","text":"<pre><code>conv4d(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None, stride: Union[_int, _size] = 1, padding: Union[_int, _size] = 0, dilation: Union[_int, _size] = 1, groups: _int = 1) -&gt; Tensor\n</code></pre> <p>Applies a 4D convolution over an input signal composed of several input planes. Based on  https://github.com/ZhengyuLiang24/Conv4d-PyTorch</p> Source code in <code>torch_extra/nn/functional/conv.py</code> <pre><code>def conv4d(\n    input: Tensor,\n    weight: Tensor,\n    bias: Optional[Tensor] = None,\n    stride: Union[_int, _size] = 1,\n    padding: Union[_int, _size] = 0,\n    dilation: Union[_int, _size] = 1,\n    groups: _int = 1,\n) -&gt; Tensor:\n    \"\"\"Applies a 4D convolution over an input signal composed of several input\n    planes. Based on  https://github.com/ZhengyuLiang24/Conv4d-PyTorch\n    \"\"\"\n    # Define shortcut names for dimensions of input and kernel\n    (Batch, _, l_i, d_i, h_i, w_i) = tuple(input.shape)\n    (l_k, d_k, h_k, w_k) = (\n        weight.size(3),\n        weight.size(4),\n        weight.size(5),\n        weight.size(6),\n    )\n    (l_p, d_p, h_p, w_p) = padding\n    (l_d, d_d, h_d, w_d) = dilation\n    (l_s, d_s, h_s, w_s) = stride\n\n    # Compute the size of the output tensor based on the zero padding\n    l_o = (l_i + 2 * l_p - (l_k) - (l_k - 1) * (l_d - 1)) // l_s + 1\n    d_o = (d_i + 2 * d_p - (d_k) - (d_k - 1) * (d_d - 1)) // d_s + 1\n    h_o = (h_i + 2 * h_p - (h_k) - (h_k - 1) * (h_d - 1)) // h_s + 1\n    w_o = (w_i + 2 * w_p - (w_k) - (w_k - 1) * (w_d - 1)) // w_s + 1\n    out_channels = weight.size(1)\n    # Pre-define output tensors\n    out = torch.zeros(Batch, out_channels, l_o, d_o, h_o, w_o).to(input.device)\n\n    # Convolve each kernel frame i with each input frame j\n    for i in range(l_k):\n        # Calculate the zero-offset of kernel frame i\n        zero_offset = -l_p + (i * l_d)\n        # Calculate the range of input frame j corresponding to kernel frame i\n        j_start = max(zero_offset % l_s, zero_offset)\n        j_end = min(l_i, l_i + l_p - (l_k - i - 1) * l_d)\n        # Convolve each kernel frame i with corresponding input frame j\n        for j in range(j_start, j_end, l_s):\n            # Calculate the output frame\n            out_frame = (j - zero_offset) // l_s\n            # Add results to this output frame\n            out[:, :, out_frame, :, :, :] += F.conv3d(\n                input[:, :, j, :, :],\n                weight[:, :, i, :, :, :],\n                bias=None,\n                stride=stride[1::],\n                padding=padding[1::],\n                dilation=dilation[1::],\n                groups=groups,\n            )\n\n    # Add bias to output\n    if bias is not None:\n        out = out + bias.view(1, -1, 1, 1, 1, 1)\n\n    return out\n</code></pre>"},{"location":"reference/torch_extra/nn/functional/#torch_extra.nn.functional.fourier_ring_correlation","title":"fourier_ring_correlation","text":"<pre><code>fourier_ring_correlation(X: Tensor, Y: Tensor, radial_masks: Optional[Tensor] = None, size_average: bool = True, delta: int = 1) -&gt; Union[int, Tensor]\n</code></pre> <p>Calculate Fourier Ring Correlation (FRC) between two images.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>First image. Shape (batch, channel, height, width).</p> required <code>Y</code> <code>Tensor</code> <p>Second image. Must have the same shape as X.</p> required <code>radial_masks</code> <code>Optional[Tensor]</code> <p>Radial masks. Defaults to None. If None, radial masks are created with ring size of 1 pixel.</p> <code>None</code> <code>size_average</code> <code>bool</code> <p>If True, average FRC over r, batch and channel else averages over batch and channels. Defaults to True.</p> <code>True</code> <code>delta</code> <code>int</code> <p>Ring size in pixels. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>Union[int, Tensor]</code> <p>Union[int, torch.Tensor]: FRC value(s).</p> Source code in <code>torch_extra/nn/functional/loss.py</code> <pre><code>def fourier_ring_correlation(\n    X: torch.Tensor,\n    Y: torch.Tensor,\n    radial_masks: Optional[torch.Tensor] = None,\n    size_average: bool = True,\n    delta: int = 1,\n) -&gt; Union[int, torch.Tensor]:\n    \"\"\"Calculate Fourier Ring Correlation (FRC) between two images.\n\n    Args:\n        X (torch.Tensor): First image. Shape (batch, channel, height, width).\n        Y (torch.Tensor): Second image. Must have the same shape as X.\n        radial_masks (Optional[torch.Tensor]): Radial masks. Defaults to None. If None, radial masks are created with ring size of 1 pixel.\n        size_average (bool): If True, average FRC over r, batch and channel else averages over batch and channels. Defaults to True.\n        delta (int): Ring size in pixels. Defaults to 1.\n\n    Returns:\n        Union[int, torch.Tensor]: FRC value(s).\"\"\"\n    return _fourier_shell_ring_correlation(\n        X,\n        Y,\n        radial_masks=radial_masks,\n        size_average=size_average,\n        dims=[-2, -1],\n        delta=delta,\n    )\n</code></pre>"},{"location":"reference/torch_extra/nn/functional/#torch_extra.nn.functional.fourier_shell_correlation","title":"fourier_shell_correlation","text":"<pre><code>fourier_shell_correlation(X: Tensor, Y: Tensor, radial_masks: Optional[Tensor] = None, size_average: bool = True, delta: int = 1) -&gt; Union[int, Tensor]\n</code></pre> <p>Calculate Fourier Shell Correlation (FSC) between two images.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>First image. Shape (batch, channel, depth, height, width).</p> required <code>Y</code> <code>Tensor</code> <p>Second image. Must have the same shape as X.</p> required <code>radial_masks</code> <code>Optional[Tensor]</code> <p>Radial masks. Defaults to None. If None, radial masks are created with ring size of delta pixel.</p> <code>None</code> <code>size_average</code> <code>bool</code> <p>If True, average FSC over r, batch and channel else averages over batch and channels. Defaults to True.</p> <code>True</code> <code>delta</code> <code>int</code> <p>Ring size in pixels. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>Union[int, Tensor]</code> <p>Union[int, torch.Tensor]: FSC value(s).</p> Source code in <code>torch_extra/nn/functional/loss.py</code> <pre><code>def fourier_shell_correlation(\n    X: torch.Tensor,\n    Y: torch.Tensor,\n    radial_masks: Optional[torch.Tensor] = None,\n    size_average: bool = True,\n    delta: int = 1,\n) -&gt; Union[int, torch.Tensor]:\n    \"\"\"Calculate Fourier Shell Correlation (FSC) between two images.\n\n    Args:\n        X (torch.Tensor): First image. Shape (batch, channel, depth, height, width).\n        Y (torch.Tensor): Second image. Must have the same shape as X.\n        radial_masks (Optional[torch.Tensor]): Radial masks. Defaults to None. If None, radial masks are created with ring size of delta pixel.\n        size_average (bool): If True, average FSC over r, batch and channel else averages over batch and channels. Defaults to True.\n        delta (int): Ring size in pixels. Defaults to 1.\n\n    Returns:\n        Union[int, torch.Tensor]: FSC value(s).\"\"\"\n    return _fourier_shell_ring_correlation(\n        X,\n        Y,\n        radial_masks=radial_masks,\n        size_average=size_average,\n        dims=[-3, -2, -1],\n        delta=delta,\n    )\n</code></pre>"},{"location":"reference/torch_extra/nn/functional/#torch_extra.nn.functional.ms_ssim","title":"ms_ssim","text":"<pre><code>ms_ssim(X: Tensor, Y: Tensor, data_range: float = 255, size_average: bool = True, win_size: int = 11, win_sigma: float = 1.5, win: Optional[Tensor] = None, weights: Optional[List[float]] = None, K: Union[Tuple[float, float], List[float]] = (0.01, 0.03), stride: int = 1) -&gt; Tensor\n</code></pre> <p>interface of ms-ssim Args:     X (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)     Y (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)     data_range (float or int, optional): value range of input images. (usually 1.0 or 255)     size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar     win_size: (int, optional): the size of gauss kernel     win_sigma: (float, optional): sigma of normal distribution     win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma     weights (list, optional): weights for different levels     K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.     stride (int, optional): stride for the convolution Returns:     torch.Tensor: ms-ssim results</p> Source code in <code>torch_extra/nn/functional/loss.py</code> <pre><code>def ms_ssim(\n    X: Tensor,\n    Y: Tensor,\n    data_range: float = 255,\n    size_average: bool = True,\n    win_size: int = 11,\n    win_sigma: float = 1.5,\n    win: Optional[Tensor] = None,\n    weights: Optional[List[float]] = None,\n    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n    stride: int = 1,\n) -&gt; Tensor:\n    r\"\"\"interface of ms-ssim\n    Args:\n        X (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)\n        Y (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)\n        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n        win_size: (int, optional): the size of gauss kernel\n        win_sigma: (float, optional): sigma of normal distribution\n        win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma\n        weights (list, optional): weights for different levels\n        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n        stride (int, optional): stride for the convolution\n    Returns:\n        torch.Tensor: ms-ssim results\n    \"\"\"\n    return _ms_ssim_wrapper(\n        X=X,\n        Y=Y,\n        data_range=data_range,\n        size_average=size_average,\n        win_size=win_size,\n        win_sigma=win_sigma,\n        win=win,\n        weights=weights,\n        K=K,\n        stride=stride,\n    )\n</code></pre>"},{"location":"reference/torch_extra/nn/functional/#torch_extra.nn.functional.ssim","title":"ssim","text":"<pre><code>ssim(X: Tensor, Y: Tensor, data_range: float = 255, size_average: bool = True, win_size: int = 11, win_sigma: float = 1.5, win: Optional[Tensor] = None, K: Union[Tuple[float, float], List[float]] = (0.01, 0.03), nonnegative_ssim: bool = False, stride: int = 1) -&gt; Tensor\n</code></pre> <p>interface of ssim Args:     X (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)     Y (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)     data_range (float or int, optional): value range of input images. (usually 1.0 or 255)     size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar     win_size: (int, optional): the size of gauss kernel     win_sigma: (float, optional): sigma of normal distribution     win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma     K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.     nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu     temporal_win (torch.Tensor, optional): 1-D gauss kernel for temporal dimension. if None, a new kernel will be created according to temporal_win_size and win_sigma     temporal_win_size (int, optional): the size of gauss kernel for temporal dimension     stride (int, optional): stride for the convolution</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: ssim results</p> Source code in <code>torch_extra/nn/functional/loss.py</code> <pre><code>def ssim(\n    X: Tensor,\n    Y: Tensor,\n    data_range: float = 255,\n    size_average: bool = True,\n    win_size: int = 11,\n    win_sigma: float = 1.5,\n    win: Optional[Tensor] = None,\n    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n    nonnegative_ssim: bool = False,\n    stride: int = 1,\n) -&gt; Tensor:\n    r\"\"\"interface of ssim\n    Args:\n        X (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)\n        Y (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)\n        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n        win_size: (int, optional): the size of gauss kernel\n        win_sigma: (float, optional): sigma of normal distribution\n        win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma\n        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n        nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu\n        temporal_win (torch.Tensor, optional): 1-D gauss kernel for temporal dimension. if None, a new kernel will be created according to temporal_win_size and win_sigma\n        temporal_win_size (int, optional): the size of gauss kernel for temporal dimension\n        stride (int, optional): stride for the convolution\n\n    Returns:\n        torch.Tensor: ssim results\n    \"\"\"\n    return _ssim_wrapper(\n        X=X,\n        Y=Y,\n        data_range=data_range,\n        size_average=size_average,\n        win_size=win_size,\n        win_sigma=win_sigma,\n        win=win,\n        K=K,\n        nonnegative_ssim=nonnegative_ssim,\n        stride=stride,\n    )\n</code></pre>"},{"location":"reference/torch_extra/nn/functional/#torch_extra.nn.functional.st_ms_ssim","title":"st_ms_ssim","text":"<pre><code>st_ms_ssim(X: Tensor, Y: Tensor, data_range: float = 255, size_average: bool = True, win_size: int = 11, win_sigma: float = 1.5, win: Optional[Tensor] = None, weights: Optional[List[float]] = None, K: Union[Tuple[float, float], List[float]] = (0.01, 0.03), temporal_win: Optional[Tensor] = None, temporal_win_size: Optional[int] = None, stride: int = 1) -&gt; Tensor\n</code></pre> <p>interface of ms-ssim Args:     X (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)     Y (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)     data_range (float or int, optional): value range of input images. (usually 1.0 or 255)     size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar     win_size: (int, optional): the size of gauss kernel     win_sigma: (float, optional): sigma of normal distribution     win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma     weights (list, optional): weights for different levels     K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.     temporal_win (torch.Tensor, optional): 1-D gauss kernel for temporal dimension. if None, a new kernel will be created according to temporal_win_size and win_sigma     temporal_win_size (int, optional): the size of gauss kernel for temporal dimension     stride (int, optional): stride for the convolution Returns:     torch.Tensor: ms-ssim results</p> Source code in <code>torch_extra/nn/functional/loss.py</code> <pre><code>def st_ms_ssim(\n    X: Tensor,\n    Y: Tensor,\n    data_range: float = 255,\n    size_average: bool = True,\n    win_size: int = 11,\n    win_sigma: float = 1.5,\n    win: Optional[Tensor] = None,\n    weights: Optional[List[float]] = None,\n    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n    temporal_win: Optional[Tensor] = None,\n    temporal_win_size: Optional[int] = None,\n    stride: int = 1,\n) -&gt; Tensor:\n    r\"\"\"interface of ms-ssim\n    Args:\n        X (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)\n        Y (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)\n        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n        win_size: (int, optional): the size of gauss kernel\n        win_sigma: (float, optional): sigma of normal distribution\n        win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma\n        weights (list, optional): weights for different levels\n        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n        temporal_win (torch.Tensor, optional): 1-D gauss kernel for temporal dimension. if None, a new kernel will be created according to temporal_win_size and win_sigma\n        temporal_win_size (int, optional): the size of gauss kernel for temporal dimension\n        stride (int, optional): stride for the convolution\n    Returns:\n        torch.Tensor: ms-ssim results\n    \"\"\"\n    return _ms_ssim_wrapper(\n        X=X,\n        Y=Y,\n        data_range=data_range,\n        size_average=size_average,\n        win_size=win_size,\n        win_sigma=win_sigma,\n        win=win,\n        weights=weights,\n        K=K,\n        temporal_win=temporal_win,\n        temporal_win_size=temporal_win_size,\n        stride=stride,\n    )\n</code></pre>"},{"location":"reference/torch_extra/nn/functional/#torch_extra.nn.functional.st_ssim","title":"st_ssim","text":"<pre><code>st_ssim(X: Tensor, Y: Tensor, data_range: float = 255, size_average: bool = True, win_size: int = 11, win_sigma: float = 1.5, win: Optional[Tensor] = None, K: Union[Tuple[float, float], List[float]] = (0.01, 0.03), nonnegative_ssim: bool = False, temporal_win: Optional[Tensor] = None, temporal_win_size: int = 5, stride: int = 1) -&gt; Tensor\n</code></pre> <p>interface of ssim Args:     X (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)     Y (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)     data_range (float or int, optional): value range of input images. (usually 1.0 or 255)     size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar     win_size: (int, optional): the size of gauss kernel     win_sigma: (float, optional): sigma of normal distribution     win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma     K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.     nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu     temporal_win (torch.Tensor, optional): 1-D gauss kernel for temporal dimension. if None, a new kernel will be created according to temporal_win_size and win_sigma     temporal_win_size (int, optional): the size of gauss kernel for temporal dimension     stride (int, optional): stride for the convolution</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: ssim results</p> Source code in <code>torch_extra/nn/functional/loss.py</code> <pre><code>def st_ssim(\n    X: Tensor,\n    Y: Tensor,\n    data_range: float = 255,\n    size_average: bool = True,\n    win_size: int = 11,\n    win_sigma: float = 1.5,\n    win: Optional[Tensor] = None,\n    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n    nonnegative_ssim: bool = False,\n    temporal_win: Optional[Tensor] = None,\n    temporal_win_size: int = 5,\n    stride: int = 1,\n) -&gt; Tensor:\n    r\"\"\"interface of ssim\n    Args:\n        X (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)\n        Y (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)\n        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n        win_size: (int, optional): the size of gauss kernel\n        win_sigma: (float, optional): sigma of normal distribution\n        win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma\n        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n        nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu\n        temporal_win (torch.Tensor, optional): 1-D gauss kernel for temporal dimension. if None, a new kernel will be created according to temporal_win_size and win_sigma\n        temporal_win_size (int, optional): the size of gauss kernel for temporal dimension\n        stride (int, optional): stride for the convolution\n\n    Returns:\n        torch.Tensor: ssim results\n    \"\"\"\n    return _ssim_wrapper(\n        X=X,\n        Y=Y,\n        data_range=data_range,\n        size_average=size_average,\n        win_size=win_size,\n        win_sigma=win_sigma,\n        win=win,\n        K=K,\n        nonnegative_ssim=nonnegative_ssim,\n        temporal_win=temporal_win,\n        temporal_win_size=temporal_win_size,\n        stride=stride,\n    )\n</code></pre>"},{"location":"reference/torch_extra/nn/functional/conv/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> conv","text":""},{"location":"reference/torch_extra/nn/functional/conv/#torch_extra.nn.functional.conv","title":"conv","text":""},{"location":"reference/torch_extra/nn/functional/conv/#torch_extra.nn.functional.conv.conv4d","title":"conv4d","text":"<pre><code>conv4d(input: Tensor, weight: Tensor, bias: Optional[Tensor] = None, stride: Union[_int, _size] = 1, padding: Union[_int, _size] = 0, dilation: Union[_int, _size] = 1, groups: _int = 1) -&gt; Tensor\n</code></pre> <p>Applies a 4D convolution over an input signal composed of several input planes. Based on  https://github.com/ZhengyuLiang24/Conv4d-PyTorch</p> Source code in <code>torch_extra/nn/functional/conv.py</code> <pre><code>def conv4d(\n    input: Tensor,\n    weight: Tensor,\n    bias: Optional[Tensor] = None,\n    stride: Union[_int, _size] = 1,\n    padding: Union[_int, _size] = 0,\n    dilation: Union[_int, _size] = 1,\n    groups: _int = 1,\n) -&gt; Tensor:\n    \"\"\"Applies a 4D convolution over an input signal composed of several input\n    planes. Based on  https://github.com/ZhengyuLiang24/Conv4d-PyTorch\n    \"\"\"\n    # Define shortcut names for dimensions of input and kernel\n    (Batch, _, l_i, d_i, h_i, w_i) = tuple(input.shape)\n    (l_k, d_k, h_k, w_k) = (\n        weight.size(3),\n        weight.size(4),\n        weight.size(5),\n        weight.size(6),\n    )\n    (l_p, d_p, h_p, w_p) = padding\n    (l_d, d_d, h_d, w_d) = dilation\n    (l_s, d_s, h_s, w_s) = stride\n\n    # Compute the size of the output tensor based on the zero padding\n    l_o = (l_i + 2 * l_p - (l_k) - (l_k - 1) * (l_d - 1)) // l_s + 1\n    d_o = (d_i + 2 * d_p - (d_k) - (d_k - 1) * (d_d - 1)) // d_s + 1\n    h_o = (h_i + 2 * h_p - (h_k) - (h_k - 1) * (h_d - 1)) // h_s + 1\n    w_o = (w_i + 2 * w_p - (w_k) - (w_k - 1) * (w_d - 1)) // w_s + 1\n    out_channels = weight.size(1)\n    # Pre-define output tensors\n    out = torch.zeros(Batch, out_channels, l_o, d_o, h_o, w_o).to(input.device)\n\n    # Convolve each kernel frame i with each input frame j\n    for i in range(l_k):\n        # Calculate the zero-offset of kernel frame i\n        zero_offset = -l_p + (i * l_d)\n        # Calculate the range of input frame j corresponding to kernel frame i\n        j_start = max(zero_offset % l_s, zero_offset)\n        j_end = min(l_i, l_i + l_p - (l_k - i - 1) * l_d)\n        # Convolve each kernel frame i with corresponding input frame j\n        for j in range(j_start, j_end, l_s):\n            # Calculate the output frame\n            out_frame = (j - zero_offset) // l_s\n            # Add results to this output frame\n            out[:, :, out_frame, :, :, :] += F.conv3d(\n                input[:, :, j, :, :],\n                weight[:, :, i, :, :, :],\n                bias=None,\n                stride=stride[1::],\n                padding=padding[1::],\n                dilation=dilation[1::],\n                groups=groups,\n            )\n\n    # Add bias to output\n    if bias is not None:\n        out = out + bias.view(1, -1, 1, 1, 1, 1)\n\n    return out\n</code></pre>"},{"location":"reference/torch_extra/nn/functional/loss/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> loss","text":""},{"location":"reference/torch_extra/nn/functional/loss/#torch_extra.nn.functional.loss","title":"loss","text":""},{"location":"reference/torch_extra/nn/functional/loss/#torch_extra.nn.functional.loss.fourier_ring_correlation","title":"fourier_ring_correlation","text":"<pre><code>fourier_ring_correlation(X: Tensor, Y: Tensor, radial_masks: Optional[Tensor] = None, size_average: bool = True, delta: int = 1) -&gt; Union[int, Tensor]\n</code></pre> <p>Calculate Fourier Ring Correlation (FRC) between two images.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>First image. Shape (batch, channel, height, width).</p> required <code>Y</code> <code>Tensor</code> <p>Second image. Must have the same shape as X.</p> required <code>radial_masks</code> <code>Optional[Tensor]</code> <p>Radial masks. Defaults to None. If None, radial masks are created with ring size of 1 pixel.</p> <code>None</code> <code>size_average</code> <code>bool</code> <p>If True, average FRC over r, batch and channel else averages over batch and channels. Defaults to True.</p> <code>True</code> <code>delta</code> <code>int</code> <p>Ring size in pixels. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>Union[int, Tensor]</code> <p>Union[int, torch.Tensor]: FRC value(s).</p> Source code in <code>torch_extra/nn/functional/loss.py</code> <pre><code>def fourier_ring_correlation(\n    X: torch.Tensor,\n    Y: torch.Tensor,\n    radial_masks: Optional[torch.Tensor] = None,\n    size_average: bool = True,\n    delta: int = 1,\n) -&gt; Union[int, torch.Tensor]:\n    \"\"\"Calculate Fourier Ring Correlation (FRC) between two images.\n\n    Args:\n        X (torch.Tensor): First image. Shape (batch, channel, height, width).\n        Y (torch.Tensor): Second image. Must have the same shape as X.\n        radial_masks (Optional[torch.Tensor]): Radial masks. Defaults to None. If None, radial masks are created with ring size of 1 pixel.\n        size_average (bool): If True, average FRC over r, batch and channel else averages over batch and channels. Defaults to True.\n        delta (int): Ring size in pixels. Defaults to 1.\n\n    Returns:\n        Union[int, torch.Tensor]: FRC value(s).\"\"\"\n    return _fourier_shell_ring_correlation(\n        X,\n        Y,\n        radial_masks=radial_masks,\n        size_average=size_average,\n        dims=[-2, -1],\n        delta=delta,\n    )\n</code></pre>"},{"location":"reference/torch_extra/nn/functional/loss/#torch_extra.nn.functional.loss.fourier_shell_correlation","title":"fourier_shell_correlation","text":"<pre><code>fourier_shell_correlation(X: Tensor, Y: Tensor, radial_masks: Optional[Tensor] = None, size_average: bool = True, delta: int = 1) -&gt; Union[int, Tensor]\n</code></pre> <p>Calculate Fourier Shell Correlation (FSC) between two images.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Tensor</code> <p>First image. Shape (batch, channel, depth, height, width).</p> required <code>Y</code> <code>Tensor</code> <p>Second image. Must have the same shape as X.</p> required <code>radial_masks</code> <code>Optional[Tensor]</code> <p>Radial masks. Defaults to None. If None, radial masks are created with ring size of delta pixel.</p> <code>None</code> <code>size_average</code> <code>bool</code> <p>If True, average FSC over r, batch and channel else averages over batch and channels. Defaults to True.</p> <code>True</code> <code>delta</code> <code>int</code> <p>Ring size in pixels. Defaults to 1.</p> <code>1</code> <p>Returns:</p> Type Description <code>Union[int, Tensor]</code> <p>Union[int, torch.Tensor]: FSC value(s).</p> Source code in <code>torch_extra/nn/functional/loss.py</code> <pre><code>def fourier_shell_correlation(\n    X: torch.Tensor,\n    Y: torch.Tensor,\n    radial_masks: Optional[torch.Tensor] = None,\n    size_average: bool = True,\n    delta: int = 1,\n) -&gt; Union[int, torch.Tensor]:\n    \"\"\"Calculate Fourier Shell Correlation (FSC) between two images.\n\n    Args:\n        X (torch.Tensor): First image. Shape (batch, channel, depth, height, width).\n        Y (torch.Tensor): Second image. Must have the same shape as X.\n        radial_masks (Optional[torch.Tensor]): Radial masks. Defaults to None. If None, radial masks are created with ring size of delta pixel.\n        size_average (bool): If True, average FSC over r, batch and channel else averages over batch and channels. Defaults to True.\n        delta (int): Ring size in pixels. Defaults to 1.\n\n    Returns:\n        Union[int, torch.Tensor]: FSC value(s).\"\"\"\n    return _fourier_shell_ring_correlation(\n        X,\n        Y,\n        radial_masks=radial_masks,\n        size_average=size_average,\n        dims=[-3, -2, -1],\n        delta=delta,\n    )\n</code></pre>"},{"location":"reference/torch_extra/nn/functional/loss/#torch_extra.nn.functional.loss.gaussian_filter","title":"gaussian_filter","text":"<pre><code>gaussian_filter(input: Tensor, win: Tensor, temporal_win: Optional[Tensor] = None, stride: int = 1) -&gt; Tensor\n</code></pre> <p>Blur input with 1-D kernel Args:     input (torch.Tensor): a batch of tensors to be blurred     window (torch.Tensor): 1-D gauss kernel Returns:     torch.Tensor: blurred tensors</p> Source code in <code>torch_extra/nn/functional/loss.py</code> <pre><code>def gaussian_filter(input: Tensor, win: Tensor, temporal_win: Optional[Tensor] = None, stride: int = 1) -&gt; Tensor:\n    r\"\"\"Blur input with 1-D kernel\n    Args:\n        input (torch.Tensor): a batch of tensors to be blurred\n        window (torch.Tensor): 1-D gauss kernel\n    Returns:\n        torch.Tensor: blurred tensors\n    \"\"\"\n    assert all([ws == 1 for ws in win.shape[1:-1]]), win.shape\n    if len(input.shape) == 4:\n        conv = F.conv2d\n    elif len(input.shape) == 5:\n        conv = F.conv3d\n    elif len(input.shape) == 6:\n        conv = F_extra.conv4d\n    else:\n        raise NotImplementedError(input.shape)\n    C = input.shape[1]\n    out = input\n    temporal_dim = 0\n    if temporal_win is not None:\n        temporal_dim = 1\n    # print(\"Input shape\", input.shape[2+temporal_dim:])\n    for i, s in enumerate(input.shape[2 + temporal_dim :]):\n        # print(\"Window size\", win.transpose(2 + temporal_dim + i, -1).size())\n        if s &gt;= win.shape[-1]:\n            out = conv(\n                out,\n                weight=win.transpose(2 + temporal_dim + i, -1),\n                stride=stride,\n                padding=0,\n                groups=C,\n            )\n        else:\n            warnings.warn(\n                f\"Skipping Gaussian Smoothing at dimension 2+{i} for input: {input.shape} and win size: {win.shape[-1]}\"\n            )\n    if temporal_win is not None:\n        if input.shape[2] &gt;= temporal_win.shape[-1]:\n            # print(\"Temporal window size\", temporal_win.size())\n\n            out = conv(out, weight=temporal_win, stride=1, padding=0, groups=C)\n        else:\n            warnings.warn(\n                f\"Skipping Gaussian Smoothing at time-dimension 2 for input: {input.shape} and temporal win size: {temporal_win.shape[-1]}\"\n            )\n    return out\n</code></pre>"},{"location":"reference/torch_extra/nn/functional/loss/#torch_extra.nn.functional.loss.ms_ssim","title":"ms_ssim","text":"<pre><code>ms_ssim(X: Tensor, Y: Tensor, data_range: float = 255, size_average: bool = True, win_size: int = 11, win_sigma: float = 1.5, win: Optional[Tensor] = None, weights: Optional[List[float]] = None, K: Union[Tuple[float, float], List[float]] = (0.01, 0.03), stride: int = 1) -&gt; Tensor\n</code></pre> <p>interface of ms-ssim Args:     X (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)     Y (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)     data_range (float or int, optional): value range of input images. (usually 1.0 or 255)     size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar     win_size: (int, optional): the size of gauss kernel     win_sigma: (float, optional): sigma of normal distribution     win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma     weights (list, optional): weights for different levels     K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.     stride (int, optional): stride for the convolution Returns:     torch.Tensor: ms-ssim results</p> Source code in <code>torch_extra/nn/functional/loss.py</code> <pre><code>def ms_ssim(\n    X: Tensor,\n    Y: Tensor,\n    data_range: float = 255,\n    size_average: bool = True,\n    win_size: int = 11,\n    win_sigma: float = 1.5,\n    win: Optional[Tensor] = None,\n    weights: Optional[List[float]] = None,\n    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n    stride: int = 1,\n) -&gt; Tensor:\n    r\"\"\"interface of ms-ssim\n    Args:\n        X (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)\n        Y (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)\n        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n        win_size: (int, optional): the size of gauss kernel\n        win_sigma: (float, optional): sigma of normal distribution\n        win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma\n        weights (list, optional): weights for different levels\n        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n        stride (int, optional): stride for the convolution\n    Returns:\n        torch.Tensor: ms-ssim results\n    \"\"\"\n    return _ms_ssim_wrapper(\n        X=X,\n        Y=Y,\n        data_range=data_range,\n        size_average=size_average,\n        win_size=win_size,\n        win_sigma=win_sigma,\n        win=win,\n        weights=weights,\n        K=K,\n        stride=stride,\n    )\n</code></pre>"},{"location":"reference/torch_extra/nn/functional/loss/#torch_extra.nn.functional.loss.ssim","title":"ssim","text":"<pre><code>ssim(X: Tensor, Y: Tensor, data_range: float = 255, size_average: bool = True, win_size: int = 11, win_sigma: float = 1.5, win: Optional[Tensor] = None, K: Union[Tuple[float, float], List[float]] = (0.01, 0.03), nonnegative_ssim: bool = False, stride: int = 1) -&gt; Tensor\n</code></pre> <p>interface of ssim Args:     X (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)     Y (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)     data_range (float or int, optional): value range of input images. (usually 1.0 or 255)     size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar     win_size: (int, optional): the size of gauss kernel     win_sigma: (float, optional): sigma of normal distribution     win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma     K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.     nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu     temporal_win (torch.Tensor, optional): 1-D gauss kernel for temporal dimension. if None, a new kernel will be created according to temporal_win_size and win_sigma     temporal_win_size (int, optional): the size of gauss kernel for temporal dimension     stride (int, optional): stride for the convolution</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: ssim results</p> Source code in <code>torch_extra/nn/functional/loss.py</code> <pre><code>def ssim(\n    X: Tensor,\n    Y: Tensor,\n    data_range: float = 255,\n    size_average: bool = True,\n    win_size: int = 11,\n    win_sigma: float = 1.5,\n    win: Optional[Tensor] = None,\n    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n    nonnegative_ssim: bool = False,\n    stride: int = 1,\n) -&gt; Tensor:\n    r\"\"\"interface of ssim\n    Args:\n        X (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)\n        Y (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)\n        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n        win_size: (int, optional): the size of gauss kernel\n        win_sigma: (float, optional): sigma of normal distribution\n        win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma\n        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n        nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu\n        temporal_win (torch.Tensor, optional): 1-D gauss kernel for temporal dimension. if None, a new kernel will be created according to temporal_win_size and win_sigma\n        temporal_win_size (int, optional): the size of gauss kernel for temporal dimension\n        stride (int, optional): stride for the convolution\n\n    Returns:\n        torch.Tensor: ssim results\n    \"\"\"\n    return _ssim_wrapper(\n        X=X,\n        Y=Y,\n        data_range=data_range,\n        size_average=size_average,\n        win_size=win_size,\n        win_sigma=win_sigma,\n        win=win,\n        K=K,\n        nonnegative_ssim=nonnegative_ssim,\n        stride=stride,\n    )\n</code></pre>"},{"location":"reference/torch_extra/nn/functional/loss/#torch_extra.nn.functional.loss.st_ms_ssim","title":"st_ms_ssim","text":"<pre><code>st_ms_ssim(X: Tensor, Y: Tensor, data_range: float = 255, size_average: bool = True, win_size: int = 11, win_sigma: float = 1.5, win: Optional[Tensor] = None, weights: Optional[List[float]] = None, K: Union[Tuple[float, float], List[float]] = (0.01, 0.03), temporal_win: Optional[Tensor] = None, temporal_win_size: Optional[int] = None, stride: int = 1) -&gt; Tensor\n</code></pre> <p>interface of ms-ssim Args:     X (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)     Y (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)     data_range (float or int, optional): value range of input images. (usually 1.0 or 255)     size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar     win_size: (int, optional): the size of gauss kernel     win_sigma: (float, optional): sigma of normal distribution     win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma     weights (list, optional): weights for different levels     K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.     temporal_win (torch.Tensor, optional): 1-D gauss kernel for temporal dimension. if None, a new kernel will be created according to temporal_win_size and win_sigma     temporal_win_size (int, optional): the size of gauss kernel for temporal dimension     stride (int, optional): stride for the convolution Returns:     torch.Tensor: ms-ssim results</p> Source code in <code>torch_extra/nn/functional/loss.py</code> <pre><code>def st_ms_ssim(\n    X: Tensor,\n    Y: Tensor,\n    data_range: float = 255,\n    size_average: bool = True,\n    win_size: int = 11,\n    win_sigma: float = 1.5,\n    win: Optional[Tensor] = None,\n    weights: Optional[List[float]] = None,\n    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n    temporal_win: Optional[Tensor] = None,\n    temporal_win_size: Optional[int] = None,\n    stride: int = 1,\n) -&gt; Tensor:\n    r\"\"\"interface of ms-ssim\n    Args:\n        X (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)\n        Y (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)\n        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n        win_size: (int, optional): the size of gauss kernel\n        win_sigma: (float, optional): sigma of normal distribution\n        win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma\n        weights (list, optional): weights for different levels\n        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n        temporal_win (torch.Tensor, optional): 1-D gauss kernel for temporal dimension. if None, a new kernel will be created according to temporal_win_size and win_sigma\n        temporal_win_size (int, optional): the size of gauss kernel for temporal dimension\n        stride (int, optional): stride for the convolution\n    Returns:\n        torch.Tensor: ms-ssim results\n    \"\"\"\n    return _ms_ssim_wrapper(\n        X=X,\n        Y=Y,\n        data_range=data_range,\n        size_average=size_average,\n        win_size=win_size,\n        win_sigma=win_sigma,\n        win=win,\n        weights=weights,\n        K=K,\n        temporal_win=temporal_win,\n        temporal_win_size=temporal_win_size,\n        stride=stride,\n    )\n</code></pre>"},{"location":"reference/torch_extra/nn/functional/loss/#torch_extra.nn.functional.loss.st_ssim","title":"st_ssim","text":"<pre><code>st_ssim(X: Tensor, Y: Tensor, data_range: float = 255, size_average: bool = True, win_size: int = 11, win_sigma: float = 1.5, win: Optional[Tensor] = None, K: Union[Tuple[float, float], List[float]] = (0.01, 0.03), nonnegative_ssim: bool = False, temporal_win: Optional[Tensor] = None, temporal_win_size: int = 5, stride: int = 1) -&gt; Tensor\n</code></pre> <p>interface of ssim Args:     X (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)     Y (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)     data_range (float or int, optional): value range of input images. (usually 1.0 or 255)     size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar     win_size: (int, optional): the size of gauss kernel     win_sigma: (float, optional): sigma of normal distribution     win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma     K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.     nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu     temporal_win (torch.Tensor, optional): 1-D gauss kernel for temporal dimension. if None, a new kernel will be created according to temporal_win_size and win_sigma     temporal_win_size (int, optional): the size of gauss kernel for temporal dimension     stride (int, optional): stride for the convolution</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: ssim results</p> Source code in <code>torch_extra/nn/functional/loss.py</code> <pre><code>def st_ssim(\n    X: Tensor,\n    Y: Tensor,\n    data_range: float = 255,\n    size_average: bool = True,\n    win_size: int = 11,\n    win_sigma: float = 1.5,\n    win: Optional[Tensor] = None,\n    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n    nonnegative_ssim: bool = False,\n    temporal_win: Optional[Tensor] = None,\n    temporal_win_size: int = 5,\n    stride: int = 1,\n) -&gt; Tensor:\n    r\"\"\"interface of ssim\n    Args:\n        X (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)\n        Y (torch.Tensor): a batch of images, (N,C,[T,D,]H,W)\n        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n        win_size: (int, optional): the size of gauss kernel\n        win_sigma: (float, optional): sigma of normal distribution\n        win (torch.Tensor, optional): 1-D gauss kernel. if None, a new kernel will be created according to win_size and win_sigma\n        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n        nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu\n        temporal_win (torch.Tensor, optional): 1-D gauss kernel for temporal dimension. if None, a new kernel will be created according to temporal_win_size and win_sigma\n        temporal_win_size (int, optional): the size of gauss kernel for temporal dimension\n        stride (int, optional): stride for the convolution\n\n    Returns:\n        torch.Tensor: ssim results\n    \"\"\"\n    return _ssim_wrapper(\n        X=X,\n        Y=Y,\n        data_range=data_range,\n        size_average=size_average,\n        win_size=win_size,\n        win_sigma=win_sigma,\n        win=win,\n        K=K,\n        nonnegative_ssim=nonnegative_ssim,\n        temporal_win=temporal_win,\n        temporal_win_size=temporal_win_size,\n        stride=stride,\n    )\n</code></pre>"},{"location":"reference/torch_extra/nn/functional/pooling/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> pooling","text":""},{"location":"reference/torch_extra/nn/functional/pooling/#torch_extra.nn.functional.pooling","title":"pooling","text":""},{"location":"reference/torch_extra/nn/modules/","title":"Index","text":""},{"location":"reference/torch_extra/nn/modules/#torch_extra.nn.modules","title":"modules","text":""},{"location":"reference/torch_extra/nn/modules/#torch_extra.nn.modules.ConvTranspose4d","title":"ConvTranspose4d","text":"<pre><code>ConvTranspose4d(in_channels: int, out_channels: int, kernel_size: _size_4_t, stride: _size_4_t = 1, padding: _size_4_t = 0, output_padding: _size_4_t = 0, groups: int = 1, bias: bool = True, dilation: _size_4_t = 1, padding_mode: str = 'zeros', device=None, dtype=None)\n</code></pre> <p>               Bases: <code>_ConvTransposeNd</code></p> <p>From https://github.com/matheusja/ConvTranspose4d-PyTorch</p> Source code in <code>torch_extra/nn/modules/conv.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_4_t,\n    stride: _size_4_t = 1,\n    padding: _size_4_t = 0,\n    output_padding: _size_4_t = 0,\n    groups: int = 1,\n    bias: bool = True,\n    dilation: _size_4_t = 1,\n    padding_mode: str = \"zeros\",\n    device=None,\n    dtype=None,\n) -&gt; None:\n    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n    kernel_size = _quadruple(kernel_size)\n    stride = _quadruple(stride)\n    padding = _quadruple(padding)\n    dilation = _quadruple(dilation)\n    if dilation != (1, 1, 1, 1):\n        raise NotImplementedError(\"Dilation other than 1 not yet implemented!\")\n    output_padding = _quadruple(output_padding)\n    super().__init__(\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        dilation,\n        True,\n        output_padding,\n        groups,\n        bias,\n        padding_mode,\n        **factory_kwargs,\n    )\n</code></pre>"},{"location":"reference/torch_extra/nn/modules/#torch_extra.nn.modules.FourierRingCorrelation","title":"FourierRingCorrelation","text":"<pre><code>FourierRingCorrelation(shape: Tuple[int, ...], size_average: bool = True, delta: int = 1)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Fourier Ring Correlation (FRC) loss function.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>tuple[int, ...]</code> <p>shape of the input tensors.</p> required <code>size_average</code> <code>bool</code> <p>if size_average=True, FRC will be averaged over radii, channel and batch. Else, FRC will be averaged over channel and batch. Default: True.</p> <code>True</code> <code>delta</code> <code>int</code> <p>delta for the ring size. Default: 1.</p> <code>1</code> Source code in <code>torch_extra/nn/modules/loss.py</code> <pre><code>def __init__(self, shape: Tuple[int, ...], size_average: bool = True, delta: int = 1) -&gt; None:\n    super(FourierRingCorrelation, self).__init__()\n    self.size_average = size_average\n    self.radial_masks = F_extra._create_radial_masks(shape, delta=delta, dims=2)\n</code></pre>"},{"location":"reference/torch_extra/nn/modules/#torch_extra.nn.modules.FourierShellCorrelation","title":"FourierShellCorrelation","text":"<pre><code>FourierShellCorrelation(shape: Tuple[int, ...], size_average: bool = True, delta: int = 1)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Fourier Shell Correlation (FSC) loss function.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>tuple[int, ...]</code> <p>shape of the input tensors.</p> required <code>size_average</code> <code>bool</code> <p>if size_average=True, FSC will be averaged over radii, channel and batch. Else, FSC will be averaged over channel and batch. Default: True.</p> <code>True</code> <code>delta</code> <code>int</code> <p>delta for the shell size. Default: 1.</p> <code>1</code> Source code in <code>torch_extra/nn/modules/loss.py</code> <pre><code>def __init__(self, shape: Tuple[int, ...], size_average: bool = True, delta: int = 1) -&gt; None:\n    super(FourierShellCorrelation, self).__init__()\n    self.size_average = size_average\n    self.radial_masks = F_extra._create_radial_masks(shape, delta=delta, dims=3)\n</code></pre>"},{"location":"reference/torch_extra/nn/modules/#torch_extra.nn.modules.MS_SSIM","title":"MS_SSIM","text":"<pre><code>MS_SSIM(data_range: float = 255, size_average: bool = True, win_size: int = 11, win_sigma: float = 1.5, channel: int = 3, spatial_dims: int = 2, weights: Optional[List[float]] = None, K: Union[Tuple[float, float], List[float]] = (0.01, 0.03), stride: int = 1)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>class for ms-ssim Args:     data_range (float or int, optional): value range of input images. (usually 1.0 or 255)     size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar     win_size: (int, optional): the size of gauss kernel     win_sigma: (float, optional): sigma of normal distribution     channel (int, optional): input channels (default: 3)     weights (list, optional): weights for different levels     K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.     stride (int, optional): stride of the sliding window</p> Source code in <code>torch_extra/nn/modules/loss.py</code> <pre><code>def __init__(\n    self,\n    data_range: float = 255,\n    size_average: bool = True,\n    win_size: int = 11,\n    win_sigma: float = 1.5,\n    channel: int = 3,\n    spatial_dims: int = 2,\n    weights: Optional[List[float]] = None,\n    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n    stride: int = 1,\n) -&gt; None:\n    r\"\"\"class for ms-ssim\n    Args:\n        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n        win_size: (int, optional): the size of gauss kernel\n        win_sigma: (float, optional): sigma of normal distribution\n        channel (int, optional): input channels (default: 3)\n        weights (list, optional): weights for different levels\n        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n        stride (int, optional): stride of the sliding window\n    \"\"\"\n\n    super(MS_SSIM, self).__init__()\n    self.win_size = win_size\n    self.win = F_extra._fspecial_gauss_1d(win_size, win_sigma).repeat([channel, 1] + [1] * spatial_dims)\n    self.size_average = size_average\n    self.data_range = data_range\n    self.weights = weights\n    self.K = K\n    self.stride = stride\n</code></pre>"},{"location":"reference/torch_extra/nn/modules/#torch_extra.nn.modules.SSIM","title":"SSIM","text":"<pre><code>SSIM(data_range: float = 255, size_average: bool = True, win_size: int = 11, win_sigma: float = 1.5, channel: int = 3, spatial_dims: int = 2, K: Union[Tuple[float, float], List[float]] = (0.01, 0.03), win: Optional[Tensor] = None, nonnegative_ssim: bool = False, stride: int = 1, device: Optional[device | int | str] = None)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>class for ssim Args:     data_range (float or int, optional): value range of input images. (usually 1.0 or 255)     size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar     win_size: (int, optional): the size of gauss kernel     win_sigma: (float, optional): sigma of normal distribution     channel (int, optional): input channels (default: 3)     K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.     win (Tensor, optional): window for ssim. The size should be 1xwin_size     nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu.     stride (int, optional): stride of the sliding window     device (torch.device, int or str, optional): the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (default: None). If provided, it will send kernel to device before forward.</p> Source code in <code>torch_extra/nn/modules/loss.py</code> <pre><code>def __init__(\n    self,\n    data_range: float = 255,\n    size_average: bool = True,\n    win_size: int = 11,\n    win_sigma: float = 1.5,\n    channel: int = 3,\n    spatial_dims: int = 2,\n    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n    win: Optional[Tensor] = None,\n    nonnegative_ssim: bool = False,\n    stride: int = 1,\n    device: Optional[torch.device | int | str] = None,\n) -&gt; None:\n    r\"\"\"class for ssim\n    Args:\n        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n        win_size: (int, optional): the size of gauss kernel\n        win_sigma: (float, optional): sigma of normal distribution\n        channel (int, optional): input channels (default: 3)\n        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n        win (Tensor, optional): window for ssim. The size should be 1xwin_size\n        nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu.\n        stride (int, optional): stride of the sliding window\n        device (torch.device, int or str, optional): the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (default: None). If provided, it will send kernel to device before forward.\n    \"\"\"\n\n    super(SSIM, self).__init__()\n    self.win_size = win_size\n    if win is not None:\n        if len(win.size()) == 1:\n            win = win.repeat([channel, 1] + [1] * spatial_dims)\n        self.win = win\n    else:\n        self.win = F_extra._fspecial_gauss_1d(win_size, win_sigma).repeat([channel, 1] + [1] * spatial_dims)\n    if device:\n        self.win = self.win.to(device=device, dtype=torch.float32)\n    self.size_average = size_average\n    self.data_range = data_range\n    self.K = K\n    self.nonnegative_ssim = nonnegative_ssim\n    self.stride = stride\n</code></pre>"},{"location":"reference/torch_extra/nn/modules/#torch_extra.nn.modules.ST_MS_SSIM","title":"ST_MS_SSIM","text":"<pre><code>ST_MS_SSIM(data_range: float = 255, size_average: bool = True, win_size: int = 11, win_sigma: float = 1.5, channel: int = 3, spatial_dims: int = 2, weights: Optional[List[float]] = None, K: Union[Tuple[float, float], List[float]] = (0.01, 0.03), temporal_win_size: Optional[int] = None)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>class for ms-ssim Args:     data_range (float or int, optional): value range of input images. (usually 1.0 or 255)     size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar     win_size: (int, optional): the size of gauss kernel     win_sigma: (float, optional): sigma of normal distribution     channel (int, optional): input channels (default: 3)     weights (list, optional): weights for different levels     K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.     temporal_win_size (int, optional): the size of gauss kernel for temporal dimension</p> Source code in <code>torch_extra/nn/modules/loss.py</code> <pre><code>def __init__(\n    self,\n    data_range: float = 255,\n    size_average: bool = True,\n    win_size: int = 11,\n    win_sigma: float = 1.5,\n    channel: int = 3,\n    spatial_dims: int = 2,\n    weights: Optional[List[float]] = None,\n    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n    temporal_win_size: Optional[int] = None,\n) -&gt; None:\n    r\"\"\"class for ms-ssim\n    Args:\n        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n        win_size: (int, optional): the size of gauss kernel\n        win_sigma: (float, optional): sigma of normal distribution\n        channel (int, optional): input channels (default: 3)\n        weights (list, optional): weights for different levels\n        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n        temporal_win_size (int, optional): the size of gauss kernel for temporal dimension\n    \"\"\"\n\n    super(ST_MS_SSIM, self).__init__()\n    self.win_size = win_size\n    self.temporal_win_size = temporal_win_size\n    temporal_dim = 1 if temporal_win_size is not None else 0\n    self.win = F_extra._fspecial_gauss_1d(win_size, win_sigma).repeat(\n        [channel, 1] + [1] * (spatial_dims + temporal_dim)\n    )\n    if temporal_dim:\n        self.temporal_win = (\n            F_extra._fspecial_gauss_1d(temporal_win_size, win_sigma)\n            .repeat([channel, 1] + [1] * (spatial_dims + temporal_dim))\n            .transpose(-1, 2)\n        )\n    else:\n        self.temporal_win = None\n    self.size_average = size_average\n    self.data_range = data_range\n    self.K = K\n    self.weights = weights\n</code></pre>"},{"location":"reference/torch_extra/nn/modules/#torch_extra.nn.modules.ST_SSIM","title":"ST_SSIM","text":"<pre><code>ST_SSIM(data_range: float = 255, size_average: bool = True, win_size: int = 11, win_sigma: float = 1.5, channel: int = 3, spatial_dims: int = 2, K: Union[Tuple[float, float], List[float]] = (0.01, 0.03), nonnegative_ssim: bool = False, temporal_win_size: Optional[int] = None, stride: int = 1)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>class for ssim Args:     data_range (float or int, optional): value range of input images. (usually 1.0 or 255)     size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar     win_size: (int, optional): the size of gauss kernel     win_sigma: (float, optional): sigma of normal distribution     channel (int, optional): input channels (default: 3)     K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.     nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu.     temporal_win_size (int, optional): the size of gauss kernel for temporal dimension     stride (int, optional): stride of the sliding window</p> Source code in <code>torch_extra/nn/modules/loss.py</code> <pre><code>def __init__(\n    self,\n    data_range: float = 255,\n    size_average: bool = True,\n    win_size: int = 11,\n    win_sigma: float = 1.5,\n    channel: int = 3,\n    spatial_dims: int = 2,\n    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n    nonnegative_ssim: bool = False,\n    temporal_win_size: Optional[int] = None,\n    stride: int = 1,\n) -&gt; None:\n    r\"\"\"class for ssim\n    Args:\n        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n        win_size: (int, optional): the size of gauss kernel\n        win_sigma: (float, optional): sigma of normal distribution\n        channel (int, optional): input channels (default: 3)\n        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n        nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu.\n        temporal_win_size (int, optional): the size of gauss kernel for temporal dimension\n        stride (int, optional): stride of the sliding window\n    \"\"\"\n\n    super(ST_SSIM, self).__init__()\n    self.win_size = win_size\n    self.temporal_win_size = temporal_win_size\n    temporal_dim = 1 if temporal_win_size is not None else 0\n    self.win = F_extra._fspecial_gauss_1d(win_size, win_sigma).repeat(\n        [channel, 1] + [1] * (spatial_dims + temporal_dim)\n    )\n    if temporal_dim:\n        self.temporal_win = (\n            F_extra._fspecial_gauss_1d(temporal_win_size, win_sigma)\n            .repeat([channel, 1] + [1] * (spatial_dims + temporal_dim))\n            .transpose(-1, 2)\n        )\n    else:\n        self.temporal_win = None\n    self.size_average = size_average\n    self.data_range = data_range\n    self.K = K\n    self.nonnegative_ssim = nonnegative_ssim\n    self.stride = stride\n</code></pre>"},{"location":"reference/torch_extra/nn/modules/conv/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> conv","text":""},{"location":"reference/torch_extra/nn/modules/conv/#torch_extra.nn.modules.conv","title":"conv","text":""},{"location":"reference/torch_extra/nn/modules/conv/#torch_extra.nn.modules.conv.ConvTranspose4d","title":"ConvTranspose4d","text":"<pre><code>ConvTranspose4d(in_channels: int, out_channels: int, kernel_size: _size_4_t, stride: _size_4_t = 1, padding: _size_4_t = 0, output_padding: _size_4_t = 0, groups: int = 1, bias: bool = True, dilation: _size_4_t = 1, padding_mode: str = 'zeros', device=None, dtype=None)\n</code></pre> <p>               Bases: <code>_ConvTransposeNd</code></p> <p>From https://github.com/matheusja/ConvTranspose4d-PyTorch</p> Source code in <code>torch_extra/nn/modules/conv.py</code> <pre><code>def __init__(\n    self,\n    in_channels: int,\n    out_channels: int,\n    kernel_size: _size_4_t,\n    stride: _size_4_t = 1,\n    padding: _size_4_t = 0,\n    output_padding: _size_4_t = 0,\n    groups: int = 1,\n    bias: bool = True,\n    dilation: _size_4_t = 1,\n    padding_mode: str = \"zeros\",\n    device=None,\n    dtype=None,\n) -&gt; None:\n    factory_kwargs = {\"device\": device, \"dtype\": dtype}\n    kernel_size = _quadruple(kernel_size)\n    stride = _quadruple(stride)\n    padding = _quadruple(padding)\n    dilation = _quadruple(dilation)\n    if dilation != (1, 1, 1, 1):\n        raise NotImplementedError(\"Dilation other than 1 not yet implemented!\")\n    output_padding = _quadruple(output_padding)\n    super().__init__(\n        in_channels,\n        out_channels,\n        kernel_size,\n        stride,\n        padding,\n        dilation,\n        True,\n        output_padding,\n        groups,\n        bias,\n        padding_mode,\n        **factory_kwargs,\n    )\n</code></pre>"},{"location":"reference/torch_extra/nn/modules/loss/","title":"<code class=\"doc-symbol doc-symbol-nav doc-symbol-module\"></code> loss","text":""},{"location":"reference/torch_extra/nn/modules/loss/#torch_extra.nn.modules.loss","title":"loss","text":""},{"location":"reference/torch_extra/nn/modules/loss/#torch_extra.nn.modules.loss.FourierRingCorrelation","title":"FourierRingCorrelation","text":"<pre><code>FourierRingCorrelation(shape: Tuple[int, ...], size_average: bool = True, delta: int = 1)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Fourier Ring Correlation (FRC) loss function.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>tuple[int, ...]</code> <p>shape of the input tensors.</p> required <code>size_average</code> <code>bool</code> <p>if size_average=True, FRC will be averaged over radii, channel and batch. Else, FRC will be averaged over channel and batch. Default: True.</p> <code>True</code> <code>delta</code> <code>int</code> <p>delta for the ring size. Default: 1.</p> <code>1</code> Source code in <code>torch_extra/nn/modules/loss.py</code> <pre><code>def __init__(self, shape: Tuple[int, ...], size_average: bool = True, delta: int = 1) -&gt; None:\n    super(FourierRingCorrelation, self).__init__()\n    self.size_average = size_average\n    self.radial_masks = F_extra._create_radial_masks(shape, delta=delta, dims=2)\n</code></pre>"},{"location":"reference/torch_extra/nn/modules/loss/#torch_extra.nn.modules.loss.FourierShellCorrelation","title":"FourierShellCorrelation","text":"<pre><code>FourierShellCorrelation(shape: Tuple[int, ...], size_average: bool = True, delta: int = 1)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>Fourier Shell Correlation (FSC) loss function.</p> <p>Parameters:</p> Name Type Description Default <code>shape</code> <code>tuple[int, ...]</code> <p>shape of the input tensors.</p> required <code>size_average</code> <code>bool</code> <p>if size_average=True, FSC will be averaged over radii, channel and batch. Else, FSC will be averaged over channel and batch. Default: True.</p> <code>True</code> <code>delta</code> <code>int</code> <p>delta for the shell size. Default: 1.</p> <code>1</code> Source code in <code>torch_extra/nn/modules/loss.py</code> <pre><code>def __init__(self, shape: Tuple[int, ...], size_average: bool = True, delta: int = 1) -&gt; None:\n    super(FourierShellCorrelation, self).__init__()\n    self.size_average = size_average\n    self.radial_masks = F_extra._create_radial_masks(shape, delta=delta, dims=3)\n</code></pre>"},{"location":"reference/torch_extra/nn/modules/loss/#torch_extra.nn.modules.loss.MS_SSIM","title":"MS_SSIM","text":"<pre><code>MS_SSIM(data_range: float = 255, size_average: bool = True, win_size: int = 11, win_sigma: float = 1.5, channel: int = 3, spatial_dims: int = 2, weights: Optional[List[float]] = None, K: Union[Tuple[float, float], List[float]] = (0.01, 0.03), stride: int = 1)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>class for ms-ssim Args:     data_range (float or int, optional): value range of input images. (usually 1.0 or 255)     size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar     win_size: (int, optional): the size of gauss kernel     win_sigma: (float, optional): sigma of normal distribution     channel (int, optional): input channels (default: 3)     weights (list, optional): weights for different levels     K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.     stride (int, optional): stride of the sliding window</p> Source code in <code>torch_extra/nn/modules/loss.py</code> <pre><code>def __init__(\n    self,\n    data_range: float = 255,\n    size_average: bool = True,\n    win_size: int = 11,\n    win_sigma: float = 1.5,\n    channel: int = 3,\n    spatial_dims: int = 2,\n    weights: Optional[List[float]] = None,\n    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n    stride: int = 1,\n) -&gt; None:\n    r\"\"\"class for ms-ssim\n    Args:\n        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n        win_size: (int, optional): the size of gauss kernel\n        win_sigma: (float, optional): sigma of normal distribution\n        channel (int, optional): input channels (default: 3)\n        weights (list, optional): weights for different levels\n        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n        stride (int, optional): stride of the sliding window\n    \"\"\"\n\n    super(MS_SSIM, self).__init__()\n    self.win_size = win_size\n    self.win = F_extra._fspecial_gauss_1d(win_size, win_sigma).repeat([channel, 1] + [1] * spatial_dims)\n    self.size_average = size_average\n    self.data_range = data_range\n    self.weights = weights\n    self.K = K\n    self.stride = stride\n</code></pre>"},{"location":"reference/torch_extra/nn/modules/loss/#torch_extra.nn.modules.loss.SSIM","title":"SSIM","text":"<pre><code>SSIM(data_range: float = 255, size_average: bool = True, win_size: int = 11, win_sigma: float = 1.5, channel: int = 3, spatial_dims: int = 2, K: Union[Tuple[float, float], List[float]] = (0.01, 0.03), win: Optional[Tensor] = None, nonnegative_ssim: bool = False, stride: int = 1, device: Optional[device | int | str] = None)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>class for ssim Args:     data_range (float or int, optional): value range of input images. (usually 1.0 or 255)     size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar     win_size: (int, optional): the size of gauss kernel     win_sigma: (float, optional): sigma of normal distribution     channel (int, optional): input channels (default: 3)     K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.     win (Tensor, optional): window for ssim. The size should be 1xwin_size     nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu.     stride (int, optional): stride of the sliding window     device (torch.device, int or str, optional): the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (default: None). If provided, it will send kernel to device before forward.</p> Source code in <code>torch_extra/nn/modules/loss.py</code> <pre><code>def __init__(\n    self,\n    data_range: float = 255,\n    size_average: bool = True,\n    win_size: int = 11,\n    win_sigma: float = 1.5,\n    channel: int = 3,\n    spatial_dims: int = 2,\n    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n    win: Optional[Tensor] = None,\n    nonnegative_ssim: bool = False,\n    stride: int = 1,\n    device: Optional[torch.device | int | str] = None,\n) -&gt; None:\n    r\"\"\"class for ssim\n    Args:\n        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n        win_size: (int, optional): the size of gauss kernel\n        win_sigma: (float, optional): sigma of normal distribution\n        channel (int, optional): input channels (default: 3)\n        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n        win (Tensor, optional): window for ssim. The size should be 1xwin_size\n        nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu.\n        stride (int, optional): stride of the sliding window\n        device (torch.device, int or str, optional): the desired device of returned tensor. Default: if None, uses the current device for the default tensor type (default: None). If provided, it will send kernel to device before forward.\n    \"\"\"\n\n    super(SSIM, self).__init__()\n    self.win_size = win_size\n    if win is not None:\n        if len(win.size()) == 1:\n            win = win.repeat([channel, 1] + [1] * spatial_dims)\n        self.win = win\n    else:\n        self.win = F_extra._fspecial_gauss_1d(win_size, win_sigma).repeat([channel, 1] + [1] * spatial_dims)\n    if device:\n        self.win = self.win.to(device=device, dtype=torch.float32)\n    self.size_average = size_average\n    self.data_range = data_range\n    self.K = K\n    self.nonnegative_ssim = nonnegative_ssim\n    self.stride = stride\n</code></pre>"},{"location":"reference/torch_extra/nn/modules/loss/#torch_extra.nn.modules.loss.ST_MS_SSIM","title":"ST_MS_SSIM","text":"<pre><code>ST_MS_SSIM(data_range: float = 255, size_average: bool = True, win_size: int = 11, win_sigma: float = 1.5, channel: int = 3, spatial_dims: int = 2, weights: Optional[List[float]] = None, K: Union[Tuple[float, float], List[float]] = (0.01, 0.03), temporal_win_size: Optional[int] = None)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>class for ms-ssim Args:     data_range (float or int, optional): value range of input images. (usually 1.0 or 255)     size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar     win_size: (int, optional): the size of gauss kernel     win_sigma: (float, optional): sigma of normal distribution     channel (int, optional): input channels (default: 3)     weights (list, optional): weights for different levels     K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.     temporal_win_size (int, optional): the size of gauss kernel for temporal dimension</p> Source code in <code>torch_extra/nn/modules/loss.py</code> <pre><code>def __init__(\n    self,\n    data_range: float = 255,\n    size_average: bool = True,\n    win_size: int = 11,\n    win_sigma: float = 1.5,\n    channel: int = 3,\n    spatial_dims: int = 2,\n    weights: Optional[List[float]] = None,\n    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n    temporal_win_size: Optional[int] = None,\n) -&gt; None:\n    r\"\"\"class for ms-ssim\n    Args:\n        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n        win_size: (int, optional): the size of gauss kernel\n        win_sigma: (float, optional): sigma of normal distribution\n        channel (int, optional): input channels (default: 3)\n        weights (list, optional): weights for different levels\n        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n        temporal_win_size (int, optional): the size of gauss kernel for temporal dimension\n    \"\"\"\n\n    super(ST_MS_SSIM, self).__init__()\n    self.win_size = win_size\n    self.temporal_win_size = temporal_win_size\n    temporal_dim = 1 if temporal_win_size is not None else 0\n    self.win = F_extra._fspecial_gauss_1d(win_size, win_sigma).repeat(\n        [channel, 1] + [1] * (spatial_dims + temporal_dim)\n    )\n    if temporal_dim:\n        self.temporal_win = (\n            F_extra._fspecial_gauss_1d(temporal_win_size, win_sigma)\n            .repeat([channel, 1] + [1] * (spatial_dims + temporal_dim))\n            .transpose(-1, 2)\n        )\n    else:\n        self.temporal_win = None\n    self.size_average = size_average\n    self.data_range = data_range\n    self.K = K\n    self.weights = weights\n</code></pre>"},{"location":"reference/torch_extra/nn/modules/loss/#torch_extra.nn.modules.loss.ST_SSIM","title":"ST_SSIM","text":"<pre><code>ST_SSIM(data_range: float = 255, size_average: bool = True, win_size: int = 11, win_sigma: float = 1.5, channel: int = 3, spatial_dims: int = 2, K: Union[Tuple[float, float], List[float]] = (0.01, 0.03), nonnegative_ssim: bool = False, temporal_win_size: Optional[int] = None, stride: int = 1)\n</code></pre> <p>               Bases: <code>Module</code></p> <p>class for ssim Args:     data_range (float or int, optional): value range of input images. (usually 1.0 or 255)     size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar     win_size: (int, optional): the size of gauss kernel     win_sigma: (float, optional): sigma of normal distribution     channel (int, optional): input channels (default: 3)     K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.     nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu.     temporal_win_size (int, optional): the size of gauss kernel for temporal dimension     stride (int, optional): stride of the sliding window</p> Source code in <code>torch_extra/nn/modules/loss.py</code> <pre><code>def __init__(\n    self,\n    data_range: float = 255,\n    size_average: bool = True,\n    win_size: int = 11,\n    win_sigma: float = 1.5,\n    channel: int = 3,\n    spatial_dims: int = 2,\n    K: Union[Tuple[float, float], List[float]] = (0.01, 0.03),\n    nonnegative_ssim: bool = False,\n    temporal_win_size: Optional[int] = None,\n    stride: int = 1,\n) -&gt; None:\n    r\"\"\"class for ssim\n    Args:\n        data_range (float or int, optional): value range of input images. (usually 1.0 or 255)\n        size_average (bool, optional): if size_average=True, ssim of all images will be averaged as a scalar\n        win_size: (int, optional): the size of gauss kernel\n        win_sigma: (float, optional): sigma of normal distribution\n        channel (int, optional): input channels (default: 3)\n        K (list or tuple, optional): scalar constants (K1, K2). Try a larger K2 constant (e.g. 0.4) if you get a negative or NaN results.\n        nonnegative_ssim (bool, optional): force the ssim response to be nonnegative with relu.\n        temporal_win_size (int, optional): the size of gauss kernel for temporal dimension\n        stride (int, optional): stride of the sliding window\n    \"\"\"\n\n    super(ST_SSIM, self).__init__()\n    self.win_size = win_size\n    self.temporal_win_size = temporal_win_size\n    temporal_dim = 1 if temporal_win_size is not None else 0\n    self.win = F_extra._fspecial_gauss_1d(win_size, win_sigma).repeat(\n        [channel, 1] + [1] * (spatial_dims + temporal_dim)\n    )\n    if temporal_dim:\n        self.temporal_win = (\n            F_extra._fspecial_gauss_1d(temporal_win_size, win_sigma)\n            .repeat([channel, 1] + [1] * (spatial_dims + temporal_dim))\n            .transpose(-1, 2)\n        )\n    else:\n        self.temporal_win = None\n    self.size_average = size_average\n    self.data_range = data_range\n    self.K = K\n    self.nonnegative_ssim = nonnegative_ssim\n    self.stride = stride\n</code></pre>"},{"location":"userguide/config/","title":"Reconstruction Configuration","text":"<p>The config file defines the parameters for the reconstruction when using the command line interface. The config file is a <code>yaml</code> file with the default parameters defined below.</p> <p>In addition to the default parameters, there are seperate config files for the different reconstruction methods. The config files are located in the <code>nect/cfg</code> folder. The following import order of the config files are used, and the parameters are overwritten by the definitions in the later files if they are defined in multiple files.:</p> <ol> <li>The default parameters in <code>nect/cfg/default.yaml</code> are loaded first.</li> <li>The parameters specific to the reconstruction method used are loaded next. This can e.g. be <code>nect/cfg/dynamic/quadcubes.yaml</code> for the 4D-CT Quadcubes method.</li> <li>The user specific <code>yaml</code> file is loaded last. This file is defined by the user when running the reconstruction, <code>python -m nect.reconstruct &lt;path/to/config/file.yaml&gt;</code></li> </ol> <p>In the user specific <code>yaml</code> file, the user can overwrite any parameter defined in the default or method specific config files. The following parameters must be defined in the user specific <code>yaml</code> file if using the command line interface:</p> <pre><code>geometry: &lt;path/to/geometry/file.yaml&gt;\nimg_path: &lt;path/to/projections/folder&gt; or &lt;path/to/projections/file.npy&gt;\nmodel: model_type\n</code></pre> <p>The supported <code>model_type</code> are:</p> <ul> <li>For static:</li> <li><code>hash_grid</code></li> <li><code>kplanes</code></li> <li>For dynamic:</li> <li><code>quadcubes</code></li> <li><code>double_hash_grid</code></li> <li><code>kplanes_dynamic</code></li> <li><code>hypercubes</code></li> </ul> nect/cfg/default.yaml<pre><code>image_interval: 300                # (float) Seconds between generating images. Negative to not save any images\ncheckpoint_interval: 900           # (float) Seconds between saving checkpoints. Negative to not save checkpoint     \nplot_type: XY                      # Slice to plot. Plots the middle slice of the specified axis. Supported: XY, XZ, YZ. Ignored if image_interval=-1\nsave_volume: False                 # (bool) save the volume to disk. Ignored if mode = dynamic\n\n\n# Optimization options\nepochs: auto                       # (int | 'auto' | '&lt;float&gt;x') Number of epochs. Base-epochs=auto is: floor(49 / num_projections * max(nDetector)). e.g. 1.5x is then 1.5 times base-epochs\nloss: L1                           # Loss function. Supported: L1, L2.  \noptimizer: \n  otype: Adam                      # Optimizer for training. Supported: Adam, NAdam, RAdam, SGD, Lion\n  weight_decay: 0.0                # (float) Weight decay.\n  beta1: 0.9                       # (float) 1st moment decay for Adam-based/Lion optimizer. Momentum for SGD\n  beta2: 0.999                     # (float) 2nd moment decay for Adam-based/Lion optimizer. Ignored for SGD\n\nbase_lr: 0.001                     # (float) Initial learning rate\n\nlr_scheduler:\n  otype: cosine                    # Learning rate scheduler. Supported: Exponential, Cosine\n  lrf: auto                        # (float &lt; 1 | 'auto') Final learning rate fraction of initial learning rate. Final learning rate is lrf * lr\n\nwarmup:\n  steps: 500                       # (int) Number of warmup steps for exponential warmup. \n  lr0: 0.001                       # (float &lt; 1) Initial learning rate factor for exponential warmup. lr0 * lr is the initial learning rate \n\ns3im: False                        # (bool) Use S3IM for training\n\nbatch_per_proj: 1                  # (all or int &gt;= 1) Number of batches per projection. If all, use all pixels in the projection. If a number, use a maximum of n batches per projection before going to the next projection\n\n\ndownsampling_detector:\n  start: 2                         # (int &gt;= 1) Start downsampling factor for the detector\n  end: 1                           # (int &gt;= 1) End downsampling factor for the detector\n  update_interval: 1000            # (int &gt;= 1) Update interval for downsampling factor. The downsampling factor is halved every update_interval steps\n\npoints_per_ray:\n  start: 100                       # (int &gt;= 1) Start number of points per ray\n  end: auto                        # (int &gt;= 1 | 'auto' | &lt;float&gt;x) End number of points per ray. 'auto' sets to 1.5 * max(nDetector). e.g. 1.5x sets maximum points per ray to 1.5 * max(nDetector)\n  update_interval: auto            # (int &gt; 0 | 'auto' | '&lt;float&gt;x') Update interval for number of points per ray. The number of points per ray is doubled every update_interval steps. If linear is used, update with 1 steps per projection. Auto calculates update interval based on epochs and projections. If e.g. 0.5x is used, end is reached after half of the epochs\n  linear: True                     # (bool) Linear increase instead of doubling\n\nadd_poisson: False                 # (bool) Add Poisson noise to the simulated projections during training\npoints_per_batch: auto             # (int &gt; 0 | 'auto') Number of points per batch. Often the highest number of points that can fit in memory will give the best results. Auto tries to fit up to 5 000 000 points per batch\n\ngeometry: null\nchannel_order: NHW\n\n# evaluation:\n#   gt_path: null                  # (str) Supported: path to numpy object or name of SciVis dataset     \n#   gt_path_mode: SciVis           # (str) Mode for ground truth path. Supported: SciVis, path. SciVis: use the SciVis dataloading. path: use dataloading of numpy object in format [Z, Y, X]\n\nreconstruction_mode: voxel         # (str) Reconstruction mode. Supported: voxel, cylindrical. Voxel will use nVoxel, sVoxel and dVoxel defined in geometry file to only sample inside the object. Cylindrical will add a cylindrical mask so that sampling only happens inside the cylinder. Radius and height can be set in the geometry file or will be calculated\n\nmodel: null                        # (str) Model type. Supported: hash_grid, double_hash, time_encode_feature_vector, double_hash_encoder, double_hash_encoder_double_network, kplanes, pirate. See docs for more information about what the different models do\nuniform_ray_spacing: True          # (bool) Sample with uniform distance if True, if False, sample uniform number of points\ncheckpoint_epoch: 1          # save every N epochs, set None/0 to disable\ndamp_multi: [0.8, 0.4, 0.4] #[enc0, enc1-3, mpl] dampening factors\nw0_steps: 1400\nw0_lr_multi: 1\n</code></pre>"},{"location":"userguide/geometry/","title":"Geometry","text":"<p>To get a good reconstruction, the geometry-parameters need to be set correctly. We follow the same style as the <code>python</code>-version of TIGRE. All parameters is usually specified in millimeters, but one can change it to the unit one likes. The important part is that the unit is the same for all the parameters. The parameters are as follows. </p> Parameter name Definition Important Type mode <code>cone</code> or <code>parallel</code> Type of CT scan <code>str</code> DSO Distance from the source to the object (mm). Used when <code>mode=\"cone\"</code>. Not needed for parallel geometry. <code>float</code> DSD Distance from the source to the detector (mm). Used when <code>mode=\"cone\"</code>. Not needed for parallel geometry. <code>float</code> nDetector Number of detector pixels <code>[height, width]</code> (px). list of 2 ints<sup>1</sup> dDetector Size of each detector pixel <code>[height, width]</code> (mm). list of 2 floats<sup>2</sup> nVoxel Number of voxels in the reconstructed volume <code>[z, y, x]</code> (px). The order is <code>[z, y, x]</code> meaning <code>[height, width, depth]</code> list of 3 ints<sup>3</sup> dVoxel Size of each voxels in the reconstructed volume <code>[z, y, x]</code> (mm). The order is <code>[z, y, x]</code> meaning <code>[height, width, depth]</code> list of 3 float<sup>4</sup> offDetector Offset vector to the true center of the detector <code>[height, width]</code> (mm) The vector defines the true center of the detector. The vector is an offset from the current center of the detector, defined as the middle point of the detector. The coordinate system starts in the bottom left corner of the detector. list of 2 floats<sup>2</sup> offOrigin Offset vector to the image center <code>[z, y, x]</code> (mm) Only moves the position of the reconstructed volume. Currently not implemented list of 3 floats<sup>4</sup> COR Center of rotation. (mm) A horizontal shift on the axis of rotation. A positive number means that the true center of rotation is to the right of the source-to-detector-line. <code>float</code> rotDetector Rotation of the detector. <code>[roll, pitch, yaw]</code> (radians) list of 3 floats<sup>4</sup> radians boolean value telling if the angles are defined in radians or degree. <code>True</code> means that the angles are in radians, while <code>False</code> means that it is <code>bool</code> angles Acqustion angles of the projections (radians). The angles needs to be ordered the same way as <code>sorted(projections)</code> when inuputing a directory of images. The unit of angles can be changed to degrees if <code>radians=False</code> list of floats<sup>5</sup> radians Unit of angles Default is True (radians). Set to False to use degrees for angles. This will not affect <code>rotDetector</code> which is in radians <code>bool</code> timesteps Acqustion timesteps of the projections. Optional. The unit of the timesteps are arbitrary, and the timesteps will be normalized between 0 and 1 for the dynamic reconstruction. If not provided, the angles are assumed to be in acqusition order with equal spacing in time between the projections. list of numbers<sup>6</sup> or None <p>Note: For 4D-CT reconstruction, the geometry needs to be extremely accurate to avoide oscillating geometry artifacts.  </p> <ol> <li> <p><code>tuple[int, int]</code> | <code>list[int]</code> | <code>torch.tensor</code> | <code>np.ndarray</code> \u21a9</p> </li> <li> <p><code>tuple[float, float]</code> | <code>list[float]</code> | <code>torch.tensor</code> | <code>np.ndarray</code> \u21a9\u21a9</p> </li> <li> <p><code>tuple[int, int, int]</code> | <code>list[int]</code> | <code>torch.tensor</code> | <code>np.ndarray</code> \u21a9</p> </li> <li> <p><code>tuple[float, float, float]</code> | <code>list[float]</code> | <code>torch.tensor</code> | <code>np.ndarray</code> \u21a9\u21a9\u21a9</p> </li> <li> <p><code>list[float]</code> | <code>torch.tensor</code> | <code>np.ndarray</code> \u21a9</p> </li> <li> <p><code>list[float | int]</code> | <code>torch.tensor</code> | <code>np.ndarray</code> \u21a9</p> </li> </ol>"},{"location":"userguide/demo/","title":"Demo","text":"<p>We provide several demo-files to easier get started. The files are placed under NeCT/demo.</p>"},{"location":"userguide/demo/#static-ct-image-reconstruction","title":"Static CT image reconstruction","text":"<ul> <li>00 - Static: from file show how to do a static reconstruction providing the path to the projections.</li> <li>01 - Static: from array show how to do a static reconstruction with the projections already loaded into memory.</li> </ul>"},{"location":"userguide/demo/#geometry","title":"Geometry","text":"<ul> <li>02 - Geometry contains info of how to load geometry from file.</li> <li>05 - Parallel beam geometry shows how to use a parallel beam geometry instead of cone-beam geometry.</li> </ul>"},{"location":"userguide/demo/#dynamic-ct-reconstruction","title":"Dynamic CT reconstruction","text":"<ul> <li>03 - Dynamic: export video show how to do a 4DCT reconstruction and export videos (original and difference video)</li> <li>04 - Dynamic: export volumes show how to do a 4DCT reconstruction when the pair of angles and projections are not ordered by the acqusition time. In the end, volumes for multiple timesteps are exported and saved as 3D tiff-files.</li> <li>06 - Dynamic: export video of projections show how to do export videos of the projections. This can be used as a quality check of the projections.</li> <li>07 - Reconstruct from config file show how to reconstruct from a configuration file. This is useful when you have a pre-defined setup and want to run the reconstruction without manually specifying all parameters. An example of this is the Bentheimer experiment.</li> </ul>"},{"location":"userguide/demo/00_static_from_file/","title":"00 - Static: from file","text":"<p>This demo show how to reconstruct by providing the path to the projections to the API.</p> demo/00_static_reconstruct_from_file.py<pre><code>\"\"\"\nDemo 00: reconstruct a static volume from a file.\n\"\"\"\n\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom nect.download_demo_data import download_demo_data, get_demo_data_path\n\nimport nect\n\ngeometry = nect.Geometry(\n    DSD=1500.0,  # Distance Source Detector\n    DSO=1000.0,  # Distance Source Origin\n    nDetector=[256, 512],  # Number of detector pixels [rows, columns]/[height, width]\n    dDetector=[1.75, 1.75],  # Size of detector pixels [row, columns]/[height, width]\n    nVoxel=[256, 512, 256],  # Number of voxels [height, width, depth]/[z, y, x]\n    dVoxel=[1.0, 1.0, 1.0],  # Size of voxels [height, width, depth]/[z, y, x]\n    angles=np.linspace(0, 360, 49, endpoint=False),  # Projection angles\n    mode=\"cone\",  # Geometry mode (cone or parallel)\n    radians=False,  # Angle units (radians (True) or degrees (False))\n)\ndemo_dir = get_demo_data_path(\"Carp-cone\")\ndownload_demo_data(\"Carp-cone\")\nvolume = nect.reconstruct(\n    geometry=geometry,\n    projections=demo_dir / \"projections.npy\",\n    log=True,\n    exp_name=\"carp\",\n    quality=\"low\",\n)\nplt.imsave(\"carp.png\", volume[128], cmap=\"gray\", dpi=300)\n</code></pre>"},{"location":"userguide/demo/01_static_from_array/","title":"01 - Static: from array","text":"<p>This demo show how to reconstruct with an array of shape (nProjections, height, width) already loaded into memory.</p> demo/01_static_reconstruct_from_array.py<pre><code>\"\"\"\nDemo 01: Reconstruct a static volume from an array\"\"\"\n\nfrom pathlib import Path\n\nimport numpy as np\nfrom nect.download_demo_data import download_demo_data, get_demo_data_path\n\nimport nect\n\ngeometry = nect.Geometry(\n    DSD=1500.0,  # Distance Source Detector\n    DSO=1000.0,  # Distance Source Origin\n    nDetector=[256, 512],  # Number of detector pixels [rows, columns]/[height, width]\n    dDetector=[1.75, 1.75],  # Size of detector pixels [row, columns]/[height, width]\n    nVoxel=[256, 512, 256],  # Number of voxels [height, width, depth]/[z, y, x]\n    dVoxel=[1.0, 1.0, 1.0],  # Size of voxels [height, width, depth]/[z, y, x]\n    angles=np.linspace(0, 360, 49, endpoint=False),  # Projection angles\n    mode=\"cone\",  # Geometry mode (cone or parallel)\n    radians=False,  # Angle units (radians (True) or degrees (False))\n)\ndownload_demo_data(\"Carp-cone\", force_download=False) # Download the demo data. You can force a re-download by setting force_download=True\ndemo_dir = get_demo_data_path(\"Carp-cone\")\nprojections = np.load(demo_dir / \"projections.npy\")\nvolume = nect.reconstruct(geometry=geometry, projections=projections, quality=\"high\")\nnp.save(\"volume.npy\", volume)\n</code></pre>"},{"location":"userguide/demo/02_geometry/","title":"02 - Geometry","text":"<p>This demo show how to load the geometry from file. The geometry file is found in demo/NeCT-data/static/geometry.</p> demo/02_geometry_load.py<pre><code>\"\"\"\nDemo 02: Load geometry from a YAML file and reconstruct a static volume from an array\"\"\"\n\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom nect.download_demo_data import download_demo_data, get_demo_data_path\n\nimport nect\n\ndemo_dir = get_demo_data_path(\"Carp-cone\")\ndownload_demo_data(\"Carp-cone\")\ngeometry = nect.Geometry.from_yaml(demo_dir / \"geometry.yaml\")\nvolume = nect.reconstruct(geometry=geometry, projections=demo_dir / \"projections.npy\")\nplt.imsave(\"carp.png\", volume[128], cmap=\"gray\", dpi=300)\n</code></pre>"},{"location":"userguide/demo/03_dynamic_video/","title":"03 - Dynamic: export video","text":"<p>This demo show how to do a 4DCT reconstruction and export a video of the reconstructed object afterwards.</p> demo/03_dynamic_reconstruction_video.py<pre><code>\"\"\"\nDemo 03: Reconstruct a dynamic volume from an array and export a video of the reconstruction.\"\"\"\nfrom pathlib import Path\n\nfrom nect.download_demo_data import download_demo_data, get_demo_data_path\n\nimport nect\n\ndemo_dir = get_demo_data_path(\"SimulatedFluidInvasion\")\ndownload_demo_data(\"SimulatedFluidInvasion\")\ngeometry = nect.Geometry.from_yaml(demo_dir / \"geometry.yaml\")\nreconstruction_path = nect.reconstruct(\n    geometry=geometry,\n    projections=demo_dir / \"projections.npy\",\n    quality=\"high\",\n    mode=\"dynamic\",\n    exp_name=\"SimulatedFluidInvasion\", # optional, name of the experiment\n    config_override={\n        \"epochs\": \"1x\",  # a multiplier of base-epochs. Base-epochs is: floor(49 / num_projections * max(nDetector))\n        \"checkpoint_interval\": 1800,  # How often to save the model in seconds\n        \"image_interval\": 30,  # How often to save images in seconds\n        \"plot_type\": \"XZ\", # XZ or XY, YZ\n    },\n)\nnect.export_video(reconstruction_path, add_scale_bar=True, acquisition_time_minutes=60)\n</code></pre>"},{"location":"userguide/demo/04_dynamic_volumes/","title":"04 - Dynamic: export volumes","text":"<p>This demo show how to do a 4DCT reconstruction and export multiple CT volumes of the different timesteps afterwards.</p> demo/04_dynamic_reconstruction_export_volume.py<pre><code>\"\"\"\nDemo 04: Reconstruct a dynamic volume and export volumes of the reconstruction. For more fine-grained control, look at the export_volumes function.\"\"\n\"\"\"\n\nfrom pathlib import Path\n\nfrom nect.download_demo_data import download_demo_data, get_demo_data_path\nimport nect\nimport torch\nprint(torch.__version__)\nprint(torch.cuda.get_arch_list())\nprint(torch.cuda.get_device_name(0))\nprint(torch.cuda.current_device())\nprint(torch.cuda.is_available())\n\ndownload_demo_data(\"SimulatedFluidInvasion\")\ndemo_dir = get_demo_data_path(\"SimulatedFluidInvasion\")\ngeometry = nect.Geometry.from_yaml(demo_dir / \"geometry.yaml\")\nreconstruction_path = nect.reconstruct(\n    geometry=geometry,\n    projections=demo_dir / \"projections.npy\",\n    quality=\"high\",\n    mode=\"dynamic\",\n    config_override={\n        \"epochs\": \"3x\",  # a multiplier of base-epochs. Base-epochs is: floor(49 / num_projections * max(nDetector))\n        \"checkpoint_interval\": 0,  # How often to save the model in seconds\n        \"image_interval\": 0,  # How often to save images in seconds\n        \"plot_type\": \"XZ\",  # XZ or XY, YZ\n        \"encoder\": {\n            \"otype\": \"HashGrid\",\n            \"n_levels\": 20,\n            \"n_features_per_level\": 4,\n            \"log2_hashmap_size\": 21,\n            \"base_resolution\": 16,\n            \"max_resolution_factor\": 2\n        },\n    },\n)\nnect.export_volumes(reconstruction_path, binning=3, avg_timesteps=5)\n</code></pre>"},{"location":"userguide/demo/05_parallel_beam/","title":"05 - Parallel beam geometry","text":"<p>This demo show how to do a reconstruction with parallel beam geometry. The geometry change is equal for both static and dynamic reconstruction.</p> demo/05_parallel_beam.py<pre><code>\"\"\"\nDemo 05: reconstruct using parallel beam geometry\n\"\"\"\n\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom nect.download_demo_data import download_demo_data, get_demo_data_path\n\nimport nect\n\ngeometry = nect.Geometry(\n    nDetector=[256, 512],  # Number of detector pixels [rows, columns]/[height, width]\n    dDetector=[1., 1.],  # Size of detector pixels [row, columns]/[height, width]\n    nVoxel=[256, 512, 256],  # Number of voxels [height, width, depth]/[z, y, x]\n    dVoxel=[1.0, 1.0, 1.0],  # Size of voxels [height, width, depth]/[z, y, x]\n    angles=np.linspace(0, 360, 49, endpoint=False),  # Projection angles\n    mode=\"parallel\",  # Geometry mode (cone or parallel)\n    radians=False,  # Angle units (radians (True) or degrees (False))\n)\ndemo_dir = get_demo_data_path(\"Carp-parallel\")\ndownload_demo_data(\"Carp-parallel\")\nvolume = nect.reconstruct(\n    geometry=geometry,\n    projections=demo_dir / \"projections.npy\",\n    log=True,\n    exp_name=\"carp_parallel\",\n    quality=\"medium\",\n)\nplt.imsave(\"carp.png\", volume[128], cmap=\"gray\", dpi=300)\n</code></pre>"},{"location":"userguide/demo/06_dynamic_export_video_projections/","title":"06 - Dynamic: export projections","text":"<p>This demo show how to create a video of the projections for analysis.</p> demo/06_export_video_projections.py<pre><code>\"\"\"\nDemo 06: export video of the projections for data analysis, when one does not have a configuration file.\n\"\"\"\n\nfrom pathlib import Path\n\nimport numpy as np\nfrom nect.download_demo_data import download_demo_data, get_demo_data_path\n\nfrom nect import Geometry\nfrom nect.config import get_config\nfrom nect.data import NeCTDataset\n\ndemo_dir = get_demo_data_path(\"SimulatedFluidInvasion\")\ndownload_demo_data(\"SimulatedFluidInvasion\")\ngeometry = Geometry.from_yaml(demo_dir / \"geometry.yaml\")\nprojections = demo_dir / \"projections.npy\"\n\nconfig = get_config(geometry, projections, mode=\"dynamic\")\nNeCTDataset(config).export_video(file=\"video_projections.mp4\")\n</code></pre>"},{"location":"userguide/demo/07_reconstruct_from_cfg_file/","title":"07 - Reconstruction from config file","text":"<p>This demo show how to reconstruct from a configuration file.</p> demo/07_reconstruct_from_cfg_file.py<pre><code>\"\"\"\nDemo 07: reconstruct from a configuration file.\nAn example of this is the Bentheimer experiment. \n\"\"\"\nfrom nect.download_demo_data import download_demo_data\nfrom nect import reconstruct_from_config_file\nimport yaml\n\nconfig_file = download_demo_data(\"Bentheimer\") / \"config.yaml\"\n\n# need to change the img_path to point to the path of the projections\nwith open(config_file, \"r\") as f:\n    config = yaml.safe_load(f)\n\nconfig[\"img_path\"] = str(config_file.parent / \"projections\")\n\nwith open(config_file, \"w\") as f:\n    yaml.safe_dump(config, f)\n\nreconstruct_from_config_file(config_file)\n</code></pre>"},{"location":"userguide/start/","title":"Getting started","text":"<p>To use NeCT, you will need a computer with a GPU. The project has been tested to work on both Windows and Linux. Follow the installation guide to get started, and then check out the quick start guide to get started with the project.</p> <ul> <li>Installation</li> <li>Quick start</li> </ul>"},{"location":"userguide/start/installation/","title":"Installation","text":"<p>NeCT has been tested on Windows and Linux with the following dependencies:</p> Package Version Notes python 3.11 | 3.12 pytorch 2.4 - 2.7 CUDA 12.X CMake (Linux) 3.24 For tiny-cuda-nn C++17 (Windows) For tiny-cuda-nn <p>Recommended: Use conda or uv to manage your Python environment.</p> <ul> <li>For video export, the <code>avc1</code> codec for <code>ffmpeg</code> is only available with conda. With uv, video export uses the <code>mp4v</code> codec. If video export using <code>avc1</code> is vital, use conda.</li> <li>Tested with <code>python=3.11, 3.12</code> and <code>pytorch&gt;=2.4,&lt;2.8</code>.</li> <li>To install for multiple compute capabilities, see below.</li> </ul> <p>Note: Ensure <code>PATH</code> and <code>LD_LIBRARY_PATH</code> include the CUDA binaries as described in tiny-cuda-nn. If you encounter installation errors related to <code>tiny-cuda-nn</code>, check their issues page. Building binaries for both tiny-cuda-nn and NeCT may take several minutes.</p>"},{"location":"userguide/start/installation/#uv-installation","title":"Uv Installation","text":"<p>If you don't have uv installed, follow the uv installation guide. This will install pytorch with CUDA 12.4. <pre><code>uv venv --python=3.12\nsource venv/bin/activate\nuv pip install git+https://github.com/haakonnese/nect[torch]\nuv pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch --no-build-isolation\n</code></pre></p>"},{"location":"userguide/start/installation/#custom-pytorch-version","title":"Custom PyTorch Version","text":"<p>To use a specific PyTorch version (2.4-2.7) visit the PyTorch Installation Page and install the desired PyTorch version into your uv environment. Then install NeCT with: <pre><code>uv pip install git+https://github.com/haakonnese/nect --no-build-isolation-package torch\nuv pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch --no-build-isolation\n</code></pre></p>"},{"location":"userguide/start/installation/#conda-installation","title":"Conda Installation","text":"<pre><code>conda create -n nect python=3.12 -y\nconda activate nect\nconda install pytorch==2.5.1 torchvision==0.20.1 pytorch-cuda=12.4 lightning==2.1 conda-forge::opencv -c pytorch -c nvidia -c conda-forge -y\npip install -v git+https://github.com/haakonnese/nect\npip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch\n</code></pre> <p>con</p>"},{"location":"userguide/start/installation/#install-multiple-compute-capabilities","title":"Install Multiple Compute Capabilities","text":"<p>To build for multiple compute capabilities (e.g., 60=P100, 70=V100, 80=A100, 90=H100), set these environment variables before installing NeCT:</p> <pre><code>export CUDA_ARCHITECTURES=\"60;70;80;90\"\nexport CMAKE_CUDA_ARCHITECTURES=${CUDA_ARCHITECTURES}\nexport TCNN_CUDA_ARCHITECTURES=${CUDA_ARCHITECTURES}\nexport TORCH_CUDA_ARCH_LIST=\"6.0 7.0 8.0 9.0\"\nexport FORCE_CUDA=\"1\"\n</code></pre>"},{"location":"userguide/start/installation/#run-on-idun","title":"RUN ON IDUN","text":"<p>source nect_a100/bin/activate git pull origin master uv pip install -e . python -m demo.08_static_initialization.py</p>"},{"location":"userguide/start/quickstart/","title":"Quick Start","text":"<p>Start using <code>NeCT</code> to reconstruct CT images or 4DCT videos. For more advanced usage, look at the demos, the geometry configuration, and how to set up the configuration file.</p> Static CT4DCT <p>A static reconstruction using <code>NeCT</code> with <code>medium</code> quality, creating a CT image that is saved to a file.</p> <p>Example</p> <pre><code>import numpy as np\nimport nect\n\ngeometry = nect.Geometry(\n    DSD=1500.0,  # Distance Source Detector\n    DSO=1000.0,  # Distance Source Origin\n    nDetector=[256, 512],  # Number of detector pixels [rows, columns]/[height, width]\n    dDetector=[1.75, 1.75],  # Size of detector pixels [row, columns]/[height, width]\n    nVoxel=[256, 512, 256],  # Number of voxels [height, width, depth]/[z, y, x]\n    dVoxel=[1.0, 1.0, 1.0],  # Size of voxels [height, width, depth]/[z, y, x]\n    angles=np.linspace(0, 360, 49, endpoint=False),  # Projection angles\n    mode=\"cone\",  # Geometry mode (cone or parallel)\n    angles_type=\"degrees\",  # Angle units (degrees or radians)\n)\n\nvolume = nect.reconstruct(\n    geometry=geometry,\n    projections=&lt;projections-path&gt;, # a single npy-file of shape [nProjections, height, width] \n                                    # or a directory of images\n    quality=\"medium\",\n)\nnp.save(\"volume.npy\", volume)\n</code></pre> <p>4DCT reconstruction with <code>NeCT</code> is done with <code>higher</code> quality. Afterwards, a video of the middle slice of the object is created through the whole time-series. The video is saved in the same folder as the reconstruction with config and the model weights.</p> <p>Example</p> <pre><code>import numpy as np\nimport nect\n\ngeometry = nect.Geometry(\n    DSD=1500.0,  # Distance Source Detector\n    DSO=1000.0,  # Distance Source Origin\n    nDetector=[256, 512],  # Number of detector pixels [rows, columns]/[height, width]\n    dDetector=[1.75, 1.75],  # Size of detector pixels [row, columns]/[height, width]\n    nVoxel=[256, 512, 256],  # Number of voxels [height, width, depth]/[z, y, x]\n    dVoxel=[1.0, 1.0, 1.0],  # Size of voxels [height, width, depth]/[z, y, x]\n    angles=np.linspace(0, 360, 49, endpoint=False),  # Projection angles\n    mode=\"cone\",  # Geometry mode (cone or parallel)\n    angles_type=\"degrees\",  # Angle units (degrees or radians)\n)\nprojections = np.load(&lt;projections-path&gt;)\n\nreconstruction_path = nect.reconstruct(\n    geometry=geometry,\n    projections=projections,\n    quality=\"highest\",\n    mode=\"dynamic\"\n)\nnect.export_video(reconstruction_path, add_scale_bar=True, acquisition_time_minutes=60)\n</code></pre>"}]}